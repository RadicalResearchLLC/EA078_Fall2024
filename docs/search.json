[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Environmental Data Visualization - EA 078 Fall 2024",
    "section": "",
    "text": "Preface\nThis is the course website for ‘EA 078 PZ - Environmental Data Visualization’. In this course, you will learn to import, manipulate, and visualize data using R, with hands-on examples. You will also learn the theory and ethics of environmental data visualization. Throughout the course, you will have the opportunity to practice and apply the skills and judgment of data visualization to provide effective illustrations of real-world data in order to persuade, inform, and communicate.",
    "crumbs": [
      "Home",
      "Preface"
    ]
  },
  {
    "objectID": "index.html#preface",
    "href": "index.html#preface",
    "title": "Environmental Data Visualization - EA 078 Fall 2024",
    "section": "",
    "text": "This book is created using the Quarto publishing system in order to provide an example of its use in environmental data visualization and as a course resource tool for display of coding examples and visualization.",
    "crumbs": [
      "Home",
      "Preface"
    ]
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "Environmental Data Visualization - EA 078 Fall 2024",
    "section": "License",
    "text": "License\nThis website is free to use, and is licensed under the Creative Commons Attribution-ShareAlike 4.0 License.",
    "crumbs": [
      "Home",
      "Preface"
    ]
  },
  {
    "objectID": "index.html#earth-selfie",
    "href": "index.html#earth-selfie",
    "title": "Environmental Data Visualization - EA 078 Fall 2024",
    "section": "Earth Selfie",
    "text": "Earth Selfie\nFigure 1 is the iconic Pale Blue Dot image taken by the Voyager 1 space probe in 1990 from beyond Neptune. The Earth in this image is only a single pixel in size.\n\n\n\n\n\nFigure 1: Carl Sagan’s Pale Blue Dot - an effective visualization?",
    "crumbs": [
      "Home",
      "Preface"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "Introductions",
    "section": "",
    "text": "Course Description\nThis is an introductory course on the theory and practice of effective communication with quantitative data. This course will introduce the theory of data visualization, discuss the ethics of data visualization, provide hands-on training in acquiring, tidying, and visualizing quantitative environmental data, and critically examine current environmental justice tools (CalEnviroScreen, EPA’s EJScreen, EPA’s EnviroAtlas, CEJST).",
    "crumbs": [
      "Home",
      "Introductions"
    ]
  },
  {
    "objectID": "intro.html#about-me",
    "href": "intro.html#about-me",
    "title": "Introductions",
    "section": "About Me",
    "text": "About Me\nCall me any of the following:\n\nMike\nDr. Mike\nProfessor Mike\nDr. McCarthy\nProfessor McCarthy\n\nMy pronouns are he/him. Here is some of my background.\n\nAdjunct Professor in Department of Environmental Analysis\nI have been an Environmental Consultant for ~20 years\n\ncurrently I am a sole proprietor at Radical Research LLC\nprior to that I worked at a company called Sonoma Technology, Inc.\n\nTrained as a chemist/atmospheric chemist\n\nundergrad at Creighton University\ngrad school at UC Berkeley\n\nIn my spare time, I enjoy\n\ngames (board, tabletop roleplaying, video games)\nyoga\nswimming\nfighting warehouse development in my neighborhood and the IE as Vice Chair of Riverside Neighbors Opposing Warehouses\n\nMy spouse is a Professor of Botany and Plant Sciences at UC Riverside, and my son is a student in middle school\nMy goals for this course\n\nbounce ideas around for cool new ways of looking at environmental data\nexplore and connect with your environmental interests\ncontinuously improve my teaching to better meet your needs\nlearn new ways of thinking about problems\nbuild tools to empower, inform, and persuade our communities",
    "crumbs": [
      "Home",
      "Introductions"
    ]
  },
  {
    "objectID": "intro.html#about-you",
    "href": "intro.html#about-you",
    "title": "Introductions",
    "section": "About You",
    "text": "About You\nNow it is your turn to introduce yourself. Please address the following four bullets. Please add anything else you think will be helpful for me and/or your colleagues to know about you.\n\nName (as you would like to be addressed)\nYear\nMajor/Field\nYour goal(s) for this course (if any)\nAn uninteresting factoid about you",
    "crumbs": [
      "Home",
      "Introductions"
    ]
  },
  {
    "objectID": "information.html",
    "href": "information.html",
    "title": "1  Fundamentals of Data Visualization",
    "section": "",
    "text": "1.1 Data, Information, and Knowledge",
    "crumbs": [
      "Home",
      "Unit 1: Introduction to Data Visualization",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Fundamentals of Data Visualization</span>"
    ]
  },
  {
    "objectID": "information.html#data-information-and-knowledge",
    "href": "information.html#data-information-and-knowledge",
    "title": "1  Fundamentals of Data Visualization",
    "section": "",
    "text": "1.1.1 What is data?\n\nFacts, or discrete elements of information.\n\nquantitative or qualitative observations or descriptions\nstatistics or values represented in a form suitable for processing by computer\nplural of datum (never ever use this, IMO, everything is data, singular or plural)\n\n1.1.2 What is information?\n\nThe act of informing or the condition of being informed; communication of knowledge\n\nProcessed, stored, or transmitted data; structured data; data in context and significance\nStimuli that has meaning in some context for its receiver\n\n1.1.3 What is knowledge?\n\nGeneral understanding or familiarity with a subject\nAwareness of a subject; the state of being informed\nIntellectual understanding; the state of appreciating the truth of information",
    "crumbs": [
      "Home",
      "Unit 1: Introduction to Data Visualization",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Fundamentals of Data Visualization</span>"
    ]
  },
  {
    "objectID": "information.html#weather-map-example",
    "href": "information.html#weather-map-example",
    "title": "1  Fundamentals of Data Visualization",
    "section": "\n1.2 Weather map example",
    "text": "1.2 Weather map example\n\n\n\n\n\nFigure 1.1: LA Times Weather Map\n\n\nThe visualization structures the underlying data into information. A good visualization communicates complex ideas with clarity, precision, and efficiency. It imparts knowledge.\n\n1.2.1 Categories of information illustrated by the newspaper weather visualization\n\n\nQuantitative (i.e., numerical) data\n\nTemperatures (high, low)\nPrecipitation\nWind speeds and directions\nAir quality index\nUV index\nTide heights and times\nSunrise & sunset times\n\n\n\nQualitative observations\n\n‘Seasonably warm’\n‘Fog, then mostly sunny’\nSky categories (partly cloudy, thunderstorms, rain, sunny, showers, fog, snow, ice)\nAir quality categories (e.g., unhealthy, moderate)\nFlood potential\nWarm front/cold front\nLow/high pressure\ndangerous rip currents risk is moderate/low\n\n\n\nSpatial\n\ngeographic properties\ntopology\ngeometric\ndistance\nprojections\n\n\n\n1.2.2 Data encodings\n\n\nGeometric primitives such as points, lines, and areas\n\nVisual channels such as size, color, shape, position, angle, and texture\n\nExamples of these encodings are shown in Figure 1.2.\n\n\n\n\n\nFigure 1.2: image credit: Nils Gehlenborg, ISMB/ECCB 2011\n\n\n\n\n\n\n\n\nWhy am I calling this information, and not data or insight?\nWhy isn’t this course called Environmental Information Visualization?",
    "crumbs": [
      "Home",
      "Unit 1: Introduction to Data Visualization",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Fundamentals of Data Visualization</span>"
    ]
  },
  {
    "objectID": "information.html#abstraction",
    "href": "information.html#abstraction",
    "title": "1  Fundamentals of Data Visualization",
    "section": "\n1.3 Abstraction",
    "text": "1.3 Abstraction\nWhen we talk about the weather and use a data visualization, we are abstracting from Figure 1.3 to a 2-D representation of some numbers, colors, or pictures on a pixelized screen.\n\n\n\n\n\nFigure 1.3: Palm Springs\n\n\n🌩️🌩️🌩️🌩️🌩️🌩️🌩️🌩️🌩️🌩️🌩️🌩️🌩️🌩️🌩️🌩️🌩️🌩️🌩️🌩️🌩️🌩️🌩️\nData visualization is an artistic abstraction.\nAny environmental data visualization is not the thing itself. We are abstracting the thing itself in order to represent it in a condensed and structured way that conveys information. A thunderstorm emoji conveys the information about the weather in an abstract way, but is not the weather. However, we can put that thunderstorm emoji\n\non a map to provide spatial information\non a clock to provide temporal information\non a phone or an electronic device to communicate the weather in shorthand\n\nSimilarly, the abstraction of data into information allows for substantial control over the stylistic choices. Just like art has impressionism, realism, and surrealism, there are many different schools of thought about the appropriate ways to convey information.\nIn other words, the colors, symbols, shapes, and other stylistic choices encode and reveal truths about the data.",
    "crumbs": [
      "Home",
      "Unit 1: Introduction to Data Visualization",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Fundamentals of Data Visualization</span>"
    ]
  },
  {
    "objectID": "information.html#ethics",
    "href": "information.html#ethics",
    "title": "1  Fundamentals of Data Visualization",
    "section": "\n1.4 Ethics",
    "text": "1.4 Ethics\n\n\n\n\n\n\nNote\n\n\n\nAvoid misrepresention!\n\n\nVisualization methods can, purposefully or inadvertently, distort the underlying data’s meaning. There are many underlying causes that can cause this distortion. Distortion can be caused (un)intentionly by the designer of the visualization. The other side is that the visualization may not be understandable to the user. Broadly, these problems can be categorized as:\n\n1.4.1 Spatial projection\n2-Dimensional cartographic representations (i.e., maps) distort either\n\n\nangles,\n\n\ndistances, or\n\nsizes\n\n1.4.2 Cognitive\nProblems associated with -\n\ngraphical elements - “Roses are red, violets are blue.”\nover-simplification - compare ozone alert images from KTLA news and the SCAQMD.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nunder-simplification\n\n\n\n\n\n\nFigure 1.4: Forward 24-hr wind trajectories from MacDonald et al., 2006, https://doi.org/10.1080/10473289.2006.10464509\n\n\n\nheterogeneity of intended audience (e.g., language barriers, color palettes for color-blindness).\n\n1.4.3 Emotional\n\nGraphical design or content may be repellent or triggering. In the syllabus, the COVID sneeze imagery is both factual model data and an intentionally gross way of visualizing the data to generate a feeling of disgust.\n\n\nA gross image of particles emissions from a sneeze\n\n\n1.4.4 Social\n\nCross-cultural norms -\n\ndirectionality of reading (left-to-right vs. right-to-left)\ncontext of color-scales (e.g., red-green in eastern vs. western cultures)\n\nUnderstanding the intended audience and their norms is always key to open and empathetic communication, whether through words or visualizations. Multiple visualizations may be required for communication with a diverse (i.e., heterogeneous) audience.",
    "crumbs": [
      "Home",
      "Unit 1: Introduction to Data Visualization",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Fundamentals of Data Visualization</span>"
    ]
  },
  {
    "objectID": "information.html#what-is-the-baseline",
    "href": "information.html#what-is-the-baseline",
    "title": "1  Fundamentals of Data Visualization",
    "section": "\n1.5 What is the Baseline?",
    "text": "1.5 What is the Baseline?\nOne of the most common distortions is changing the scale of the axis to distort magnitudes. Here’s an example.\nThe code below loads some R packages that are useful for data processing and visualization.\n\nlibrary(data.table)\nlibrary(tidyverse)\nlibrary(plotly)\n\nThis code imports monthly mean Mauna Loa CO2 data from NOAA GMD CMDL, renames the columns, and then displays the bottom five rows to make sure it shows what we think it should.\n\nco2 &lt;- read_table('https://www.gml.noaa.gov/webdata/ccgg/trends/co2/co2_mm_mlo.txt',\n                  skip = 57 ) #%&gt;%\n\nfieldNames &lt;- c('year', 'month', 'decDate', 'meanCO2', 'trendedCO2', 'days', 'stdev', 'unc')\ncolnames(co2) &lt;- fieldNames\n\ntail(co2)\n\n# A tibble: 6 × 8\n   year month decDate meanCO2 trendedCO2  days stdev   unc\n  &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1  2024     2   2024.    425.       424.    22  1.24  0.51\n2  2024     3   2024.    425.       424.    22  0.99  0.4 \n3  2024     4   2024.    427.       424.    24  0.98  0.38\n4  2024     5   2024.    427.       424.    29  0.76  0.27\n5  2024     6   2024.    427.       424.    20  0.65  0.28\n6  2024     7   2025.    426.       425.    24  0.69  0.27\n\nhead(co2)\n\n# A tibble: 6 × 8\n   year month decDate meanCO2 trendedCO2  days stdev   unc\n  &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1  1959     7   1960.    317.       316.    -1 -9.99 -0.99\n2  1959     8   1960.    315.       316.    -1 -9.99 -0.99\n3  1959     9   1960.    314.       317.    -1 -9.99 -0.99\n4  1959    10   1960.    313.       316.    -1 -9.99 -0.99\n5  1959    11   1960.    315.       317.    -1 -9.99 -0.99\n6  1959    12   1960.    316.       316.    -1 -9.99 -0.99\n\n\nThe CO2 data is displayed in #fig-KeelingCurve1 and #fig-KeelingCurve2. How does the y-axis scale affect the interpretation of the same dataset?\n\nplot1 &lt;- co2 %&gt;% \n  ggplot(aes(x = decDate, y = meanCO2)) +\n  geom_line() +\n  theme_bw() +\n  labs(x = 'Year', y = 'Concentration CO2 (ppm)')\n\nggplotly(plot1)\n\n\n\n\n\n\nFigure 1.5: The Keeling Curve showing monthly average CO2 concentrations (ppm) at Mauna Loa\n\n\n\n\nplot2 &lt;- plot1 + \n  scale_y_continuous(limits = c(0,425))\n\nggplotly(plot2)\n\n\n\n\n\n\nFigure 1.6: The Keeling Curve showing monthly average CO2 concentrations (ppm) at Mauna Loa\n\n\n\nA longer viewpoint is shown in Figure 1.7 going back well beyond the late 1950s using CO2 data from ice cores, sediments, and other paleo-climatological sources. It shows another y-axis scale.\nHow do the three different y-axis scales distort the user impression of the data?\n\n\n\n\n\nFigure 1.7: HistoricalCO2",
    "crumbs": [
      "Home",
      "Unit 1: Introduction to Data Visualization",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Fundamentals of Data Visualization</span>"
    ]
  },
  {
    "objectID": "information.html#color",
    "href": "information.html#color",
    "title": "1  Fundamentals of Data Visualization",
    "section": "\n1.6 Color",
    "text": "1.6 Color\nColor is fraught with peril and cultural associations. Moreover, roughly 10% of the population has some form of color-blindness.\n\n\n\n\n\n\nNote\n\n\n\nColor is very commonly used to manipulate the audience in environmental data visualization.\n\n\n\nBe cognizant of how choosing a color palette manipulates the audience\nDon’t use too many colors (rule of seven)\nBe aware of the three types of color palettes and choose the right one as in Figure 1.8.\n\ncontinuous sequential\ncategorical\ncontinuous diverging\n\n\n\n\n\n\n\n\nFigure 1.8: Rcolorbrewer color palettes\n\n\nHow does CalEnviroScreen look when changing from a Blues colorPalette to viridis magma?\n\n\n\n\n\n\n\n\n\n\n\n\n\nLooking for more on color palettes? Check out this Data Visualization Society post, or this color harmony blog post for marketing design.",
    "crumbs": [
      "Home",
      "Unit 1: Introduction to Data Visualization",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Fundamentals of Data Visualization</span>"
    ]
  },
  {
    "objectID": "information.html#salience",
    "href": "information.html#salience",
    "title": "1  Fundamentals of Data Visualization",
    "section": "\n1.7 Salience",
    "text": "1.7 Salience\nVisual salience is the distinctive perceptual measure of how much a visual stimuli stands out from its surrounding neighbors to grab an observer’s attention.\nRead this!\n\n\n\n\n\n\nNote\n\n\n\nFocus on the pictures and captions to get an impression of visual salience. We’ll be using this as reading as a topic for discussion.",
    "crumbs": [
      "Home",
      "Unit 1: Introduction to Data Visualization",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Fundamentals of Data Visualization</span>"
    ]
  },
  {
    "objectID": "coding1.html",
    "href": "coding1.html",
    "title": "2  Coding in R - Basics",
    "section": "",
    "text": "2.1 Overview",
    "crumbs": [
      "Home",
      "Unit 1: Introduction to Data Visualization",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Coding in R - Basics</span>"
    ]
  },
  {
    "objectID": "coding1.html#overview",
    "href": "coding1.html#overview",
    "title": "2  Coding in R - Basics",
    "section": "",
    "text": "Step 0 - Install R and RStudio\n\n\nStep 1 - Open RStudio \nA. Put in a header comment - who, when, what, where\nB. Install key packages\nC. Load key packages\n\n\nStep 2 - Acquire and/or Load Data\nA. Identify the path to the data\nB. Identify the data format\nC. Choose the right function to load the data - go to Step 1B and 1C again as needed D. Write code to import the data\nE. Run the code to import the data\n\nCheck for Error messages and warning messages in console; if failure, go back to Step 2D\n\nCheck to make sure data is loaded (look in Environment window)\nF. Step 2F - Look at the data - did it import correctly\n\nCheck column headers\n\nCheck data types\nG. Repeat Step 2 as needed for any other data required for visualization\n\n\n\n\nStep 3 - Tidy the data Advanced data science\n\n\nStep 4 - Visualize the data\nA. Choose the visualization type\nB. Choose the right functions\nC. Write code to do a basic visualization\nD. Add code to improve the visualization (repeat as needed)\nE. Annotate labels, axes, points, legends\nF. Export or publish the visualization\n\n\nStep 5 - Communicate with your audience using the visualization A. Get feedback from audience B. Revise visualization (Step 4D as needed) to improve for intended audience",
    "crumbs": [
      "Home",
      "Unit 1: Introduction to Data Visualization",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Coding in R - Basics</span>"
    ]
  },
  {
    "objectID": "coding1.html#example-1---mpg-dataset",
    "href": "coding1.html#example-1---mpg-dataset",
    "title": "2  Coding in R - Basics",
    "section": "\n2.2 Example 1 - mpg dataset",
    "text": "2.2 Example 1 - mpg dataset\n\n2.2.1 Step 1 - Open RStudio; install and load packages\n\n2.2.1.1 A. Open RStudio\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nOpening RStudio loads R.\nOpening R will not load RStudio.\n\n\nFigure 2.2 shows an annotated image of RStudio with the four panels labeled. In the default layout, the top-left is the text editor panel, the bottom-left is the console panel, the top-right is the files, plots, and packages panel, and the bottom-right is the environment panel.\n\n\n\n\n\nFigure 2.2: annotated.RStudio\n\n\n\nText Editor Panel - This is where you can enter code and have the editor color code it.\nConsole Panel - This is where errors and warnings appear when you run code. It can also be used to do direct coding, which I don’t recommend for beginners.\nFiles, plots, and packages panels - This is where files loaded in the working directory and packages in the default R directory are organized.\nEnvironment Panel - This is where data and variables you define in your coding will be organized\n\n2.2.1.2 Add a Header\nIt is good coding practice to put a basic header on your script.\nGo to the text editor and type #. Any line in an R script that starts with # is a comment and is not executable code. Lines starting with # will have a unique color.\nI usually add:\n\nName of project\n\nAuthor(s) of project\n\nMonth and Year created\n\nMonth and Year last modified\n\n2.2.1.3 Install and load packages\nType the following code into the text editor. This will download and install the tidyverse package onto your machine. Note that installing packages requires the package name in quotes.\n\ninstall.packages('ggplot2')\n\n\n\n\n\n\n\nPackages only need to be installed once.\n\n\n\nOnce that has completed, you will need to load the library using the library() function. In this call, the package name does not need to be quoted. Every time you open a session where you want to use a package, you need to run this code to load the package.\n\nlibrary(ggplot2)\n\nThe current libraries loaded can be founded in the file manager panel under the Packages tab.\n\n2.2.2 Step 2 - Acquire and/or Load Data\nIn this case we’re going go to do SUPER EZ mode. Acquiring and loading data has lots of detail oriented stuff, that we’re going to skip today to get to the fun stuff. We’ll jump into acquiring data and loading data on Friday.\nThe mpg dataset is an example dataset included in the tidyverse package. No loading required.\nWe can look at the first ten rows of the dataset by typing mpg and running that line of code.\n\nmpg\n\n# A tibble: 234 × 11\n   manufacturer model      displ  year   cyl trans drv     cty   hwy fl    class\n   &lt;chr&gt;        &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt;\n 1 audi         a4           1.8  1999     4 auto… f        18    29 p     comp…\n 2 audi         a4           1.8  1999     4 manu… f        21    29 p     comp…\n 3 audi         a4           2    2008     4 manu… f        20    31 p     comp…\n 4 audi         a4           2    2008     4 auto… f        21    30 p     comp…\n 5 audi         a4           2.8  1999     6 auto… f        16    26 p     comp…\n 6 audi         a4           2.8  1999     6 manu… f        18    26 p     comp…\n 7 audi         a4           3.1  2008     6 auto… f        18    27 p     comp…\n 8 audi         a4 quattro   1.8  1999     4 manu… 4        18    26 p     comp…\n 9 audi         a4 quattro   1.8  1999     4 auto… 4        16    25 p     comp…\n10 audi         a4 quattro   2    2008     4 manu… 4        20    28 p     comp…\n# ℹ 224 more rows\n\n\nThere are some key categorical variables (manufacturer, model, trans, cyl, class, year, drv) and others that are continuous variables (cty, hwy). As you may be able to guess, this data shows automobile average fuel efficiency in units of miles per gallon. We will using this dataset to showcase the grammar of graphical visualization in R.\n\n2.2.3 Step 3. Tidy the Data\nThis dataset is already reasonably tidy and so this step is not necessary for this example dataset. No munging is required.\n\n2.2.4 Step 4. Visualize the Data\nThe package ggplot is the most common graphics package within the tidyverse framework. It is extremely versatile, but requires an understanding of the grammar of graphics. ggplot is loaded as part of the tidyverse package.\n\n2.2.4.1 Choose the visualization type\nWe’ll be exploring point, line, and smoothed visualization types - geom_point, geom_line, ’geom_smooth`. In point plots, the individual data are shown as points. In line plots, individual points are connected by lines. In a smoothed plot, the points are usually shown with a curve attempting to fit the data to a model.\nThere are many more types of visualizations (text, histogram, box, bar, heatmaps, density, jitter, polygons, maps, quantiles, rasters, and violins) available, and we’ll explore the grammar for interesting ones in future classes.\n\n2.2.4.2 Choose the visualization function\n\n\nggplot()\n\ngeom_point()\ngeom_line()\ngeom_smooth()\n\n\n\nWe’ll apply ggplot() for every visualization for now, and add at least one geom function. We will then combine them to make fancier visualizations with overlays.\n\n2.2.4.3 Write the code to do a basic visualization\nCoding in the R tidyverse is a lot like writing a sentence, just in a foreign language that puts things in an order that may not be familiar.\n\nAdd a verb or two for an action - usually this is the function.\nAdd an object to apply the action to - this is usually the dataframe, but can be a list or another type of data object.\nAdd adjectives and adverbs to modify the action or the object\n\nFigure 2.3 shows a very basic visualization.\n\nggplot(data = mpg) +\n  geom_point(aes(x = displ, y = hwy))\n\n\n\n\n\n\nFigure 2.3: Basic visualization\n\n\n\n\nThis is a basic point plot. The x-axis shows engine displacement (Liters) for gasoline vehicles, and the y-axis shows highway driving fuel efficiency in miles per gallon.\nThree functions were used.\n\n\nggplot() - make a figure using mpg as the dataset\n\n\ngeom_point() - shows the data as points\n\n\naes() - aes is an abbreviation for aesthetics; map these variables for display\n\nIn the abstract, a code template for a basic graph is:\n\nggplot(data = &lt;DATA&gt;) + \n  &lt;GEOM_FUNCTION&gt;(mapping = aes(&lt;MAPPINGS&gt;))\n\nFigure 2.4 shows a basic line plot using geom_line instead of points. It is a visual abomination for this dataset. We’ll show why below.\n\nggplot(data = mpg) +\n  geom_line(aes(x = displ, y = hwy))\n\n\n\n\n\n\nFigure 2.4: Basic line visualization\n\n\n\n\nFigure 2.5 shows a smoothed line fit with geom_smooth.\n\nggplot(data = mpg) +\n  geom_smooth(aes(x = displ, y = hwy))\n\n\n\n\n\n\nFigure 2.5: Basic smooth visualization\n\n\n\n\n\n2.2.4.4 Noodle Zone\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.2.4.5 Improve the Visualization\nThe basic visualization is in need of some improvement. First, let’s explore how the dataset looks by adding the aesthetics of color, then shape.\nFigure 2.6 shows the geom_point() plot with vehicle class in different colors. We do this by defining the category color = class within the aes().\n\nggplot(data = mpg) +\n  geom_point(aes(x = displ, y = hwy, color = class))\n\n\n\n\n\n\nFigure 2.6: Basic color visualization\n\n\n\n\nInteresting! The 2seater vehicle class gets better fuel efficiency then the SUV and pickups with similar displacement - likely because they are smaller. I also see that the subcompact and compact vehicle classes have the smallest engine displacement which is correlated with better fuel efficiency.\nFigure 2.7 uses a shape aesthetic instead of color.\n\nggplot(data = mpg) +\n  geom_point(aes(x = displ, y = hwy, shape = class))\n\nWarning: The shape palette can deal with a maximum of 6 discrete values because more\nthan 6 becomes difficult to discriminate\nℹ you have requested 7 values. Consider specifying shapes manually if you need\n  that many have them.\n\n\nWarning: Removed 62 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\nFigure 2.7: Basic shape visualization\n\n\n\n\nNotice anything missing? Our SUV class is gone because ggplot defaults to only allowing six individual shapes at a time. We can override this default.\nThe last new thing I want to show is a facet_wrap() which will make this visualization much easier to interpret on a class basis. Figure 2.8 shows how this works on our basic visualization by adding a line to our basic visualization.\n\nggplot(data = mpg) +\n  geom_point(aes(x = displ, y = hwy)) +\n  facet_wrap(~class)\n\n\n\n\n\n\nFigure 2.8: Basic color visualization\n\n\n\n\nThis helps us to better identify the individual classes of vehicles and understand the range of data available for each type of automobile.\n\n2.2.5 Putting It Together\nThe previous section shows examples for individual changes to our basic visualization. In this section, I’ll show you how easy it is to combine those lines.\n\n2.2.5.1 Example 1: Points and smooth\nFigure 2.9 shows a geom_point and geom_smooth overlaid on each other. We’ve also moved the aes function into the ggplot but could have put in both the geom_point and geom_smooth instead.\n\nggplot(data = mpg, aes(x = displ, y = hwy)) +\n  geom_point() +\n  geom_smooth()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\nFigure 2.9: Basic point and smooth visualization\n\n\n\n\n\n2.2.5.2 Example 2: Points, color, and smooth\nFigure 2.10 shows a geom_point and geom_smooth overlaid on each other but we’ve added the color for vehicle class as well. Unfortunately, the standard error on the smooth function is detracting from the graphic. Figure 2.11 removes that by specifying se = FALSE to the geom_smooth function and it makes the visualization much cleaner.\n\nggplot(data = mpg, aes(x = displ, y = hwy, color = class)) +\n  geom_point() +\n  geom_smooth()\n\n\n\n\n\n\nFigure 2.10: Basic point, color, and smooth visualization\n\n\n\n\n\nggplot(data = mpg, aes(x = displ, y = hwy, color = class)) +\n  geom_point() +\n  geom_smooth(se = FALSE)\n\n\n\n\n\n\nFigure 2.11: Basic point, color, and smooth visualization with standard error removed\n\n\n\n\n\n2.2.5.3 Example 3: Points, color, smooth, and facet\nThis last example will put it all together.\nFigure 2.12 shows a geom_point, geom_smooth, and facet_wrap overlaid on each other with the color for vehicle class as well. This figure combines most of what we’ve explored today in one figure. But we don’t need that legend if we already define each class separately!\nIn Figure 2.13 I remove the legend to make a final figure. This involves a function called theme() which specifies a lot of the meta components of a figure like fonts, legends, and the default look and feel of the figure. Here, I added a line of code that specifies theme(legend.position = 'none') to remove that redundant legend.\n\nggplot(data = mpg, aes(x = displ, y = hwy, color = class)) +\n  geom_point() +\n  geom_smooth(se = FALSE)+\n  facet_wrap(~class)\n\n\n\n\n\n\nFigure 2.12: Point, smooth, facet, and color visualization with standard error removed\n\n\n\n\n\nggplot(data = mpg, aes(x = displ, y = hwy, color = class)) +\n  geom_point() +\n  geom_smooth(se = FALSE)+\n  facet_wrap(~class) + \n  theme(legend.position = 'none')\n\n\n\n\n\n\nFigure 2.13: Point, smooth, facet, and color visualization with Legend and standard error removed\n\n\n\n\n\n2.2.6 In-Class Exercises\n\nCreate a point visualization with cty on the x-axis and hwy on the y-axis.\n\nFit that relationship by adding a geom_smooth()\n\nImprove that visualization by adding class as a color class\nImprove that visualization by adding a facet_wrap by a categorical variable of your choice\n\nFigure 2.14 shows the city-highway fuel efficiency relationship colored by class and faceted by cyl with the ugly gray background removed using theme_bw().\n\n\n\n\n\n\n\nFigure 2.14: Point, smooth, facet, and color visualization example\n\n\n\n\n\n2.2.6.1 Noodle Zone 2",
    "crumbs": [
      "Home",
      "Unit 1: Introduction to Data Visualization",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Coding in R - Basics</span>"
    ]
  },
  {
    "objectID": "maps1.html",
    "href": "maps1.html",
    "title": "3  Introduction to Spatial Visualization - 1",
    "section": "",
    "text": "3.1 Load and Install Packages\nAs a warmup, let’s load the tidyverse. Remember, you can check to make sure a package is loaded in your R session by checking on the files, plots, and packages panel, clicking on the Packages tab, and scrolling down to tidyverse to make sure it is checked.\n#library(tidyverse)",
    "crumbs": [
      "Home",
      "Unit 1: Introduction to Data Visualization",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to Spatial Visualization - 1</span>"
    ]
  },
  {
    "objectID": "maps1.html#load-and-install-packages",
    "href": "maps1.html#load-and-install-packages",
    "title": "3  Introduction to Spatial Visualization - 1",
    "section": "",
    "text": "3.1.1 Geospatial Packages\nR has multiple packages that enable geospatial data visualization. Today, we create basic maps using the sf and leaflet. sf stands for Simple Features which is an open-source geographic information systems data format that works pretty well with tidyverse. Leaflet is an open-source library for mobile-friendly interactive maps.\nAs a reminder, when we first use a package, we need to install it locally. Let’s start by installing both packages. This only needs to be done once.\n\ninstall.packages('sf')\ninstall.packages('leaflet')\n\nNext, we load the packages to make sure they are available within the R environment.\n\nlibrary(sf)\nlibrary(leaflet)\n\nInstalling and loading packages is EZ!",
    "crumbs": [
      "Home",
      "Unit 1: Introduction to Data Visualization",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to Spatial Visualization - 1</span>"
    ]
  },
  {
    "objectID": "maps1.html#acquiring-data",
    "href": "maps1.html#acquiring-data",
    "title": "3  Introduction to Spatial Visualization - 1",
    "section": "\n3.2 Acquiring Data",
    "text": "3.2 Acquiring Data\nWe are going to acquire two datasets for testing today. The first dataset is the nc shapefile dataset that should be installed as part of the sf package. Unlike mpg, this dataset is not directly available in the global environment.\nWe also will import geospatial environmental data set for our test examples. We are going to use a curated version of the CalEnviroscreen4.0 data. The data for the full state of California can be acquired as a zipped esri shapefile .shp. I’ve hosted a smaller version just for SoCal on my gitHub repository for this analysis. Downloading, unzipping, and then importing files is a big part of data science. Unfortunately, it has been my experience that having 15 people all have the same directory and file structures on 15 different machines is fraught with peril. I don’t want to troubleshoot that for an hour in class, so I tried to make the EZ mode code below.\n\n3.2.1 North Carolina shapefile\nWe will read the data in using the sf function st_read and the base R function system.file.\n\n\nst_read() is a specialized function that reads and loads in geospatial data files of various formats.\n\n\nsystem_file() is a function that checks for files that are within system directories that have been loaded as packages. It searches within the package directories on your computer.\n\n\nst_transform() is required to display the polygons properly in leaflet in the WGS84 coordinate reference system.\n\nWe assign the North Carolina data we’re reading into a dataset using the &lt;- operator. The &lt;- operator tells R that the data we are loading should be placed into a dataset that we have named nc. In future functions, nc will access that underlying data within nc.\nLastly, there is the pipe operator |&gt;. These pipes are amazing coding features that transformed coding for me. I cannot express my love for the |&gt; in words alone. It allows existing data or information to be passed to the next line of code after some operation has been done on the data. The pipe operator says, ‘take the output from this line of code, and now do this next thing’. I like to think of them as Super Mario pipes that Mario enters and is transported to another place. For now, take it on faith that this is the most useful operator in the tidyverse and will be littered throughout this course.\n\nnc &lt;- st_read(system.file(\"shape/nc.shp\", package=\"sf\")) |&gt; \n  st_transform(\"+proj=longlat +ellps=WGS84 +datum=WGS84\")\n\nReading layer `nc' from data source \n  `C:\\Users\\MichaelMcCarthy\\AppData\\Local\\R\\cache\\R\\renv\\cache\\v5\\R-4.4\\x86_64-w64-mingw32\\sf\\1.0-16\\ad57b543f7c3fca05213ba78ff63df9b\\sf\\shape\\nc.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 100 features and 14 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -84.32385 ymin: 33.88199 xmax: -75.45698 ymax: 36.58965\nGeodetic CRS:  NAD27\n\n\nWe then use the head() function to print the first 5 rows of the nc dataset.\n\n\n\n\n\n\nPro tip: Always look at the data after import. Importing data is fraught with peril.\n\n\n\n\nhead(nc)\n\nSimple feature collection with 6 features and 14 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -81.74091 ymin: 36.0729 xmax: -75.7728 ymax: 36.58973\nGeodetic CRS:  +proj=longlat +ellps=WGS84 +datum=WGS84\n   AREA PERIMETER CNTY_ CNTY_ID        NAME  FIPS FIPSNO CRESS_ID BIR74 SID74\n1 0.114     1.442  1825    1825        Ashe 37009  37009        5  1091     1\n2 0.061     1.231  1827    1827   Alleghany 37005  37005        3   487     0\n3 0.143     1.630  1828    1828       Surry 37171  37171       86  3188     5\n4 0.070     2.968  1831    1831   Currituck 37053  37053       27   508     1\n5 0.153     2.206  1832    1832 Northampton 37131  37131       66  1421     9\n6 0.097     1.670  1833    1833    Hertford 37091  37091       46  1452     7\n  NWBIR74 BIR79 SID79 NWBIR79                       geometry\n1      10  1364     0      19 MULTIPOLYGON (((-81.47258 3...\n2      10   542     3      12 MULTIPOLYGON (((-81.23971 3...\n3     208  3616     6     260 MULTIPOLYGON (((-80.45614 3...\n4     123   830     2     145 MULTIPOLYGON (((-76.00863 3...\n5    1066  1606     3    1197 MULTIPOLYGON (((-77.21736 3...\n6     954  1838     5    1237 MULTIPOLYGON (((-76.74474 3...\n\n\n\n3.2.2 Calenviroscreen4.0 geoJSON\nThe second dataset is hosted on the github repository where the class materials are version controlled. The code below names the internet URL where the data is stored as URL.path. If you go to the URL link, you will see the raw .geoJSON formatted spatial data file.\nNext, we apply the same st_read() function to read the geoJSON file. geoJSON is a common format for encoding geographic data structures, and st_read() natively knows how to read it in. The st_transform() function again changes the polygons into the WGS84 coordinate reference system.\nThis time, we’ve named the dataset SoCalEJ.\nFinally, we use the head() function on our SoCalEJ dataset to look at the first 5 rows.\n\nURL.path &lt;- 'https://raw.githubusercontent.com/RadicalResearchLLC/EDVcourse/main/CalEJ4/CalEJ.geoJSON'\nSoCalEJ &lt;- st_read(URL.path) |&gt; \n  st_transform(\"+proj=longlat +ellps=WGS84 +datum=WGS84\")\n\nReading layer `CalEJ' from data source \n  `https://raw.githubusercontent.com/RadicalResearchLLC/EDVcourse/main/CalEJ4/CalEJ.geoJSON' \n  using driver `GeoJSON'\nSimple feature collection with 3747 features and 66 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 97418.38 ymin: -577885.1 xmax: 539719.6 ymax: -236300\nProjected CRS: NAD83 / California Albers\n\nhead(SoCalEJ)\n\nSimple feature collection with 6 features and 66 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -117.874 ymin: 33.5556 xmax: -117.7157 ymax: 33.64253\nGeodetic CRS:  +proj=longlat +ellps=WGS84 +datum=WGS84\n       Tract   ZIP County     ApproxLoc TotPop19   CIscore   CIscoreP\n1 6059062640 92656 Orange   Aliso Viejo     3741  9.642007 12.1028744\n2 6059062641 92637 Orange   Aliso Viejo     5376 10.569290 14.3343419\n3 6059062642 92625 Orange Newport Beach     2834  3.038871  0.6807867\n4 6059062643 92657 Orange Newport Beach     7231  6.538151  5.8497226\n5 6059062644 92660 Orange Newport Beach     8487  8.873604 10.4009077\n6 6059062645 92657 Orange Newport Beach     6527  6.033648  4.7402925\n       Ozone   OzoneP     PM2_5  PM2_5_P   DieselPM DieselPM_P  Pesticide\n1 0.05165298 65.36403  9.445785 43.88301 0.14536744   50.13068 0.00000000\n2 0.05219839 66.80772  9.785209 46.89484 0.07372588   27.41755 0.00000000\n3 0.04827750 55.38270 10.433417 52.03485 0.03478566   12.40821 0.06496406\n4 0.04901945 58.23273 10.013395 49.29683 0.03594981   12.90604 0.00000000\n5 0.04863521 56.96329 10.565404 52.63223 0.07701293   28.73678 0.00000000\n6 0.04863521 56.96329 10.329985 51.28811 0.04047264   14.58619 0.00000000\n  PesticideP   Tox_Rel Tox_Rel_P   Traffic TrafficP DrinkWat DrinkWatP\n1    0.00000  293.8420  41.44786  999.0182  57.7125 177.3831  5.907331\n2    0.00000  681.0703  57.65191 1051.9778  61.2250 312.7169 25.939803\n3   22.58621 1557.6400  74.05601  874.8193  49.5000 332.6654 32.284251\n4    0.00000 1349.2300  70.69267 1169.7532  67.4250 332.6654 32.284251\n5    0.00000 1889.5133  78.03201 1383.9867  74.9500 332.6654 32.284251\n6    0.00000 1589.1400  74.44361 1025.2790  59.5750 332.6654 32.284251\n       Lead    Lead_P Cleanup  CleanupP GWThreat GWThreatP HazWaste HazWasteP\n1 18.450498 10.737240     0.0  0.000000      0.0  0.000000    0.280  46.80344\n2  9.898042  3.730309     0.0  0.000000      0.0  0.000000    0.280  46.80344\n3 14.263664  6.830498     4.5 40.836133      2.0 14.311805    0.150  26.67108\n4  5.594984  1.600504     0.0  0.000000      0.5  2.722896    0.210  37.68365\n5 16.777373  9.061122     0.0  0.000000      7.5 39.448780    0.395  60.22502\n6 12.086059  5.343415     0.7  9.593132      2.0 14.311805    0.100  16.63799\n  ImpWatBod ImpWatBodP SolWaste SolWasteP PollBurd PolBurdSc PolBurdP Asthma\n1         7   66.73667        0   0.00000 30.50123  3.724194 18.95457  21.70\n2         7   66.73667        2  52.89805 35.23480  4.302163 29.71998  21.74\n3         8   72.15456        0   0.00000 35.68847  4.357555 30.77785  14.93\n4         6   58.69383        0   0.00000 30.97653  3.782228 19.87554  10.33\n5         2   23.87652        2  52.89805 39.48486  4.821094 41.44368  13.88\n6         3   33.15834        2  52.89805 32.98028  4.026885 24.04480  10.51\n     AsthmaP LowBirtWt   LowBirWP Cardiovas CardiovasP Educatn     EducatP\n1 11.2786640      4.45 37.2337696      9.74 27.5049850     1.0    1.771703\n2 11.3908275      3.50 16.2176033      8.72 18.8310070     8.4   35.876993\n3  3.7263210      1.18  0.2950988      5.41  1.8569292  -999.0 -999.000000\n4  0.9845464      5.39 61.9450860      4.46  0.3988036     2.0    5.859276\n5  2.7542373      2.86  7.6597383      5.43  1.9192423     3.7   14.781068\n6  1.0343968      4.64 42.1991275      4.65  0.4860419     0.0    0.000000\n  Ling_Isol Ling_IsolP Poverty  PovertyP Unempl     UnemplP HousBurd HousBurdP\n1       1.1   7.375829    20.0 34.208543    3.0   17.113483     19.9 62.420786\n2       4.4  33.942347    12.9 16.821608    2.6   11.868818     19.5 60.925222\n3       0.0   0.000000     9.5  8.982412    0.9    1.145237     14.5 35.817490\n4       3.9  30.694275     6.9  4.170854    2.6   11.868818      8.5  8.504436\n5       5.0  37.664095    12.3 15.326633    4.0   30.882353     18.9 58.225602\n6       1.0   6.266071    11.5 13.517588 -999.0 -999.000000     14.8 37.477820\n    PopChar PopCharSc   PopCharP Child_10 Pop_10_64 Elderly65 Hispanic   White\n1 24.958604 2.5890185 13.2375189  10.9864   81.7696    7.2441  16.4662 63.2184\n2 23.683405 2.4567389 11.7750882  13.2812   61.9792   24.7396  22.0238 55.2455\n3  6.722867 0.6973798  0.2647504   5.9280   49.6471   44.4248   5.6104 89.6260\n4 16.664505 1.7286508  4.9167927   7.9657   67.7361   24.2982   4.4530 63.9331\n5 17.743511 1.8405788  5.8119012  10.4866   72.3224   17.1910   8.8488 82.0785\n6 14.444279 1.4983412  3.3787191  11.0311   68.4388   20.5301   3.8302 74.7664\n  AfricanAm NativeAm OtherMult Shape_Leng Shape_Area    AAPI\n1    4.9452   0.0000    3.6087   5604.596    1467574 11.7616\n2    1.3765   0.0000    3.5900   6244.598    2143255 17.7641\n3    0.3881   0.0000    1.4467   6803.947    2048571  2.9287\n4    0.0000   0.3042    6.0987  26448.643   18729420 25.2109\n5    0.0000   0.0000    1.1076  10964.108    4361037  7.9651\n6    0.2911   0.0000    0.8733   9833.066    5754643 20.2390\n                        geometry\n1 MULTIPOLYGON (((-117.7178 3...\n2 MULTIPOLYGON (((-117.7166 3...\n3 MULTIPOLYGON (((-117.8596 3...\n4 MULTIPOLYGON (((-117.7986 3...\n5 MULTIPOLYGON (((-117.8521 3...\n6 MULTIPOLYGON (((-117.8269 3...\n\n\n\n\n\n\n\n\nSometimes the data import messes up at the end of the dataset. A thorough scholar will use the tail function to check the bottom 5 rows as well.\n\n\n\nIn future classes, we may dive into the perilous world of file structures and directories. However, that is unfun, unrewarding, and gruesomely detail oriented, so we’ll avoid it for now.\n\n3.2.3 Creating a New Geospatial Dataset\nIn addition to importing data, sometimes we just to create our own dataset. Let’s demonstrate how to create a very simple data frame in R for the latitude and longitude of a couple different locations we want to put on a map.\nI want to show the location of this classroom at the Redford Conservancy and the neighborhood where I live.\nI can use Google Maps or similar websites to get the latitude and longitude data. Latitude is North and South. Longitude is East and West.\n\nThe Redford Conservancy is at 34.1100576 N and -117.710074 W.\nMy zip code 92508 has a centroid at 33.8895145 N and -117.319014 W.\n\nThe code below will create a data frame which is an R data structure that has rows and columns. A data frame is a generic data object that can store tabular data of many different types.\nWe’ll use two base R functions to create the data frame, c() and data.frame().\n\n\nc() - this function concatenates data. It is used to create a list.\n’data.frame()` - this function turns a list (or lists) into a data.frame type.\n\nWe assign latitude values to a variable named lat, and a longitude values to a variable named lng. We use c() because we have multiple values that we want to include. We combine them into the locations data.frame. Then we check it worked by typing locations.\n\nlat &lt;- c(34.1100576, 33.8895145)\nlng &lt;- c(-117.710074, -117.319014)\n\nlocations &lt;- data.frame(lat, lng) \nlocations\n\n       lat       lng\n1 34.11006 -117.7101\n2 33.88951 -117.3190\n\n\n\n3.2.3.1 Exercise 1\n\nFind another location’s latitude and longitude coordinates that you want to add to a map.\nHack the code to add your location of interest to the two existing locations data.frame.\nDisplay the output of the locations data.frame to show that your code worked!",
    "crumbs": [
      "Home",
      "Unit 1: Introduction to Data Visualization",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to Spatial Visualization - 1</span>"
    ]
  },
  {
    "objectID": "maps1.html#visualize-the-data---geospatial-edition",
    "href": "maps1.html#visualize-the-data---geospatial-edition",
    "title": "3  Introduction to Spatial Visualization - 1",
    "section": "\n3.3 Visualize the Data - Geospatial Edition",
    "text": "3.3 Visualize the Data - Geospatial Edition\n\n3.3.1 Choose the visualization type\nWe’ll be exploring leaflet() maps for our visualizations. Leaflet has a number of in-built display functions for geospatial data that make things look really cool. I will note that ggplot can make maps too with geom_sf() and other functionality, but I prefer the leaflet tiles for interactive maps as a superior visualization tool.\n\n3.3.2 Choose the visualization function\n\n\nleaflet()\n\naddTiles()\naddMarkers()\naddPolygons()\naddLegends()\n\n\n\nWe’ll apply leaflet() to every visualization for geospatial data today, and add at least one add function to display spatial information.\n\n3.3.3 Write the code for a basic geospatial visualization\nLeaflet is a super cool package for making interactive maps with very few lines of code. As before let’s start with basics and then iterate.\nNote that Leaflet has a different style and coding aesthetic than ggplot, so some of the syntax and grammar is a bit different. However, the steps are the same as for ggplot.\n\nAdd a verb or two for an action - usually this is the function.\nAdd an object to apply the action to - this is usually the dataframe, but can be a list or another type of data object.\nAdd adjectives and adverbs to modify the action or the object\n\n\n3.3.3.1 Example 1 - Basic Tile Map\nFigure 3.2 shows the standard leaflet map with a minimal reproducible example. This map shows the whole world in a Mercator projection. The map is interactive, just like most of the maps you come across in standard apps and websites on the modern internet. You can zoom in and see the details of an area like Los Angeles, with all the annotation and standard roads, forests, city names, etc.\nThe code is simply two lines with two actionable functions.\n\n\nleaflet() - this function identifies the type of visualization.\n\naddTiles() - this function tells the leaflet map to display the default visualization ‘tile’. Tiles are the way geospatial data are rendered depending on the zoom level that makes it work for not displaying too much detail as you zoom out.\n\n\nleaflet() |&gt; \n  addTiles()\n\n\n\n\n\n\nFigure 3.2: A very basic Leaflet map\n\n\n\nA map is useful by itself, but we have not added any information to it. A map alone is not going to be useful as a visualization unless we add some data.\n\n3.3.3.2 Example 2. Tile Map with Location Markers\nRemember that locations data frame we created earlier. Let’s add that to the map.\naddMarkers() is a useful function to add point data to a map.\nFigure 3.3 shows the map with my two locations on it added as markers. We added another line of code with a pipe operator to put the locations data on the map.\n\nleaflet() |&gt; \n  addTiles() |&gt; \n  addMarkers(data = locations)\n\n\n\n\n\n\nFigure 3.3: Leaflet map with locations\n\n\n\nWhoot! A map with locations! And now the spatial extent of the map defaults to just show an area that encompasses the markers in the locations dataset. If your location that you added to the dataset happens to be outside California, your map will be zoomed way out compared to someone who only includes locations in SoCal.\n\n\n\n\n\n\nI purposely made the column names lat and lng because then we don’t have to assign those variables in the addMarkers call. If your columns are named something else (e.g., Lati or long), leaflet will need to be told that those are the correct columns to look for.\n\n\n\n\n3.3.3.3 Example 3. North Carolina Counties on a Tile Map\nGoing beyond point locations, geospatial data really shines when we display shapes. We’ll start by displaying the nc dataframe.\naddPolygons() is the function used to display polygons.\nFigure 3.4 shows the North Carolina counties.\n\nleaflet() |&gt; \n  addTiles() |&gt; \n  addPolygons(data = nc)\n\n\n\n\n\n\nFigure 3.4: Leaflet map with North Carolina Counties\n\n\n\nWe have done it!\n\n3.3.3.4 Exercise 2\n\nMake a leaflet map with the SoCalEJ dataset. It should look like Figure 3.5.\n\n\n\n\n\n\n\n\nFigure 3.5: Leaflet map with Calenviroscreen 4.0\n\n\n\n\n3.3.4 Improve the Visualization\nThe basic visualization is in need of some improvement. First let’s explore colors, fillcolors, stroke, and opacity.\nWithin the addPolygons() function, there are many options that can be modified to alter the output from the default settings. We won’t cover all these options today.\n\ncolor\nweight\nopacity\nfillColor\nfillOpacity\nstroke\ngroup\nlabel\n\nLet’s iterate and make some improvements!\nFigure 3.6 shows what happens we just change the color from blue to black. Both the fill and line color are changed.\n\nleaflet() |&gt; \n  addTiles() |&gt; \n  addPolygons(data = nc, color = 'black')\n\n\n\n\n\n\nFigure 3.6: Leaflet map with North Carolina Counties and color changed to black\n\n\n\nThe lines are too heavy. Let’s lower the weight - we’ll let the color go back to blue to keep the example minimal.\nFigure 3.7 shows the result, which is a much cleaner look.\n\nleaflet() |&gt; \n  addTiles() |&gt; \n  addPolygons(data = nc, weight = 1)\n\n\n\n\n\n\nFigure 3.7: Leaflet map for North Carolina Counties with line weight changed to 1.\n\n\n\nMuch better!\nWithin the North Carolina dataset are some numerical values by county. Let’s use fillcolor the counties to indicate the values.\nUnfortunately, leaflet requires us to do a bit of coding on our color palette.\nFunctions that help define the color palette in leaflet are\n\n\ncolorNumeric() - a linear color scale\n\ncolorQuantile() - a color scale that makes sure the bin sizes are approximately equal\n\ncolorFactor() - a color scale that is good for categorical (‘non-numeric’) data\n\nThis creates a numeric color palette for the BIR79 category of data for North Carolina in a numeric and quantile format. I have chosen the YlGn palette from Figure 1.8.\n\npal1 &lt;- colorNumeric(palette = 'YlGn', domain = nc$BIR79)\npal2 &lt;- colorQuantile(palette = 'YlGn', domain = nc$BIR79, n = 5)\n\nNow we see what those two palettes look like in the map. Figure 3.8 shows the default numeric palette, but it is too faint with the background tiles.\n\nleaflet() |&gt; \n  addTiles() |&gt; \n  addPolygons(data = nc,\n              weight = 1,\n              fillColor = ~pal1(BIR79))\n\n\n\n\n\n\nFigure 3.8: Leaflet map for North Carolina Counties with numeric palette for BIR79 and line weight = 1.\n\n\n\nIncreasing the fillOpacity to 0.8 will help us to see the data much better. Also, let’s remove the lines completely by setting stroke to FALSE. Figure 3.9 shows\n\nleaflet() |&gt; \n  addTiles() |&gt; \n  addPolygons(data = nc,\n              stroke = FALSE,\n              fillColor = ~pal1(BIR79),\n              fillOpacity = 0.8)\n\n\n\n\n\n\nFigure 3.9: Leaflet map for North Carolina Counties with numeric palette for BIR79, increasing opacity, removing lines.\n\n\n\nThat has a much better pop and we can really see the county differences in the high and low population areas.\nFigure 3.10 shows the quantile binned palette pal2. Now the number of counties in each color bin is about the same, which helps to sort the differences among the lowest population counties.\n\nleaflet() |&gt; \n  addTiles() |&gt; \n  addPolygons(data = nc, \n              stroke = FALSE,\n              fillColor = ~pal2(BIR79),\n              fillOpacity = 0.8)\n\n\n\n\n\n\nFigure 3.10: Leaflet map for North Carolina Counties with quantile palette for BIR79, opacity optimized, and lines removed.\n\n\n\n\n3.3.5 Exercise 3\n\nSelect a different color palette from Figure 1.8 to replace ‘YlGn’ in pal1.\nGenerate a North Carolina map with your choice of color palette for the BIR79 category.\n\n\n3.3.5.1 Adding a Legend\nThe existing map is not bad, but the color scale is not self-explanatory. We need to add a legend so a user can understand the data scale.\naddLegend() will provides that functionality.\nFigure 3.11 shows the legend overlay with the map. Note that we need to either move the data = nc into the initial call to leaflet() or define it separately in both the addPolygons and addLegend functions.\nThe addLegend polygon require the inputs for pal and values. Defining the color palette pal1 and the values of the scale ~BIR79 are required. The title is optional, but the scale doesn’t make much sense without a description.\n\nleaflet(data = nc) |&gt; \n  addTiles() |&gt; \n  addPolygons(stroke = FALSE,\n              fillColor = ~pal1(BIR79),\n              fillOpacity = 0.8) |&gt; \n  addLegend(pal = pal1, \n            title = 'Births in 1979', \n            values = ~BIR79)\n\n\n\n\n\n\nFigure 3.11: Leaflet map for North Carolina Counties with numeric palette for BIR79, color legend, opacity optimized, and lines removed.\n\n\n\n\n3.3.5.2 Noodle Zone\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.3.5.3 Exercise 4\n\nCreate a colorPalette for a SoCalEJ dataset variable. Choose one of the following numerical variables and a colorPalette from Figure 1.8\n\n\n\nPoverty\nHispanic\nAfricanAm\nOzone\nDieselPM_P\n\n\nCreate a map for the SoCalEJ dataset with census tracts color coded for your chosen category of the following categories:\nAdd a Legend to the map\n\nThat map might look like Figure 13.3.\n\npalDPM &lt;- colorNumeric(palette = 'YlOrBr', domain = SoCalEJ$DieselPM_P, n = 5)\n\nleaflet(data = SoCalEJ) |&gt; \n  addTiles() |&gt; \n  setView(lat = 34, lng = -117.60, zoom = 9) |&gt; \n  addPolygons(stroke = FALSE,\n              fillColor = ~palDPM(DieselPM_P),\n              fillOpacity = 0.8) |&gt; \n  addLegend(pal = palDPM, \n            title = 'Diesel Particulate Matter (%)', \n            values = ~DieselPM_P)\n\n\n\n\n\n\nFigure 3.12: Leaflet map for Diesel PM percentile in Southern California",
    "crumbs": [
      "Home",
      "Unit 1: Introduction to Data Visualization",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction to Spatial Visualization - 1</span>"
    ]
  },
  {
    "objectID": "maps2.html",
    "href": "maps2.html",
    "title": "4  Introduction to Spatial Visualization 2",
    "section": "",
    "text": "4.1 Load and Install Packages\nLoad ggplot2 and sf packages. Today we will be making static maps in ggplot2 which is part of the tidyverse ecosystem. We need the sf package to load, transform, and display geospatial data.\nlibrary(ggplot2)\nlibrary(sf)\n\nLinking to GEOS 3.12.1, GDAL 3.8.4, PROJ 9.3.1; sf_use_s2() is TRUE",
    "crumbs": [
      "Home",
      "Unit 1: Introduction to Data Visualization",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Introduction to Spatial Visualization 2</span>"
    ]
  },
  {
    "objectID": "maps2.html#acquire-data",
    "href": "maps2.html#acquire-data",
    "title": "4  Introduction to Spatial Visualization 2",
    "section": "\n4.2 Acquire Data",
    "text": "4.2 Acquire Data\nImport the nc and SoCalEJ datasets again. Refer to Chapter 3 for details on functions.\n\nnc &lt;- st_read(system.file(\"shape/nc.shp\", package=\"sf\")) |&gt; \n  st_transform(\"+proj=longlat +ellps=WGS84 +datum=WGS84\")\n\nReading layer `nc' from data source \n  `C:\\Users\\MichaelMcCarthy\\AppData\\Local\\R\\cache\\R\\renv\\cache\\v5\\R-4.4\\x86_64-w64-mingw32\\sf\\1.0-16\\ad57b543f7c3fca05213ba78ff63df9b\\sf\\shape\\nc.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 100 features and 14 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -84.32385 ymin: 33.88199 xmax: -75.45698 ymax: 36.58965\nGeodetic CRS:  NAD27\n\ntail(nc)\n\nSimple feature collection with 6 features and 14 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -79.07426 ymin: 33.88212 xmax: -76.28737 ymax: 35.01676\nGeodetic CRS:  +proj=longlat +ellps=WGS84 +datum=WGS84\n     AREA PERIMETER CNTY_ CNTY_ID        NAME  FIPS FIPSNO CRESS_ID BIR74 SID74\n95  0.125     2.868  2156    2156    Carteret 37031  37031       16  2414     5\n96  0.225     2.107  2162    2162      Bladen 37017  37017        9  1782     8\n97  0.214     2.152  2185    2185      Pender 37141  37141       71  1228     4\n98  0.240     2.365  2232    2232    Columbus 37047  37047       24  3350    15\n99  0.042     0.999  2238    2238 New Hanover 37129  37129       65  5526    12\n100 0.212     2.024  2241    2241   Brunswick 37019  37019       10  2181     5\n    NWBIR74 BIR79 SID79 NWBIR79                       geometry\n95      341  3339     4     487 MULTIPOLYGON (((-77.14865 3...\n96      818  2052     5    1023 MULTIPOLYGON (((-78.26123 3...\n97      580  1602     3     763 MULTIPOLYGON (((-78.02565 3...\n98     1431  4144    17    1832 MULTIPOLYGON (((-78.65546 3...\n99     1633  6917     9    2100 MULTIPOLYGON (((-77.96045 3...\n100     659  2655     6     841 MULTIPOLYGON (((-78.65546 3...\n\n\n\nURL.path &lt;- 'https://raw.githubusercontent.com/RadicalResearchLLC/EDVcourse/main/CalEJ4/CalEJ.geoJSON'\nSoCalEJ &lt;- st_read(URL.path) |&gt; \n  st_transform(\"+proj=longlat +ellps=WGS84 +datum=WGS84\")\n\nReading layer `CalEJ' from data source \n  `https://raw.githubusercontent.com/RadicalResearchLLC/EDVcourse/main/CalEJ4/CalEJ.geoJSON' \n  using driver `GeoJSON'\nSimple feature collection with 3747 features and 66 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 97418.38 ymin: -577885.1 xmax: 539719.6 ymax: -236300\nProjected CRS: NAD83 / California Albers\n\ntail(SoCalEJ)\n\nSimple feature collection with 6 features and 66 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -118.2062 ymin: 33.96317 xmax: -117.9014 ymax: 34.16102\nGeodetic CRS:  +proj=longlat +ellps=WGS84 +datum=WGS84\n          Tract   ZIP      County    ApproxLoc TotPop19  CIscore CIscoreP\n3742 6037400602 91702 Los Angeles        Azusa     4250 32.63778 63.50227\n3743 6037430302 91016 Los Angeles     Monrovia     5339 17.12483 30.61019\n3744 6037430723 91007 Los Angeles      Arcadia     4365 13.84199 22.56682\n3745 6037431100 91016 Los Angeles     Monrovia     6758 39.69785 74.50832\n3746 6037533603 90201 Los Angeles         Bell     6986 62.93104 97.04992\n3747 6037534101 90201 Los Angeles Bell Gardens     2358 63.31505 97.22642\n          Ozone   OzoneP    PM2_5  PM2_5_P   DieselPM DieselPM_P Pesticide\n3742 0.06236471 88.69944 11.32762 58.23273 0.25141035   70.16801   0.00000\n3743 0.06236471 88.69944 11.87334 72.10952 0.03484052   12.43311   0.00000\n3744 0.05938691 79.98755 11.81607 70.24269 0.05856793   21.30678   0.00000\n3745 0.06133785 84.57996 11.89265 72.90604 0.52993389   90.91475   0.00000\n3746 0.04632538 46.99440 12.01973 79.45240 0.11636699   42.26509   0.00000\n3747 0.04716458 50.54138 12.02588 79.77598 0.89400714   97.51089  25.17289\n     PesticideP  Tox_Rel Tox_Rel_P   Traffic TrafficP DrinkWat DrinkWatP\n3742    0.00000 1128.208  66.72918  515.5913  22.0875 365.8082  37.91682\n3743    0.00000 1235.359  69.21730  471.5739  18.7625 570.8214  66.86649\n3744    0.00000 1324.937  70.43011 1003.0337  57.9875 667.6980  73.14850\n3745    0.00000 1575.716  74.35609 1560.1642  79.2625 497.7552  60.33471\n3746    0.00000 4435.870  91.04776 1071.6749  62.4000 639.0632  71.36256\n3747   67.58621 4409.633  90.89772 3653.6009  98.0750 514.1607  61.83340\n         Lead   Lead_P Cleanup  CleanupP GWThreat GWThreatP HazWaste HazWasteP\n3742 59.07638 64.54946    3.00 31.243001    46.85  89.10842    0.460  66.60490\n3743 71.04484 80.34026    1.40 19.914147     1.80  11.30666    0.320  52.64064\n3744 50.87304 52.71582    0.40  5.636432     5.00  30.88162    0.150  26.67108\n3745 54.59725 57.99622   43.50 94.438223    36.75  84.14411    1.885  92.15089\n3746 87.11610 94.82042   14.65 74.300112    17.05  63.42354    0.375  58.80874\n3747 90.85649 97.39130   20.35 82.232176    15.00  59.60485    1.110  86.60490\n     ImpWatBod ImpWatBodP SolWaste SolWasteP PollBurd PolBurdSc PolBurdP Asthma\n3742         0    0.00000     0.00  0.000000 46.49568  5.677114 60.95831  71.92\n3743         3   33.15834     0.25 11.592211 42.60985  5.202654 50.36714  48.33\n3744         0    0.00000     0.00  0.000000 39.69752  4.847059 42.02862  18.84\n3745         2   23.87652     8.75 87.949599 68.86648  8.408583 97.88426  46.90\n3746         7   66.73667     2.00 52.898053 61.77303  7.542474 93.03049  54.25\n3747         7   66.73667     0.20  9.667812 73.95742  9.030186 99.39017  52.71\n       AsthmaP LowBirtWt  LowBirWP Cardiovas CardiovasP Educatn  EducatP\n3742 79.847956      5.09 54.849885     17.02   76.34596    14.1 53.40420\n3743 53.950648      2.72  6.209905     11.22   39.70588     6.4 27.61326\n3744  7.614656      3.48 15.871183      7.73   11.83948     4.5 18.80537\n3745 51.856929      3.91 24.570182     10.87   36.88933    16.0 57.61832\n3746 62.076271      5.87 72.478830     23.93   96.68495    42.6 91.88813\n3747 59.895314      2.87  7.775212     23.15   95.45115    46.0 94.25462\n     Ling_Isol Ling_IsolP Poverty PovertyP Unempl   UnemplP HousBurd HousBurdP\n3742       5.6   40.89863    24.0 42.61307    2.9 15.838105     17.1  49.72117\n3743       7.0   48.72107    18.9 31.73367    1.2  1.900052     15.4  40.89987\n3744      13.6   73.09514    23.2 41.03015    4.0 30.882353     17.7  52.61090\n3745      10.1   62.24117    32.2 57.29899    6.2 59.383134     13.3  29.72117\n3746      21.7   88.69942    59.9 91.55779    9.1 82.326913     20.4  64.46134\n3747      11.7   67.72229    56.1 88.31658    9.3 83.224883     22.0  70.53232\n      PopChar PopCharSc PopCharP Child_10 Pop_10_64 Elderly65 Hispanic   White\n3742 55.42148  5.749009 58.49723  13.3882   74.3294   12.2824  68.1412 20.7294\n3743 31.73120  3.291557 22.33989   9.7584   72.7664   17.4752  28.7132 53.3995\n3744 27.52994  2.855750 16.50277  12.0504   77.4570   10.4926  10.9507 26.3918\n3745 45.51235  4.721110 43.58296  11.2607   81.4442    7.2951  58.2273 16.1438\n3746 80.43337  8.343554 92.93999  18.4226   72.1586    9.4188  91.4114  6.9425\n3747 67.59201  7.011489 76.72718  14.8431   75.0636   10.0933  91.0941  1.3147\n     AfricanAm NativeAm OtherMult Shape_Leng Shape_Area    AAPI\n3742    2.0941   0.3529    0.8941   6661.381  1537875.2  7.7882\n3743    1.5733   0.0000    7.1549   7166.131  1938015.8  9.1590\n3744    3.3677   0.0000    3.3677   3941.782   485563.0 55.9221\n3745    8.9967   0.0000    1.1098   8020.091  3015660.7 15.5223\n3746    0.6728   0.2577    0.7157   4949.117   811895.5  0.0000\n3747    1.9084   0.0000    0.0000   4420.127   509871.8  5.6828\n                           geometry\n3742 MULTIPOLYGON (((-117.9024 3...\n3743 MULTIPOLYGON (((-117.9917 3...\n3744 MULTIPOLYGON (((-118.0496 3...\n3745 MULTIPOLYGON (((-117.9987 3...\n3746 MULTIPOLYGON (((-118.1874 3...\n3747 MULTIPOLYGON (((-118.1636 3...\n\n\n\n4.2.1 Create a Locations Table\nI want to show the location of this classroom and my neighborhood. Let’s add locations again.\n\nlat &lt;- c(34.1100576, 33.8895145)\nlng &lt;- c(-117.710074, -117.319014)\n\nlocations &lt;- data.frame(lat, lng) \nlocations\n\n       lat       lng\n1 34.11006 -117.7101\n2 33.88951 -117.3190",
    "crumbs": [
      "Home",
      "Unit 1: Introduction to Data Visualization",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Introduction to Spatial Visualization 2</span>"
    ]
  },
  {
    "objectID": "maps2.html#visualize-the-data---geospatial-ggplot-edition",
    "href": "maps2.html#visualize-the-data---geospatial-ggplot-edition",
    "title": "4  Introduction to Spatial Visualization 2",
    "section": "\n4.3 Visualize the Data - Geospatial ggplot Edition",
    "text": "4.3 Visualize the Data - Geospatial ggplot Edition\nWhile the tidyverse doesn’t have all the features of leaflet, it can be a quick way to visualize geospatial data for static maps and there are times when adding a static ggplot map is sufficient and actually preferred to the more detailed leaflet maps.\nLet’s do a few example ggplot maps.\n\n4.3.1 Visualization functions\n\n\nggplot()\n\n\ngeom_sf() - display sf spatial data\n\ntheme_bw() - a cleaner background and visualization than default gray background for ggplot\n\n\ntheme_minimal() - minimalist theme\n\n\n\n4.3.2 Make a basic visualization\nStart with the North Carolina data and make a basic ggplot and geom_sf map as shown in Figure 4.2.\n\nggplot(data = nc) + \n  geom_sf()\n\n\n\n\n\n\nFigure 4.2: ggplot map for North Carolina counties\n\n\n\n\nNote that ggplot uses + rather than the magrittr |&gt; for connecting lines of code.\nIn contrast to the leaflet map, the ggplot defaults to showing the x- and y-axis coordinates (latitude and longitude), shows guidelines, and only draws the counties in the dataset, rather than defaulting to showing an interactive map.\nFigure 4.3 shows the same style of map replacing nc with SoCalEJ.\n\nggplot() + \n  geom_sf(data = SoCalEJ) \n\n\n\n\n\n\nFigure 4.3: ggplot map for census tracts in Inland SoCal counties\n\n\n\n\n\n4.3.3 Improve the Visualization\nWe have many options to improve a ggplot visualization. Let’s start by cleaning up the background using theme_bw(). theme_bw() changes the background from gray to a cleaner black-white style as shown in Figure 4.4.\n\nggplot() + \n  geom_sf(data = nc) +\n  theme_bw()\n\n\n\n\n\n\nFigure 4.4: ggplot map for North Carolina counties using theme_bw()\n\n\n\n\nWe can apply a minimalist aesthetic by choosing theme_minimal() as shown in Figure 4.5\n\nggplot() + \n  geom_sf(data = nc) +\n  theme_minimal()\n\n\n\n\n\n\nFigure 4.5: ggplot map for North Carolina counties using theme_bw()\n\n\n\n\nLet’s add colors in a ggplot way.\nUse aes(fill = &lt;VARIABLE NAME&gt;) to assign a category to color the counties by. The color palette to fill with is selected in scale_fill_&lt;TYPE&gt; where TYPE can be any of the following categories\n\nbinned\nbrewer\ncontinuous\ndate or datetime\ndiscrete\nfermenter\nviridis\n\nFirst let’s use the default palette for BIR79 to show the county birthrates in 1979. Adding fill = BIR79 to the aes() defaults to the Blues palette. Figure 4.6 shows the result of adding a fill color scale.\n\nggplot() + \n  geom_sf(data = nc, aes(fill = BIR79)) +\n  theme_bw() \n\n\n\n\n\n\nFigure 4.6: ggplot map for North Carolina counties using theme_bw() with fill\n\n\n\n\nLet’s change that to a viridis color scale. The function scale_fill_viridis_c() adds a fancier color-blind viridis palette in a continuous scale. Figure 4.7 shows this color scale option.\n\nggplot() + \n  geom_sf(data = nc, aes(fill = BIR79)) +\n  theme_bw() +\n  scale_fill_viridis_c()\n\n\n\n\n\n\nFigure 4.7: ggplot map for North Carolina counties using theme_bw() with viridis\n\n\n\n\nWe can also add other geoms, like points or labels to this map. Let’s try to label the counties.\nThe nc dataset has a variable called NAME for the county names. Figure 4.8 shows the figure when we add the county names using the function geom_sf_text().\n\nggplot(data = nc) + \n  geom_sf(aes(fill = BIR79)) +\n  geom_sf_text(aes(label = NAME), size = 1.5, color = 'white') +\n  theme_bw() +\n  scale_fill_viridis_c()\n\nWarning in st_point_on_surface.sfc(sf::st_zm(x)): st_point_on_surface may not\ngive correct results for longitude/latitude data\n\n\n\n\n\n\n\nFigure 4.8: ggplot map for North Carolina counties with names overlaid\n\n\n\n\nThere is a lot going on in that function. I made the text white color = ‘white’, the size of the font 1.5 size = 1.5, and added the label aesthetic with aes(label = NAME). If you remove the size or the color, you can see why those alterations were made.\n\n4.3.3.1 Exercise - Improve the SoCalEJ Visualization\n\nAdd a different theme from one of the theme options\n\nShow a variable (categorical, continuous, or quantile) using a fill option.\nAdd two or more SoCal locations to the map using geom_point and your locations table. If that is easy, try increasing the salience of the points through size, color, or shape modifications to that layer.\n\nFigure 4.9 shows a potential example of what that might look like.\n\n\n\n\n\n\n\nFigure 4.9: ggplot map for SoCal Diesel PM Percentile\n\n\n\n\nIt is really hard to see the details here. Let’s learn one last trick to zoom in on a ggplot to adjust the axes. The scale_x_continuous() and scale_y_continuous() functions allow us to set different axis limits. Figure 4.10 shows the\n\nggplot() + \n  geom_sf(data = SoCalEJ, aes(fill = DieselPM_P)) +\n  geom_point(data = locations,  aes(x = lng, y = lat), color = 'orange') +\n  theme_bw() +\n  scale_fill_viridis_c(option = 'A', direction = -1) +\n  scale_x_continuous(limits = c(-118, -117)) +\n  scale_y_continuous(limits = c(33.7, 34.2))\n\n\n\n\n\n\nFigure 4.10: ggplot map for SoCal Diesel PM Percentile zoomed into Claremont and Ontario.\n\n\n\n\n\n4.3.3.2 Noodle Zone\nOnly includes nc data at this point. Can add SoCalEJ if needed.",
    "crumbs": [
      "Home",
      "Unit 1: Introduction to Data Visualization",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Introduction to Spatial Visualization 2</span>"
    ]
  },
  {
    "objectID": "chartJunk.html",
    "href": "chartJunk.html",
    "title": "5  Chartjunk, Data-Ink Ratios, and Visualization Theory",
    "section": "",
    "text": "5.1 Chartjunk\nChart Junk is a term first used by Edward Tufte in his book The Quantitative Display of Visual Information. He defined it as:\nIn other words, Tufte believes that embellishment, decoration, and ornamentation is typically bad in a data visualization. While I can talk about this, it is better to just show examples of chartjunk.\nJust do an image search for chartjunk in your browser. Here’s a few examples.\nFigure 5.1 shows a very simple bar chart with lots of colors, patterns, and\nFigure 5.2 shows a price line chart embellished with a decorative reclining lady. This is what Tufte calls a Duck - elevating design over data.\nFigure 5.3 shows a pie chart of video analysis by medical professionals. Only three values are shown, yet there is a flourish of colors, cartoons, clipart, and embellishment.\nTufte was pretty crufty about anything that was not minimalist. He is an pro-modernist design and anti-baroque design. And there is some research that suggests more ornamented and interesting visualizations stick with people longer than minimal designs.",
    "crumbs": [
      "Home",
      "Unit 1: Introduction to Data Visualization",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Chartjunk, Data-Ink Ratios, and Visualization Theory</span>"
    ]
  },
  {
    "objectID": "chartJunk.html#chartjunk",
    "href": "chartJunk.html#chartjunk",
    "title": "5  Chartjunk, Data-Ink Ratios, and Visualization Theory",
    "section": "",
    "text": "The interior decoration of graphics generates a lot of ink that does not tell the viewer anything new. The purpose of decoration varies-to make the graphic appear more scientific and precise, to enliven the display, to give the designer an opportunity to exercise artistic skills. Regardless of its cause it is all non-data-ink or redundant data-ink, and it is often chartjunk.\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 5.1: Mario looking bar chart\n\n\n\n\n\n\n\n\n\n\nFigure 5.2: Infamous diamonds line\n\n\n\n\n\n\n\n\n\n\nFigure 5.3: Pie Chartjunk",
    "crumbs": [
      "Home",
      "Unit 1: Introduction to Data Visualization",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Chartjunk, Data-Ink Ratios, and Visualization Theory</span>"
    ]
  },
  {
    "objectID": "chartJunk.html#data-ink-ratios",
    "href": "chartJunk.html#data-ink-ratios",
    "title": "5  Chartjunk, Data-Ink Ratios, and Visualization Theory",
    "section": "5.2 Data-Ink Ratios",
    "text": "5.2 Data-Ink Ratios\nThe second Tufte-ism is the ratio of data-ink. This is a quantitative measure indicating the amount of ‘ink’ used to convey data/information in a visualization. Any ‘ink’ not conveying information is considered superfluous and redundant.\n\n\n\nData Ink Ratio\n\n\nA simple example from the tidyverse would be to compare the default theme for a ggplot() with theme_bw() or theme_minimal().\nHere’s Figure 4-2 with the default theme. Notice all that background ‘ink’ in gray.\nTufte would definitely prefer Figure 45-5 where we removed all that background and even the frame around the outside of the map.\nThis is an aesthetic preference, especially in the modern era where almost all of the visualization we engage with is on a computer/tablet/phone. There is no amount of pixel-ink that is consumed. And in some cases the brightness of a white background may be antithetical and harmful, such as in the ProPublica Sacrifice Zone visualizations as shown in Figure 17.2.",
    "crumbs": [
      "Home",
      "Unit 1: Introduction to Data Visualization",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Chartjunk, Data-Ink Ratios, and Visualization Theory</span>"
    ]
  },
  {
    "objectID": "chartJunk.html#what-makes-a-map-in-leaflet-bad",
    "href": "chartJunk.html#what-makes-a-map-in-leaflet-bad",
    "title": "5  Chartjunk, Data-Ink Ratios, and Visualization Theory",
    "section": "5.3 What makes a map in leaflet bad?",
    "text": "5.3 What makes a map in leaflet bad?\nMost of the teams are using leaflet maps. What things in leaflet look bad and don’t work?\nDiscuss",
    "crumbs": [
      "Home",
      "Unit 1: Introduction to Data Visualization",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Chartjunk, Data-Ink Ratios, and Visualization Theory</span>"
    ]
  },
  {
    "objectID": "chartJunk.html#class-exercise---work-on-group-visualization",
    "href": "chartJunk.html#class-exercise---work-on-group-visualization",
    "title": "5  Chartjunk, Data-Ink Ratios, and Visualization Theory",
    "section": "5.4 Class Exercise - work on group visualization",
    "text": "5.4 Class Exercise - work on group visualization\nHere’s a framework for the final group visualization from the Junk Charts Blog\n\nWhat is the practical question?\nWhat does the data you have say about the question?\nWhat do the individual visualizations say?\n\nI would add:\n\nWho is the audience for the visualization?",
    "crumbs": [
      "Home",
      "Unit 1: Introduction to Data Visualization",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Chartjunk, Data-Ink Ratios, and Visualization Theory</span>"
    ]
  },
  {
    "objectID": "assignment1.html",
    "href": "assignment1.html",
    "title": "6  Assignments for Unit 1",
    "section": "",
    "text": "6.1 Graded Assignment - Visualization Critique Paper\nChoose a environmental data visualization. This can include static figures, interactive graphics, infographics, apps, videos, animations, etc. Please get approval from Mike if you are choosing something that may not meet a standard definition of an environmental data visualization.\nWrite a two-page paper (12-point font or less) describing the visualization and salient features that you think are interesting and noteworthy. Include a picture or link to your chosen visualization (does not count towards 2-page minimum). In your critique, please include ten individual points that fit into at least one of these categories. Include at least 3 of these categories in your paper.\nAssignment is due September 5, 2024 at the beginning of class (9:45 AM). Assignments can be emailed or physically turned in. This assignment is worth 150 points. Spelling, grammar, and sentence structure will be a minor component of the score (~10 points).",
    "crumbs": [
      "Home",
      "Unit 1: Introduction to Data Visualization",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Assignments for Unit 1</span>"
    ]
  },
  {
    "objectID": "assignment1.html#graded-assignment---visualization-critique-paper",
    "href": "assignment1.html#graded-assignment---visualization-critique-paper",
    "title": "6  Assignments for Unit 1",
    "section": "",
    "text": "Things that you like, or think are done well\nThings that you dislike, or think are done poorly\nImprovements you would make to the visualization (can be additions or subtractions from features in the visualization)\nFeatures that provide interesting structure to the data and make it more informative\nFeatures that you find aid in communicating knowledge or help tell the story.",
    "crumbs": [
      "Home",
      "Unit 1: Introduction to Data Visualization",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Assignments for Unit 1</span>"
    ]
  },
  {
    "objectID": "EJtheory.html",
    "href": "EJtheory.html",
    "title": "7  EJ - Screening Tools",
    "section": "",
    "text": "7.1 Definitions",
    "crumbs": [
      "Home",
      "Unit 2: Environmental Justice",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>EJ - Screening Tools</span>"
    ]
  },
  {
    "objectID": "EJtheory.html#definitions",
    "href": "EJtheory.html#definitions",
    "title": "7  EJ - Screening Tools",
    "section": "",
    "text": "Environmental Protection Agency and California EPA - The fair treatment and meaningful involvement of all people regardless of race, color, culture, national origin, income, and educational levels with respect to the development, implementation, and enforcement of protective environmental laws, regulations, and policies. Fair treatment means that no population, due to policy or economic disempowerment, is forced to bear a disproportionate burden of the negative human health or environmental impacts of pollution or other environmental consequences resulting from industrial, municipal, and commercial operations or the execution of federal, state, local, and tribal programs and policies\nSchlosberg et al., 2002 - - The equitable distribution of environmental risks and benefits\nThe difference between Environmentalism and Environmental Justice is that Environmental Justice brings forward the underlying issues of race, ethnicity, class, wealth, and/or sovereignty in environmental decision-making.\n\n\n7.1.1 Discussion 1\n\nIs there anything missing from these definitions of Environmental Justice?\nHow does the EPA’s focus on the negative impacts frame Environmental Justice?\nIs a focus on equitable distribution possible in the United States, California, or local municipalities?",
    "crumbs": [
      "Home",
      "Unit 2: Environmental Justice",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>EJ - Screening Tools</span>"
    ]
  },
  {
    "objectID": "EJtheory.html#concepts-of-environmental-justice-movement",
    "href": "EJtheory.html#concepts-of-environmental-justice-movement",
    "title": "7  EJ - Screening Tools",
    "section": "7.2 Concepts of Environmental Justice Movement",
    "text": "7.2 Concepts of Environmental Justice Movement\nThe Environmental Justice is a broad and diffuse global movement with many facets. We will not be able to cover most of these concepts in-depth due to time constraints and the visualization focus of this course.\nI did want to do an overview of these topics such that we can compare them to the current tools used in the U.S. and California for measuringing and quantifying Environmental Justice.\n\nEnvironmental Racism - racial discrimination in environmental policy making, enforcement, and decision-making. This applies both within the United States and globally (e.g., hazardous waste export to southern hemisphere).\nEcological debt - the accumulation of obligations through inequitable resource exploitation, pollution, and habitat degradation between the Northern and Southern hemisphere. This has been quantified through climate debt which examines emissions differentials in carbon budgets and adaptation costs which will accrue primarily to poorer countries.\nClimate justice - equitable distribution of benefits and burdens of climate change adaptation and costs.\nFood sovereignty - a food system in which the people who produce, distribute, and consume food also control the mechanisms and policies of food production and distribution; this stands in contrast to a corporate food regime in which corporations and market insitutions control the food system.\n\nSacrifice Zones - a geographic area permanently impaired by heavy environmental alterations or economic disinvestment, often through locally unwanted land use (LULU).\nEnvironmentalism of the poor - social movement that emphasizes social justice issues instead of emphasizing conservation and eco-efficiency; focuses on sustainability and sovereignty.",
    "crumbs": [
      "Home",
      "Unit 2: Environmental Justice",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>EJ - Screening Tools</span>"
    ]
  },
  {
    "objectID": "EJtheory.html#environmental-justice-tools",
    "href": "EJtheory.html#environmental-justice-tools",
    "title": "7  EJ - Screening Tools",
    "section": "7.3 Environmental Justice Tools",
    "text": "7.3 Environmental Justice Tools\nWe are going to be talking about four EJ tools in this course.\n\nCalEnviroScreen4.0\nEJScreen\nEJI\nClimate and Economic Justice Tool\n\nFor today, I want to focus on the intersection of the concepts of environmental justice listed above and the metrics of environmental justice in these tools.\n\n7.3.1 CalEnviroScreen\n\n\n\n\n\n\nFigure 7.1: CalEnviroScreen Pollution Burden\n\n\n\nFigure 7.1 shows the CalEnviroScreen tool pollution burden index for SoCal. The CalEnviroScreen tool is made up of two categories of data indicators, as documented in the 207 page CalEnviroScreen4.0 report.\n\nPollution Burden - negative environmental indicators of either pollution exposure or environmental effects (e.g., ozone, PM, traffic, drinking water contaminants, toxic release facilities)\nPopulation Characteristics - health and socio-economic indicators (e.g., asthma, education, unemployment, low birth weight) - economic, educational, and linguistic indicators are included.\n\nThe Pollution Burden score and Population Characteristics score are multiplied to determine the final CalEnviroScreen Score.\nCalEnviroScreen4.0 does not have race/ethnicity explicitly as an explanatory variable or part of the base indicator. A six-page supplementary analysis and brief storymap give a cursory description of the EJ results.\nIndividual indicators can be explored as shown in Figure 7.2 visualization of SoCal cardiovascular disease percentiles.\n\n\n\n\n\n\nFigure 7.2: CalEnviroScreen Cardiovascular Disease\n\n\n\n\n\n7.3.2 EPA’s EJScreen\n\n\n\n\n\n\nFigure 7.3: EJSCREEN Ozone Indicator\n\n\n\nFigure 7.3 shows the EPA’s EJScreen index for ozone in SoCal. The U.S. Environmental Protection Agency’s EJScreen tool is also made up of two categories of data indicators, as documented in the 115-page technical report. EPA documentation states that EJScreen is a ’pre-decisional screening tool, and was not meant to be the basis for agency decision-making or determinations regarding the existence or absence of EJ concerns. It should also not be used to identify or label an area as an “EJ community.”\n\nEnvironmental Indicators - negative environmental indicators (i.e., air pollution, traffic, lead paint, proximity to hazardous waste sites, and wastewater discharge)\nDemographic Indicators - an average of the percentage of minority (i.e., non-white non-hispanic) and percentage of low-income households (2x poverty level).\n\nThe individual environmental indicators are combined with a demographic index value to provide individual EJ index values by environmental issue. Thus, there are 12 separate EJ indices provided in EJScreen.\n\n\n7.3.3 CDC and ATSDR Environmental Justice Index\n\n\n\n\n\n\nFigure 7.4: EJI for SoCal\n\n\n\nFigure 7.4 shows the Environmental Justice Index quartiles for SoCal. The Centers for Disease Control CDC Agency for Toxic Substances and Disease Registry ATSDR is a public health agency that has created its own indicator, which is documented in a 94 page technical report\nThe EJI uses three broad categories of indicators.\n\nSocial Vulnerability - Subgroups in this category include racial/ethnic minority status, socioeconomic categories, household demographics, and housing type\nEnvironmental Burden - Subgroups in this category include air pollution, hazardous and toxic sites, built environment, transportation corridors, and water pollution\nHealth Vulnerablity - This category includes five pre-existing chronic diseases, including asthma, cancer, blood pressure, diabetes, and mental health.\n\nThe three indicator categories are each weighted equally and averaged to generate a final single indicator value for the EJI. Each individual category is also available via mouseover or click on specific map locations.\n\n\n7.3.4 Climate and Economic Justice Screening Tool\n\n\n\n\n\n\nFigure 7.5: Climate and Environmental Justice Screening Tool\n\n\n\nFigure 7.5 shows the CEJST base visualization for environmental justice. The CEJST is from the President’s Council on Environmental Quality. The tool is intended to help identify disadvantaged communities that will benefit from programs included in the Justice40 initiative, which seeks to deliver 40% of overall benefits in climate, clean energy, and related areas to disadvantaged communities.\nCEJST uses a two-fold methodology somewhat similar to that followed in EJScreen. The tool uses multiple datasets as indicators of burden. A community is highlighted as disadvantaged on the CEJST map if it is at or above the…\n\nBurden threshold (climate change, energy, legacy pollution, health, housing, transportation, water, and workforce development)\nEconomic threshold (below 35th percentile income usually, low educational attainment for workforce development)\nIn a census tract completely surrounded by disadvantaged communities and at or above the low income median percentile.\n\n\n\n7.3.5 Discussion 2\n\nVisualization - What are your first thoughts about the visualization choices made by creators of these EJ tools? What do you like and dislike; what would you change?\nIndicators - Compare the sets of indicators in these tools. Are they measuring Environmental Justice? What EJ concepts are not covered by these indicators?\nGroupthink - Gather round in groups of 4 and pick a data layer to explore and compare in each of these tools. Once your group has decided, that layer cannot be picked by another group. Come up with a brief group story about your EJ data layer. In your story, tell the class which tool is best and why for illustrating your story. Similarly, tell the class which tool is worst and why for illustrating your story.",
    "crumbs": [
      "Home",
      "Unit 2: Environmental Justice",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>EJ - Screening Tools</span>"
    ]
  },
  {
    "objectID": "EJcategories.html",
    "href": "EJcategories.html",
    "title": "8  EJ - Praxis and Visualization",
    "section": "",
    "text": "8.1 Data Categories in EJ Tools\nAs discussed, in the previous lesson, there are a few broad categories of data that are currently used in Environmental Justice (EJ) tools. Let’s recap them here.\nAs we noted in the last class, these visualizations are more about identifying or screening for locations experiencing environmental injustice than about achieving or visualizing Environmental Justice.",
    "crumbs": [
      "Home",
      "Unit 2: Environmental Justice",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>EJ - Praxis and Visualization</span>"
    ]
  },
  {
    "objectID": "EJcategories.html#data-categories-in-ej-tools",
    "href": "EJcategories.html#data-categories-in-ej-tools",
    "title": "8  EJ - Praxis and Visualization",
    "section": "",
    "text": "Pollution Burden - negative environmental indicators of either pollution exposure, built environment, or environmental effects (e.g., ozone, PM, traffic, drinking water contaminants, toxic release facilities)\n\nSocioeconomic indicators - demographic and economic indicators of population\n\nHealth vulnerability - an indicator of population level health-effect data such as asthma, cancer, diabetes, cardiovascular, and low birth-weight\n\n\n\n8.1.1 Discussion 1\n\nWhat data is needed to understand the fair treatment principle of Environmental Justice?\nWhat data is needed to understand the meaningful involvement principle of Environmental Justice?\nHow does data availability limit our understanding and ability to visualize Environmental Justice?",
    "crumbs": [
      "Home",
      "Unit 2: Environmental Justice",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>EJ - Praxis and Visualization</span>"
    ]
  },
  {
    "objectID": "EJcategories.html#not-data---not-available",
    "href": "EJcategories.html#not-data---not-available",
    "title": "8  EJ - Praxis and Visualization",
    "section": "\n8.2 Not Data - Not Available",
    "text": "8.2 Not Data - Not Available\nMeaningful involvement is a very nebulous and hard-to-measure concept. Within the context of EJ, it indicates public participation with stakeholders and the influence to shape decision-making.\nThe EPA has a resource on public participation in decision-making.\n\n\n\n\n\n\nPublic participation is a process, not a single event. It consists of a series of activities and actions by a sponsor agency over the full lifespan of a project to both inform the public and obtain input from them. Public participation affords stakeholders (those that have an interest or stake in an issue, such as individuals, interest groups, communities) the opportunity to influence decisions that affect their lives.\n\n\n\nA large part of that framework is based on a schematic as shown in Figure 8.1 of the different possible levels of involvement by stakeholders in decision-making. The schematic is from the International Association of Public Participation.\n\n\n\n\n\nFigure 8.1: Public Participation Spectrum\n\n\nQuantifying meaningful involvement in a public participation process of decision-making is complicated and difficult to track. It is also a subjective judgement, although one could have systematic criteria for evaluating it. Moreover, the issue is probably better described as one in which the involvement levels are unequal between different stakeholder groups. In other words, developers and industry stakeholders are provided greater opportunity to shape policy and decision-making compared to residential and environmental stakeholders.\n\n8.2.1 Discussion 2.\n\nHow does a lack of data shape our ability to communicate and visualize an issue?\nHow could one collect information to visualize meaningful involvement?",
    "crumbs": [
      "Home",
      "Unit 2: Environmental Justice",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>EJ - Praxis and Visualization</span>"
    ]
  },
  {
    "objectID": "EJcategories.html#case-study---socal-warehouses---march-jpa-west-campus-upper-plateau",
    "href": "EJcategories.html#case-study---socal-warehouses---march-jpa-west-campus-upper-plateau",
    "title": "8  EJ - Praxis and Visualization",
    "section": "\n8.3 Case Study - SoCal Warehouses - March JPA West Campus Upper Plateau",
    "text": "8.3 Case Study - SoCal Warehouses - March JPA West Campus Upper Plateau\nI have been doing work with the Redford Conservancy on warehouses in the Inland Empire. As part of that work, I have developed a few mapping tools to visualize warehouse information.\nThe primary tool is called WarehouseCITY. WarehouseCITY is intended to provide a means for the public to easily access the impact of existing warehouses on their community. The code repository is located on github.\nA secondary tool provides a visualization of the existing and planned warehouse growth along the 215/60 freeways around the March Air Reserve Base in Riverside County (my backyard). That project’s draft Environmental Impact Report (EIR) is here\n\n8.3.1 Is Visualization Effective in Social Praxis?\nSometimes.\nYes, it works to get media attention and makes very convincing storytelling to those who already agree with you. It can change or engage people who are already on your side. It can surprise and influence people who are in the middle.\nNo, it does not seem to be very effective at engaging decision-makers directly. My experience has been that they are more interested in people and stories. In private meetings, they ooh and aah at the visualization but are more interested in strategy and coalition building - on the ground organization and political influence.\n\n8.3.2 Warehouse Visualization is Easy\n\n8.3.2.1 Load libraries\n\nlibrary(sf)\nlibrary(tidyverse)\nlibrary(leaflet)\n\n\n8.3.2.2 Acquire data\nWe will also pull warehouse data for the first time! New data incoming!\nAlso note that I made this dataset smaller by using the filter() function to only include data from Riverside County; this removes about 7,500 warehouses from LA and San Bernardino counties.\n\nWH.url &lt;- 'https://raw.githubusercontent.com/RadicalResearchLLC/WarehouseMap/main/WarehouseCITY/geoJSON/comboFinal.geojson'\nwarehouses &lt;- st_read(WH.url) |&gt;  \n  filter(county == 'Riverside County') |&gt;  \n  st_transform(\"+proj=longlat +ellps=WGS84 +datum=WGS84\")\n\nReading layer `comboFinal' from data source \n  `https://raw.githubusercontent.com/RadicalResearchLLC/WarehouseMap/main/WarehouseCITY/geoJSON/comboFinal.geojson' \n  using driver `GeoJSON'\nSimple feature collection with 9106 features and 8 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -118.8037 ymin: 33.43325 xmax: -114.4085 ymax: 35.55527\nGeodetic CRS:  WGS 84\n\n\nCheck to see what the warehouses dataset looks like.\n\nhead(warehouses)\n\nSimple feature collection with 6 features and 8 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -117.6083 ymin: 33.87729 xmax: -117.5314 ymax: 33.97265\nGeodetic CRS:  +proj=longlat +ellps=WGS84 +datum=WGS84\n        apn shape_area category year_built                 class\n1 115050036     343300 Existing       2000 warehouse/dry storage\n2 115060057     212500 Existing       1980 warehouse/dry storage\n3 115670012      73600 Existing       1999 warehouse/dry storage\n4 144010065      90600 Existing       1980 warehouse/dry storage\n5 144010079     213300 Existing       2019 warehouse/dry storage\n6 144010064      93200 Existing       2018 warehouse/dry storage\n            county unknown place_name                       geometry\n1 Riverside County   FALSE     Corona MULTIPOLYGON (((-117.543 33...\n2 Riverside County    TRUE     Corona MULTIPOLYGON (((-117.5532 3...\n3 Riverside County   FALSE     Corona MULTIPOLYGON (((-117.5314 3...\n4 Riverside County    TRUE   Eastvale MULTIPOLYGON (((-117.5954 3...\n5 Riverside County   FALSE   Eastvale MULTIPOLYGON (((-117.6073 3...\n6 Riverside County   FALSE   Eastvale MULTIPOLYGON (((-117.5946 3...\n\n\n\n8.3.2.3 Basic Visualization\nThis is geospatial data, so we should put it in an interactive leaflet map to do an initial visualization. Figure 8.2 shows a very basic polygon leaflet map.\n\nleaflet() |&gt; \n  addTiles() |&gt; \n  addPolygons(data = warehouses)\n\n\n\n\n\n\nFigure 8.2: Basic leaflet warehouse map\n\n\n\n\n8.3.2.4 Improve the Visualization\nThe setView() function allows us to set the zoom level and the centerpoint of the map using the arguments lng, lat, and zoom.\nWithin the addPolygons() function, I set the color to brown and the weight of the line to 1.\nFigure 8.3 shows the result for my neighborhood in Riverside.\n\nleaflet() |&gt; \n  addTiles() |&gt;  \n  addPolygons(data = warehouses,\n              color = 'black',\n              weight = 1) |&gt; \n  setView(lng = -117.24, lat = 33.875, zoom = 12) \n\n\n\n\n\n\nFigure 8.3: Leaflet warehouse map making warehouses brown\n\n\n\nLet’s add two more helpful things to orient viewers at a glance.\n\nLet’s change the underlying tile to satellite/aerial imagery using addProviderTiles()\n\nLet’s add a mini-map to orient the viewer to where this is using addMiniMap().\n\nFigure 8.4 shows the resulting map - note I added a palette to distinguish between existing warehouses (orange) and planned and approved warehouses (red) because brown has low salience in satellite imagery of SoCal.\n\npalWHtype &lt;- colorFactor(palette = c( 'darkred', 'darkorange', 'black'),\n                         domain = warehouses$category)\n\nleaflet()  |&gt;  \n  addProviderTiles(provider = providers$Esri.WorldImagery) |&gt; # This shows satellite imagery\n  addPolygons(data = warehouses,\n              color = ~palWHtype(category), #this creates the color categories\n              weight = 1) |&gt; \n  setView(lng = -117.24, lat = 33.875, zoom = 12) |&gt; #this provides the location and zoom level\n  addMiniMap(position = 'bottomleft') #this adds a minimap\n\n\n\n\n\n\nFigure 8.4: Leaflet warehouse map near March JPA",
    "crumbs": [
      "Home",
      "Unit 2: Environmental Justice",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>EJ - Praxis and Visualization</span>"
    ]
  },
  {
    "objectID": "EJdata.html",
    "href": "EJdata.html",
    "title": "9  EJ - Exploratory Analysis Case Study",
    "section": "",
    "text": "9.1 Load and Import Steps\nLoad the libraries we’ll be using today.\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(leaflet)\nImport the SoCalEJ dataset again, if you don’t have it already loaded.\nURL.path &lt;- 'https://raw.githubusercontent.com/RadicalResearchLLC/EDVcourse/main/CalEJ4/CalEJ.geoJSON'\nSoCalEJ &lt;- st_read(URL.path) |&gt;  \n  st_transform(crs = 4326)",
    "crumbs": [
      "Home",
      "Unit 2: Environmental Justice",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>EJ - Exploratory Analysis Case Study</span>"
    ]
  },
  {
    "objectID": "EJdata.html#basic-visualization",
    "href": "EJdata.html#basic-visualization",
    "title": "9  EJ - Exploratory Analysis Case Study",
    "section": "\n9.2 Basic Visualization",
    "text": "9.2 Basic Visualization\nLet’s compare some variables by county to see how the counties are different.\nPick your own variable to plot - do not pick OzoneP which is my variable for now.\nUse filter(...) to only keep values above or equal to zero. We don’t want to include census tracts that are missing data that have -999 values.\nFigure 9.2 shows the distribution of ozone exposure percentages by county.\n\nSoCalEJ |&gt;  \n  filter(OzoneP &gt;= 0) |&gt;  # This selects records with values greater or equal to zero\n  ggplot(aes(x = County, y = OzoneP)) +\n  geom_boxplot()\n\n\n\n\n\n\nFigure 9.2: Ozone census tract distribution by county\n\n\n\n\nThere are clear differences in ozone by county, with the Inland counties having higher ozone than the coastal counties. The differences are statistically significant.\nUnfortunately, a wide dataset is not great at displaying multivariate information in ggplot. A bit of tidying is needed to display multiple variables.",
    "crumbs": [
      "Home",
      "Unit 2: Environmental Justice",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>EJ - Exploratory Analysis Case Study</span>"
    ]
  },
  {
    "objectID": "EJdata.html#tidy-and-transform",
    "href": "EJdata.html#tidy-and-transform",
    "title": "9  EJ - Exploratory Analysis Case Study",
    "section": "\n9.3 Tidy and Transform",
    "text": "9.3 Tidy and Transform\nI am going to demonstrate a few somewhat fancy data manipulation techniques. This is somewhat advanced database programming. While this is very helpful for visualization, it goes beyond the things I expect you to learn for this course.\nThis code does three things.\n\nRemove the geometry using st_set_geometry(value = NULL)\n\nTransform the data table from wide to long using pivot_longer(...)\n\nRemove values below zero using filter()\n\n\n\n# select socioeconomic indicators and make them narrow - only include counties above 70%\nSoCal_narrow &lt;- SoCalEJ |&gt;  \n  st_set_geometry(value = NULL) |&gt;  # remove geometry\n  pivot_longer(cols = c(5:66), names_to = 'variable', values_to = 'value') |&gt; #data science table manipulation\n  filter(value &gt;=0) #select only values greater than or equal to zero\n\nLet’s compare the SoCalEJ and SoCal_narrow datasets using head().\n\nhead(SoCalEJ)\n\nSimple feature collection with 6 features and 66 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -117.874 ymin: 33.5556 xmax: -117.7157 ymax: 33.64253\nGeodetic CRS:  WGS 84\n       Tract   ZIP County     ApproxLoc TotPop19   CIscore   CIscoreP\n1 6059062640 92656 Orange   Aliso Viejo     3741  9.642007 12.1028744\n2 6059062641 92637 Orange   Aliso Viejo     5376 10.569290 14.3343419\n3 6059062642 92625 Orange Newport Beach     2834  3.038871  0.6807867\n4 6059062643 92657 Orange Newport Beach     7231  6.538151  5.8497226\n5 6059062644 92660 Orange Newport Beach     8487  8.873604 10.4009077\n6 6059062645 92657 Orange Newport Beach     6527  6.033648  4.7402925\n       Ozone   OzoneP     PM2_5  PM2_5_P   DieselPM DieselPM_P  Pesticide\n1 0.05165298 65.36403  9.445785 43.88301 0.14536744   50.13068 0.00000000\n2 0.05219839 66.80772  9.785209 46.89484 0.07372588   27.41755 0.00000000\n3 0.04827750 55.38270 10.433417 52.03485 0.03478566   12.40821 0.06496406\n4 0.04901945 58.23273 10.013395 49.29683 0.03594981   12.90604 0.00000000\n5 0.04863521 56.96329 10.565404 52.63223 0.07701293   28.73678 0.00000000\n6 0.04863521 56.96329 10.329985 51.28811 0.04047264   14.58619 0.00000000\n  PesticideP   Tox_Rel Tox_Rel_P   Traffic TrafficP DrinkWat DrinkWatP\n1    0.00000  293.8420  41.44786  999.0182  57.7125 177.3831  5.907331\n2    0.00000  681.0703  57.65191 1051.9778  61.2250 312.7169 25.939803\n3   22.58621 1557.6400  74.05601  874.8193  49.5000 332.6654 32.284251\n4    0.00000 1349.2300  70.69267 1169.7532  67.4250 332.6654 32.284251\n5    0.00000 1889.5133  78.03201 1383.9867  74.9500 332.6654 32.284251\n6    0.00000 1589.1400  74.44361 1025.2790  59.5750 332.6654 32.284251\n       Lead    Lead_P Cleanup  CleanupP GWThreat GWThreatP HazWaste HazWasteP\n1 18.450498 10.737240     0.0  0.000000      0.0  0.000000    0.280  46.80344\n2  9.898042  3.730309     0.0  0.000000      0.0  0.000000    0.280  46.80344\n3 14.263664  6.830498     4.5 40.836133      2.0 14.311805    0.150  26.67108\n4  5.594984  1.600504     0.0  0.000000      0.5  2.722896    0.210  37.68365\n5 16.777373  9.061122     0.0  0.000000      7.5 39.448780    0.395  60.22502\n6 12.086059  5.343415     0.7  9.593132      2.0 14.311805    0.100  16.63799\n  ImpWatBod ImpWatBodP SolWaste SolWasteP PollBurd PolBurdSc PolBurdP Asthma\n1         7   66.73667        0   0.00000 30.50123  3.724194 18.95457  21.70\n2         7   66.73667        2  52.89805 35.23480  4.302163 29.71998  21.74\n3         8   72.15456        0   0.00000 35.68847  4.357555 30.77785  14.93\n4         6   58.69383        0   0.00000 30.97653  3.782228 19.87554  10.33\n5         2   23.87652        2  52.89805 39.48486  4.821094 41.44368  13.88\n6         3   33.15834        2  52.89805 32.98028  4.026885 24.04480  10.51\n     AsthmaP LowBirtWt   LowBirWP Cardiovas CardiovasP Educatn     EducatP\n1 11.2786640      4.45 37.2337696      9.74 27.5049850     1.0    1.771703\n2 11.3908275      3.50 16.2176033      8.72 18.8310070     8.4   35.876993\n3  3.7263210      1.18  0.2950988      5.41  1.8569292  -999.0 -999.000000\n4  0.9845464      5.39 61.9450860      4.46  0.3988036     2.0    5.859276\n5  2.7542373      2.86  7.6597383      5.43  1.9192423     3.7   14.781068\n6  1.0343968      4.64 42.1991275      4.65  0.4860419     0.0    0.000000\n  Ling_Isol Ling_IsolP Poverty  PovertyP Unempl     UnemplP HousBurd HousBurdP\n1       1.1   7.375829    20.0 34.208543    3.0   17.113483     19.9 62.420786\n2       4.4  33.942347    12.9 16.821608    2.6   11.868818     19.5 60.925222\n3       0.0   0.000000     9.5  8.982412    0.9    1.145237     14.5 35.817490\n4       3.9  30.694275     6.9  4.170854    2.6   11.868818      8.5  8.504436\n5       5.0  37.664095    12.3 15.326633    4.0   30.882353     18.9 58.225602\n6       1.0   6.266071    11.5 13.517588 -999.0 -999.000000     14.8 37.477820\n    PopChar PopCharSc   PopCharP Child_10 Pop_10_64 Elderly65 Hispanic   White\n1 24.958604 2.5890185 13.2375189  10.9864   81.7696    7.2441  16.4662 63.2184\n2 23.683405 2.4567389 11.7750882  13.2812   61.9792   24.7396  22.0238 55.2455\n3  6.722867 0.6973798  0.2647504   5.9280   49.6471   44.4248   5.6104 89.6260\n4 16.664505 1.7286508  4.9167927   7.9657   67.7361   24.2982   4.4530 63.9331\n5 17.743511 1.8405788  5.8119012  10.4866   72.3224   17.1910   8.8488 82.0785\n6 14.444279 1.4983412  3.3787191  11.0311   68.4388   20.5301   3.8302 74.7664\n  AfricanAm NativeAm OtherMult Shape_Leng Shape_Area    AAPI\n1    4.9452   0.0000    3.6087   5604.596    1467574 11.7616\n2    1.3765   0.0000    3.5900   6244.598    2143255 17.7641\n3    0.3881   0.0000    1.4467   6803.947    2048571  2.9287\n4    0.0000   0.3042    6.0987  26448.643   18729420 25.2109\n5    0.0000   0.0000    1.1076  10964.108    4361037  7.9651\n6    0.2911   0.0000    0.8733   9833.066    5754643 20.2390\n                        geometry\n1 MULTIPOLYGON (((-117.7178 3...\n2 MULTIPOLYGON (((-117.7166 3...\n3 MULTIPOLYGON (((-117.8596 3...\n4 MULTIPOLYGON (((-117.7986 3...\n5 MULTIPOLYGON (((-117.8521 3...\n6 MULTIPOLYGON (((-117.8269 3...\n\nhead(SoCal_narrow)\n\n# A tibble: 6 × 6\n       Tract   ZIP County ApproxLoc   variable     value\n       &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;       &lt;chr&gt;        &lt;dbl&gt;\n1 6059062640 92656 Orange Aliso Viejo TotPop19 3741     \n2 6059062640 92656 Orange Aliso Viejo CIscore     9.64  \n3 6059062640 92656 Orange Aliso Viejo CIscoreP   12.1   \n4 6059062640 92656 Orange Aliso Viejo Ozone       0.0517\n5 6059062640 92656 Orange Aliso Viejo OzoneP     65.4   \n6 6059062640 92656 Orange Aliso Viejo PM2_5       9.45  \n\n\nThe SoCal_narrow dataset has taken the 60+ columns from SoCalEJ and condensed them into a single column indicating the variable and another column indicating the value for that variable. This is very useful for grouping and visualizing by category of information.\nNow let’s display a box plot with three pollution variables simultaneously using this narrow dataset as shown in Figure 9.3. We again use filter(), but we combine it with the %in% operator to select multiple variables to display.\n\nSoCal_narrow |&gt;  \n  filter(variable %in% c('OzoneP', 'DieselPM_P', 'PolBurdP')) |&gt;  # pick three variables\n  ggplot(aes(x = County, y = value, fill= variable)) +\n  geom_boxplot()\n\n\n\n\n\n\nFigure 9.3: Comparison of ozone, diesel PM, and Pollution burden by county\n\n\n\n\nCool! Now we are seeing some interesting differences.\n\n9.3.1 Exercise 1.\n\nCreate a boxplot that displays five simultaneous variables by County, either by adding two new variables and/or replacing the existing variables. I recommend showing the percentage values that end in P.\nChoose a different theme()\n\nShow a box plot of the six racial and ethnic variables - Hispanic, White, AfricanAm, NativeAm, OtherMult, and AAPI. It should look something like Figure 9.4\n\n\n\n\n\n\n\n\n\nFigure 9.4: Comparison of racial and ethnic population distributions by county\n\n\n\n\n\n9.3.2 Explore the Dataset\nData visualization isn’t just a final product. To get to the final product usually requires doing significant visual exploration to reveal information and knowledge.\nLet’s walk through a few examples of methods to explore the data.\n\n9.3.2.1 Scatter plots\nIs there a relationship between a dependent and an independent variable or 3? Scatter plots and fits help to examine that.\nFigure 9.5 investigates poverty as an independent variable with the pollution burden indicator by county.\n\nSoCalEJ  |&gt;  \n  filter(PovertyP &gt;= 0) |&gt;  \n  ggplot(aes(x = PovertyP, y = PolBurdP, color = County)) +\n  geom_point(size = 0.5, alpha = 0.5) +\n  geom_smooth() +\n  theme_bw() +\n  facet_wrap(~County)\n\n\n\n\n\n\nFigure 9.5: Relationship of poverty and pollution burden by county\n\n\n\n\nVery interesting dataset here. Poverty percentage in a census tract increases pollution burden in Orange, LA, and Riverside County but has no impact in San Bernardino. Riverside is the least pollution burdened on average, while both LA and Orange County have the highest pollution burden.\nLet’s look at one other scatter plot of estimated pollution burden and a health outcome.\n\nSoCalEJ |&gt;  \n  filter(PolBurdP &gt;= 0 & CardiovasP &gt;= 0) |&gt;  \n  ggplot(aes(x = PolBurdP, y = CardiovasP, color = County)) +\n  geom_point(size = 0.5, alpha = 0.5) +\n  geom_smooth() +\n  theme_bw() +\n  facet_wrap(~County)\n\n\n\n\n\n\nFigure 9.6: Relationship of pollution burden and cardiovascular disease by county\n\n\n\n\nVery strange here. Orange County has a positive relationship, but the other counties have non-linear relationships between these two variables.\n\n9.3.2.2 Exercise 2.\n\nGenerate a hypothesis of a causal relationship that you can test. Does variable X cause variable Y to increase/decrease?\nPrepare a four-county scatter-plot of your selected variables you think may have a causal relationship.\nExamine the results. Is there a relationship? Does it vary by county?\n\n9.3.2.3 Histograms\nHistograms are useful ways to explore a distribution of values.\nThe basic histogram is shown in Figure 9.7. The distribution of high pollution burden census tracts is skewed right towards higher values (i.e., worst scores).\n\nSoCalEJ |&gt;  \n  filter(PolBurdP &gt;= 0)  |&gt;  \n  ggplot(aes(x = PolBurdP)) +\n  geom_histogram() #+\n  #theme_bw() +\n  #facet_wrap(~County)\n\n\n\n\n\n\nFigure 9.7: Distribution of pollution burden scores by census tract\n\n\n\n\nNow let’s make that prettier and add a facet_wrap() by county as shown in Figure 9.8. I’ll also fix the axis labels using the labs() function to name them real names.\n\nSoCalEJ |&gt;  \n  filter(PolBurdP &gt;= 0)  |&gt;  \n  ggplot(aes(x = PolBurdP, fill = County)) +\n  geom_histogram() +\n  theme_bw() +\n  facet_wrap(~County) +\n  labs(x = 'Pollution Burden (%)', \n       y = 'Count of census tracts')\n\n\n\n\n\n\nFigure 9.8: Distribution of pollution burden scores by census tract and county\n\n\n\n\nNow, we can clearly see very big differences in census tract counts and distributions of the pollution burden variable. LA County has a massive distribution of highly burdened census tracts.\n\n9.3.2.4 Exercise 3.\n\nChoose a variable you think is interesting and make a four-county histogram plot of it.\nIs there a story that you can start to craft with your histogram?\n\n9.3.3 Bar and Column plots (column is usually better)\nBasic example of a bar plot is shown in Figure 9.9.\n\nSoCalEJ |&gt;  \n  filter(PolBurdP &gt;= 0)  |&gt;  \n  ggplot(aes(x = County)) +\n  geom_bar() \n\n\n\n\n\n\nFigure 9.9: Basic bar plot\n\n\n\n\nThere are more census tracts in LA than the other counties, because far more people live in LA than the other counties.\nWe can try to put the places in there, but it gets messy as shown in fig-Bar2 when looking at San Bernardino places. I’ve switched them to the y-axis to make it horizontal and made the font text smaller using\n\nSoCalEJ |&gt;  \n  filter(County == 'San Bernardino') |&gt;  \n  ggplot(aes(y = ApproxLoc)) +\n  geom_bar() +\n  theme(axis.text = element_text(size = 6)) +\n  labs(y = '', x = 'Count of census tracts')\n\n\n\n\n\n\nFigure 9.10: Basic bar plot\n\n\n\n\nNote that geom_bar() works well with categorical variables, but doesn’t like continuous and numerical values.",
    "crumbs": [
      "Home",
      "Unit 2: Environmental Justice",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>EJ - Exploratory Analysis Case Study</span>"
    ]
  },
  {
    "objectID": "EJPolygons.html",
    "href": "EJPolygons.html",
    "title": "10  EJ - Drawing Polygons for Warehouse downzoning in Mead Valley",
    "section": "",
    "text": "10.1 Existing Conditions\nThe Mead Valley Area Plan is part of the Riverside County General Plan. A picture of the land use planning is shown in that document in Figure 3, reproduced here.\nIndustrial development is focused around the 215 freeway corridor to the East of the Mead Valley Planning area.\nLet’s poke around Warehouse CITY to get a sense of the current development and future development.\nNotice that the adjacent unincorporated communities of Woodcrest and Lake Mathews have zero warehouses - they are majority white communities.",
    "crumbs": [
      "Home",
      "Unit 2: Environmental Justice",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>EJ - Drawing Polygons for Warehouse downzoning in Mead Valley</span>"
    ]
  },
  {
    "objectID": "EJPolygons.html#cumulative-impacts",
    "href": "EJPolygons.html#cumulative-impacts",
    "title": "10  EJ - Drawing Polygons for Warehouse downzoning in Mead Valley",
    "section": "\n10.2 Cumulative Impacts",
    "text": "10.2 Cumulative Impacts\nToday, I am going to ask you all to help with a cumulative impacts analysis for understand the cumulative impacts of the Foundation Amendments. Under CEQA Section 15355, cumulative impacts are defined as ‘two or more individual effects, when considered together, are considerable or which compound or increase other environmental impacts…The cumulative impact from several projects is the change in the environment which results from the incremental impact of the project when added to other closely related past, present, and reasonably foreseeable future projects.’\nA cumulative impacts analysis identifies the past, present, and probable future projects that should be included in environmental analysis.\nIn Spring 2023, my class helped add dozens of planned warehouses to the existing warehouses map. This work helped us to add a layer of ’planned and approved warehouses to the tool allowing users to see what warehouses are likely to be built in the near future.\nIn this class, I hope to assess the cumulative impacts of the General Plan amendments to the Mead Valley community to answer three key questions and illustrate them using an environmental data visualization to submit as public comment to the County (planning commission and Board of Supervisors).\n\nHow are land uses shifting (e.g., from residential to industrial)?\nWhat is the spatial pattern of shifting land-use?\nHow much land is lost for housing (acres and dwelling units)?",
    "crumbs": [
      "Home",
      "Unit 2: Environmental Justice",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>EJ - Drawing Polygons for Warehouse downzoning in Mead Valley</span>"
    ]
  },
  {
    "objectID": "EJPolygons.html#visualization---drawing-a-polygon",
    "href": "EJPolygons.html#visualization---drawing-a-polygon",
    "title": "10  EJ - Drawing Polygons for Warehouse downzoning in Mead Valley",
    "section": "\n10.3 Visualization - Drawing a Polygon",
    "text": "10.3 Visualization - Drawing a Polygon\nWe’re going to draw some polygons. Just like in our first spatial visualization lecture, we’re going to identify latitude and longitude pairs for the vertices of the polygon. However, we’ll run a couple of sf functions on them to transform the individual numbers into a simple features polygon.\n\n\n\n\n\n\nNote, this is a highly manual approach. There are better ways to do this systematically and I’ve developed a tool to start doing that called the Polygon Export Tool. I will show you how to use that in the next unit, because it requires importing data.\n\n\n\nFirst, we do the most basic way. After that, we’ll show the tool and how it can simplify the process - assuming you know where to find files on your machine.\n\n10.3.1 Load libraries\n\nlibrary(sf)\nlibrary(leaflet)\nlibrary(dplyr)\n\n\n10.3.2 Manually Identify the Polygon Vertices.\nOpen Google Maps or an equivalent mapping tool with satellite imagery and an ability to click on a location and retrieve a decimal degree location.\nFind a vertex on the map - input longitude and latitude into a list in the form c(lng, lat).\nDo that for all the vertex points and bind them together as a list of lists as shown in the code below.\nLet’s do that for the boundary of Mead Valley.\n\nMeadValley &lt;- rbind(\n                    c(-117.319618, 33.865782),\n                    c(-117.261695, 33.86626),\n                    c(-117.23327, 33.8012),\n                    c(-117.24842, 33.80129),\n                    c(-117.24842, 33.804726),\n                    c(-117.25254, 33.804726),\n                    c(-117.25254, 33.80829),\n                    c(-117.26752, 33.80829),\n                    c(-117.26752, 33.80025),\n                    c(-117.31279, 33.80025),\n                    c(-117.31279, 33.82981),\n                    c(-117.32242, 33.83189),\n                    c(-117.32242, 33.83561),\n                    c(-117.33111, 33.83595),\n                    c(-117.33111, 33.85819),\n                    c(-117.32265, 33.8584),\n                    c(-117.32267, 33.86331),\n                    c(-117.319618, 33.865782)\n                    )\n\nLook at the MeadValley table in your Environment panel - it looks like a table of point coordinates.\nWe need one more bit of code to convert that into a polygon. It is complicated.\n\nMeadValleyPoly &lt;- st_sf(\n                      name = 'Mead Valley', \n                      geom = st_sfc(st_polygon(list(MeadValley))), \n                      crs = 4326\n                      )\n\nThe name is our label for the polygon, so that’s easy. The crs is the coordinate reference system, in this case WGS84 = 4326 for easy display in leaflet.\nThe geom is the geometry. Three functions are applied - list() which converts the MeadValley table to a list, st_polygon() which returns a polygon from a list of coordinates, and st_sfc() which verifies the contents and sets its class.\n\n\n\n\n\n\nPolygons always have to start and end with the same vertex to be a closed loop.\n\n\n\nNow we should display it to make sure it looks correct. Figure 10.1 shows the polygon with a basic map.\n\nleaflet()  |&gt;  \n  addTiles() |&gt; \n  addPolygons(data = MeadValleyPoly)\n\n\n\n\n\n\nFigure 10.1: Mead Valley Polygon Check\n\n\n\nThat looks pretty good, although I skipped a couple of wiggles.\nNow let’s do a modification of the basic map using addPolylines() instead of addPolygons() and showing you how to add a few warehouses.\nNow let’s add a warehouse layer showing all planned and approved warehouse projects within California that went through CEQA review from 2020-2024.\nFirst import the warehouse dataset. Let’s name this dataset warehouses, just like we did in 8.3.2.2.\n\nWH.url &lt;- 'https://raw.githubusercontent.com/RadicalResearchLLC/WarehouseMap/main/WarehouseCITY/geoJSON/comboFinal.geojson'\nwarehouses &lt;- st_read(WH.url) |&gt;  \n  st_transform(crs = 4326) |&gt; \n  filter(county == 'Riverside County')\n\nReading layer `comboFinal' from data source \n  `https://raw.githubusercontent.com/RadicalResearchLLC/WarehouseMap/main/WarehouseCITY/geoJSON/comboFinal.geojson' \n  using driver `GeoJSON'\nSimple feature collection with 9106 features and 8 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -118.8037 ymin: 33.43325 xmax: -114.4085 ymax: 35.55527\nGeodetic CRS:  WGS 84\n\n\nNow let’s add the existing warehouses to the map.\nFigure 10.2\n\npalWH &lt;- colorFactor(palette = c('darkred', 'orange', 'black'),\n                     domain = warehouses$category)\n\nleaflet() |&gt; \n  addTiles() |&gt; \n  addPolylines(data = MeadValleyPoly,\n              color = 'blue',\n              weight = 3) |&gt; \n  addPolygons(data = warehouses,\n              color = ~palWH(category),\n              weight = 1) \n\n\n\n\n\n\nFigure 10.2: Mead Valley warehouses in bigger context with less busy map.\n\n\n\nIt is a map, but the zoom level is not focused on the salient part we’re interested in. We can use the setView() function to set the zoom level. We showed an example of this in lesson 3.\nFigure 10.3\n\nleaflet() |&gt; \n  addTiles() |&gt; \n  addPolylines(data = MeadValleyPoly,\n              color = 'blue',\n              weight = 3) |&gt; \n  addPolygons(data = warehouses,\n              color = ~palWH(category),\n              weight = 1) |&gt; \n  setView(zoom = 12, lat = 33.85, lng = -117.2) ## Add zoom level and map center point\n\n\n\n\n\n\nFigure 10.3: Mead Valley warehouses zoomed just to the local area.\n\n\n\nThis is better, but I want a better underlying tile view. We can use the addProviderTiles() function to replace addTiles() to get a different underlying tile map. Here we will add the Esri.WorldImagery layer to see the satellite view.\nFigure 10.4\n\nleaflet() |&gt; \n  addProviderTiles(provider = providers$Esri.WorldImagery) |&gt; ## We modified this line to change to satellite view  \n  addPolylines(data = MeadValleyPoly,\n              color = 'blue',\n              weight = 3) |&gt; \n  addPolygons(data = warehouses,\n              color = ~palWH(category),\n              weight = 1,\n              fillOpacity = 0.6) |&gt; \n  addLegend(data = warehouses,\n            pal = palWH,\n            values = ~category) |&gt; \n  setView(zoom = 12, lat = 33.85, lng = -117.25) \n\n\n\n\n\n\nFigure 10.4: Mead Valley warehouses zoomed just to the local area with a satellite image tile map\n\n\n\n\n10.3.3 Existing Conditions\n\nI went to Riverside County Mapping Portal to download the industrial zoned parcels in Mead Valley. The Boundaries & Sites url has the data that can be used to filter the dataset down to the information needed to create the map.\nI used my polygon export tool to draw maps for each General Plan Amendment proposed that is light-industrial or business park. There aren’t enough details to show where building parcels will go and two of the projects are reserving some space for other uses (open space or residential).\n\n10.3.4 Planned Warehouses map for Mead Valley Foundation plan amendments\nFigure 10.5\n\n\n\n\n\nFigure 10.5: limeGreen\n\n\nFigure 10.6\n\n\n\n\n\nFigure 10.6: Amendments\n\n\n\n10.3.5 Planned Warehouses Map with Major Planned Roadway Widenings",
    "crumbs": [
      "Home",
      "Unit 2: Environmental Justice",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>EJ - Drawing Polygons for Warehouse downzoning in Mead Valley</span>"
    ]
  },
  {
    "objectID": "assignment2.html",
    "href": "assignment2.html",
    "title": "11  Assignments for Unit 2 - EJ",
    "section": "",
    "text": "11.1 Graded Assignment - EJ Visualization\nChoose an environmental justice dataset. Three options we discuss in class are:\nIf data downloads are not something you are comfortable with, it is perfectly ok to use the in-class SoCalEJ dataset we have used as an example for in-class coding assignments.\nThe assignment is to create a new visualization or remix of a visualization using some of the data from the chosen EJ dataset. Visualization options include:\nPoints will be awarded for successfully generating a unique visualization, for having made four distinct visualization choices that are different from the standard tool, and for documenting those four unique features in the accompanying email, or as commented text within the .R script file you submit as part of the assignment. Note, the goal is to make an attractive figure, not a super ugly one. Choices that do not aid in improving the feature for the audience will not be considered on point. For example, making the font enormous or tiny is a negative feature.\nIn other words, the goal is to make a better visualization, not just a different visualization. When describing the four feature changes, describe in one sentence why the features improve the visualization.\nAssignment is currently due October 1st, 2024 at the beginning of class (9:45 AM). Please send the .R script and an exported visualization via email. This assignment is worth 150 points. Coding style will not be graded. Scripts will be used to reproduce the figure on my local machine - deviations between code and exported figure will likely result in deducted points.",
    "crumbs": [
      "Home",
      "Unit 2: Environmental Justice",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Assignments for Unit 2 - EJ</span>"
    ]
  },
  {
    "objectID": "assignment2.html#graded-assignment---ej-visualization",
    "href": "assignment2.html#graded-assignment---ej-visualization",
    "title": "11  Assignments for Unit 2 - EJ",
    "section": "",
    "text": "CalEnviroScreen - WARNING - 20+Mb zip file\n\nEPA EJScreen - WARNING - 480+ Mb zip file\n\nCDC & ATSRD EJI tool - state level downloads - pick geoJSON format for maps!\n\nCEJST WARNING - 357 Mb zip file\n\n\n\n\nLeaflet map(s)\nggplot geom_sf map(s)\nggplot figure(s) using some other geom (boxplot, point, bar, histogram)\nSome combination of 1, 2, and 3",
    "crumbs": [
      "Home",
      "Unit 2: Environmental Justice",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Assignments for Unit 2 - EJ</span>"
    ]
  },
  {
    "objectID": "import2.html",
    "href": "import2.html",
    "title": "12  Importing Data - Part 1",
    "section": "",
    "text": "12.1 Load and Install Packages\nAs always, we should load the packages we need to import the data. There are many specialized data import packages, but tidyverse and sf are a good start and can handle many standard tables and geospatial data files. Remember, you can check to make sure a package is loaded in your R session by checking on the files, plots, and packages panel, clicking on the Packages tab, and scrolling down to tidyverse and sf to make sure they are checked.\nlibrary(tidyverse) ## \n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(sf)\n\nLinking to GEOS 3.12.1, GDAL 3.8.4, PROJ 9.3.1; sf_use_s2() is TRUE\n\nlibrary(leaflet)",
    "crumbs": [
      "Home",
      "Unit 3: Data",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Importing Data - Part 1</span>"
    ]
  },
  {
    "objectID": "import2.html#option-1.-point-and-click-download-file-save-read",
    "href": "import2.html#option-1.-point-and-click-download-file-save-read",
    "title": "12  Importing Data - Part 1",
    "section": "\n12.2 Option 1. Point and Click Download, File Save, Read",
    "text": "12.2 Option 1. Point and Click Download, File Save, Read\nThe basic way to acquire data is the Point and Click method. This is a step-by-step instruction for doing that.\n\n12.2.1 Find and Download Data\nGo to CalEnviroScreen\nDownload the Zipped Shapefile shown in the screenshot in Figure 12.2\n\n\n\n\n\nFigure 12.2: CalEnviroScreen Shapefile Location\n\n\nBy default, downloads are often placed in a Downloads directory, although you may have changed that on your local machine.\n\n\n\n\n\n\nYou can skip the next step if you directly save the zip file to your working directory.\n\n\n\n\n12.2.2 Move the Zipped Shapefile to the R Working Directory\nBy default, downloads are often placed in a Downloads directory, although you may have changed that on your local machine. If this occurred in your download, the zipped needs to be either (a) moved to the R working directory or (b) identify the filepath of the default download directory and work with it from there.\nFor today, I will only show path (a) because it is good data science practice to keep the data in a directory associated with the visualization.\n\nIdentify the directory where the zipped shapefile was downloaded. On my machine, this is a Downloads folder which can be accessed through my web browser after the file download is complete. The name of the file is calenviroscreen40shpf2021shp.zip.\nIdentify the R working directory on your machine using the getwd() function.\n\n\ngetwd()\n\n[1] \"C:/Dev/EA078_Fall2024\"\n\nwd &lt;- getwd()\n\n\nMove calenviroscreen40shpf2021shp.zip from the default download directory to the R working directory. Either drag it, copy and paste it, or cut and paste it. For Macs - use the Finder tool. Here’s a youTube video on how to move a file - start at 0:27 seconds.\n\nFor PCs, use File Explorer.\n\nCheck your Files, Plots, and Packages panel to see the zipped file is identified by RStudio. See the example in Figure 12.3.\n\n\n\n\n\n\nFigure 12.3: Files, Plots, and Packages Panel\n\n\nIf you see the calenviroscreen40shpf2021shp.zip in the directory on your machine, congratulations! You are a winner!\n\n12.2.3 Unzip the data - Two Ways\nAlthough the data is in the right place, it is not directly readable while zipped.\n\n12.2.3.1 Point and Click Unzip\nI think the process is basically the same for Mac and PC, but we will identify this in class.\n\nOn a Mac, Double-click the .zip file. The unzipped item appears in the same folder as the .zip file.\nOn a PC, right-clicking on a zipped file will bring up a menu that includes an Extract All option. Choosing the Extract All option brings up a pathname to extract the file to. The default is to extract the zip file to a subfolder named after the zip file.\n\nAgain, go to the Files, Plots, and Packages panel and check if there is a folder called calenviroscreen40shpf2021shp as shown in Figure 12.4.\n\n\n\n\n\nFigure 12.4: Shapefile folder\n\n\nThe sf library is used to import geospatial data. As before, st_read() is function used to import geospatial files.\nShapefiles are the esri propietary geospatial format and are very common.\nThe CalEnviroScreen data are in the shapefile format, which is a bunch of individual files organized in a folder directory. In the calenviroscreen40shpf2021shp directory, there are 8 individual files with 8 different file extensions. We can ignore that and just point read_sf() at the directory and it will do the rest. The dsn = argument stands for data source name which can be a directory, file, or a database.\n\nwd &lt;- getwd()\ndirectory &lt;- 'calenviroscreen40shpf2021shp'\nCalEJ &lt;- sf::st_read(dsn = directory) |&gt; \n  sf::st_transform(crs = 4326)\n\nReading layer `CES4 Final Shapefile' from data source \n  `C:\\Dev\\EA078_Fall2024\\calenviroscreen40shpf2021shp' using driver `ESRI Shapefile'\nSimple feature collection with 8035 features and 66 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -373976.1 ymin: -604512.6 xmax: 539719.6 ymax: 450022.5\nProjected CRS: NAD83 / California Albers\n\n\nCheck the Environment panel after running this line of code. Is there a CalEJ file with 8035 observations of 67 variables present?\nIf so, success is yours! Let’s make a map of Pesticide census tract percentiles to celebrate with Figure 12.5!\n\n12.2.4 Visualize the data\nThe whole California map is too big, so I am just going to show the southernmost counties here by using the filter function for a small subset of counties. We’ll also remove the tracts with no pesticide information (-999).\n\nCalEJ2 &lt;- CalEJ  |&gt;  \n  filter(County %in% c('Imperial', 'Riverside', 'San Diego', 'Orange')) |&gt; \n  filter(PesticideP &gt;=0) |&gt; \n  st_transform(crs = 4326)\n\n\npalPest &lt;- colorNumeric(palette = 'Greys', domain = CalEJ2$PesticideP)\n\nleaflet(data = CalEJ2) |&gt; \n    addTiles() |&gt; \n    addPolygons(color = ~palPest(PesticideP),\n                fillOpacity = 0.5,\n                weight = 2,\n                label = ~ApproxLoc) |&gt; \n    addLegend(pal = palPest,\n              title = 'Pesticide (%)', \n              values = ~PesticideP)\n\n\n\n\n\n\nFigure 12.5: Pesticide percentile census tracts in California\n\n\n\n\n12.2.5 Option 2 - Directly Read the Dataset\nMethane monthly average concentrations sampled by flasks\nToday I am selecting Mauna Loa (MLO) in Hawaii. Methane’s chemical formula is CH4. Therefore, I will assign the path of URL.MLO.CH4\n\nURL.MLO.CH4 &lt;- file.path( 'https://gml.noaa.gov/aftp/data/trace_gases/ch4/flask/surface/txt/ch4_mlo_surface-flask_1_ccgg_month.txt')\n\nWe did this before for Alert, let’s try the successful code using the read_table() function. Note, that when I follow the link, the first line of the dataset says there are 71 header lines.\n\nMLO.CH4 &lt;- read_table(URL.MLO.CH4, skip = 71)\n\n\n── Column specification ────────────────────────────────────────────────────────\ncols(\n  MLO = col_character(),\n  `1984` = col_double(),\n  `10` = col_double(),\n  `1673.79` = col_double()\n)\n\nhead(MLO.CH4)\n\n# A tibble: 6 × 4\n  MLO   `1984`  `10` `1673.79`\n  &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;\n1 MLO     1984    11     1676.\n2 MLO     1984    12     1671.\n3 MLO     1985     1     1662.\n4 MLO     1985     2     1665.\n5 MLO     1985     3     1677.\n6 MLO     1985     4     1674.\n\n\n\nheaders &lt;- c('site', 'year', 'month', 'value')\ncolnames(MLO.CH4) &lt;- headers\n\nhead(MLO.CH4)\n\n# A tibble: 6 × 4\n  site   year month value\n  &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 MLO    1984    11 1676.\n2 MLO    1984    12 1671.\n3 MLO    1985     1 1662.\n4 MLO    1985     2 1665.\n5 MLO    1985     3 1677.\n6 MLO    1985     4 1674.\n\n\nThis is better.\nWe can now visualize the data in Figure 12.6.\n\nMLO.CH4 |&gt; \n  mutate(decimal.Date = (year + month/12)) |&gt; \n  ggplot(aes(x = decimal.Date, y = value)) +\n  geom_point() +\n  geom_line(alpha = 0.6) +\n  geom_smooth() +\n  theme_bw() +\n  labs(x = 'Year', y = 'Methane concentration (ppb)',\n       title = 'Mauna Loa - methane trend')\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\nFigure 12.6: Trend in Methane concentrations (ppb) at Mauna Loa, Hawaii\n\n\n\n\n\n12.2.6 Advanced data visualization\nNow that we have Mauna Loa, I want to add the Alert dataset to it using the code we developed last week. This code downloads the Alert dataset and renames its headers.\n\nURL.ALT.CH4 &lt;- file.path( 'https://gml.noaa.gov/aftp/data/trace_gases/ch4/flask/surface/txt/ch4_alt_surface-flask_1_ccgg_month.txt')\nALT.CH4 &lt;- read_table(URL.ALT.CH4, skip = 70)\n\n\n── Column specification ────────────────────────────────────────────────────────\ncols(\n  ALT = col_character(),\n  `1986` = col_double(),\n  `10` = col_double(),\n  `1774.12` = col_double()\n)\n\ncolnames(ALT.CH4) &lt;- headers\n\nNow we can put the datasets together to make a combined visualization. The bind_rows() function from tidyverse let’s us put the datasets take together since they have the same headers. Then we can use the color argument to aes() to get two separate time series as shown in Figure 12.7. I also grouped the data by the shape of the symbol to ensure that the two datasets are distinguishable.\n\nCH4 &lt;- bind_rows(ALT.CH4, MLO.CH4)\n\nCH4 |&gt; \n  mutate(decimal.Date = (year + month/12)) |&gt; \n  ggplot(aes(x = decimal.Date, y = value, color = site, shape = site)) +\n  geom_point() +\n  geom_line(alpha = 0.6) +\n  #geom_smooth(se = FALSE) +\n  theme_bw() +\n  labs(x = 'Year', y = 'Methane concentration (ppb)',\n       title = 'Methane trend')\n\n\n\n\n\n\nFigure 12.7: Trend in Methane concentrations (ppb) at Mauna Loa, Hawaii and Alert, Canada\n\n\n\n\n\n12.2.7 Downloading secured zip files\nI have not yet found a reliable method to get this to work every time on Macs and PCs. Stay tuned.",
    "crumbs": [
      "Home",
      "Unit 3: Data",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Importing Data - Part 1</span>"
    ]
  },
  {
    "objectID": "import2.html#exercise-1.",
    "href": "import2.html#exercise-1.",
    "title": "12  Importing Data - Part 1",
    "section": "\n12.3 Exercise 1.",
    "text": "12.3 Exercise 1.\n\nGo to the Environmental Justice Index Accessibility Tool.\nPick a state from the dropdown menu.\nPress the Apply button\nAn Actions button should appear; Figure 12.8 shows where that is. Press the Actions button, select Export All and choose Export to geoJSON.\n\nFigure 12.8\n\n\n\n\n\nFigure 12.8: ActionButton\n\n\n\nA file named Environmental Justice Index 2022 result.geojson should appear in your default download folder.\n\nMove the Environmental Justice Index 2022 result.geojson file to the working directory.\n\nCheck the Files panel. Is Environmental Justice Index 2022 result.geojson there?\n\nRead in the file using read_sf(). The dsn argument can point directly to the file name for this type of file. Assign it a name that incorporates EJI and the state abbreviation.\n\nCheck the Environment panel. Did it import?\n\nMake a visualization - but not a map because projections are wonky?\n\n\ngetwd()\n\n[1] \"C:/Dev/EA078_Fall2024\"\n\nCO_EJI_raw &lt;- sf::st_read(dsn = 'Environmental Justice Index 2022 result.geojson')  |&gt; \n  sf::st_transform(crs = 4326) |&gt; \n  mutate(DSLPM = as.numeric(EPL_DSLPM)) # for some reason all the values are importing as character values\n\nReading layer `Environmental Justice Index 2022 result' from data source \n  `C:\\Dev\\EA078_Fall2024\\Environmental Justice Index 2022 result.geojson' \n  using driver `GeoJSON'\nSimple feature collection with 100 features and 119 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: -106.0394 ymin: 37.35623 xmax: -103.7057 ymax: 40.00146\nGeodetic CRS:  WGS 84\n\n  # this code converts one row to numeric\n\nFigure 12.9 shows Diesel PM from their environmental indicators layer for San Bernardino County.\n\nggplot(data = CO_EJI_raw) +\n  geom_sf(aes(fill = DSLPM), linewidth = 0) +\n  theme_bw()\n\n\n\n\n\n\nFigure 12.9: Diesel PM indicator in California census tracts from the EJI tool",
    "crumbs": [
      "Home",
      "Unit 3: Data",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Importing Data - Part 1</span>"
    ]
  },
  {
    "objectID": "export.html",
    "href": "export.html",
    "title": "13  Exporting Visualizations and Data",
    "section": "",
    "text": "13.1 Load libraries\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(sf)\n\nLinking to GEOS 3.12.1, GDAL 3.8.4, PROJ 9.3.1; sf_use_s2() is TRUE\n\nlibrary(leaflet)",
    "crumbs": [
      "Home",
      "Unit 3: Data",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Exporting Visualizations and Data</span>"
    ]
  },
  {
    "objectID": "export.html#import-data",
    "href": "export.html#import-data",
    "title": "13  Exporting Visualizations and Data",
    "section": "\n13.2 Import data",
    "text": "13.2 Import data\nLoad SoCalEJ for the demonstration.\n\nURL.path &lt;- 'https://raw.githubusercontent.com/RadicalResearchLLC/EDVcourse/main/CalEJ4/CalEJ.geoJSON'\nSoCalEJ &lt;- st_read(URL.path) |&gt; \n  st_transform(\"+proj=longlat +ellps=WGS84 +datum=WGS84\")\n\nReading layer `CalEJ' from data source \n  `https://raw.githubusercontent.com/RadicalResearchLLC/EDVcourse/main/CalEJ4/CalEJ.geoJSON' \n  using driver `GeoJSON'\nSimple feature collection with 3747 features and 66 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 97418.38 ymin: -577885.1 xmax: 539719.6 ymax: -236300\nProjected CRS: NAD83 / California Albers",
    "crumbs": [
      "Home",
      "Unit 3: Data",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Exporting Visualizations and Data</span>"
    ]
  },
  {
    "objectID": "export.html#transform-data-for-ggplot-group-facet-visualization",
    "href": "export.html#transform-data-for-ggplot-group-facet-visualization",
    "title": "13  Exporting Visualizations and Data",
    "section": "\n13.3 Transform data for ggplot group & facet visualization",
    "text": "13.3 Transform data for ggplot group & facet visualization\n\nSoCal_narrow &lt;- SoCalEJ |&gt; \n  st_set_geometry(value = NULL) |&gt; \n  pivot_longer(cols = c(5:66), names_to = 'variable', values_to = 'value') |&gt; \n  filter(value &gt;=0)",
    "crumbs": [
      "Home",
      "Unit 3: Data",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Exporting Visualizations and Data</span>"
    ]
  },
  {
    "objectID": "export.html#create-a-ggplot-visualization",
    "href": "export.html#create-a-ggplot-visualization",
    "title": "13  Exporting Visualizations and Data",
    "section": "\n13.4 Create a ggplot visualization",
    "text": "13.4 Create a ggplot visualization\nI will start with one of our pretty ggplot examples from Chapter 9. Let’s remix the racial and ethnic population percentage by census tract and county. Figure 13.1 shows the figure.\n\nSoCal_narrow |&gt; \n  filter(variable %in% c('Hispanic', 'White', 'AfricanAm',\n                         'NativeAm', 'OtherMult', 'AAPI')) |&gt; \n  ggplot(aes(x = variable, y = value, fill= County)) +\n  geom_boxplot() +\n  theme_bw() + \n  labs(x = '', y = 'Percent of population')\n\n\n\n\n\n\nFigure 13.1: Racial and ethnic population distribution by county in SoCal census tracts.",
    "crumbs": [
      "Home",
      "Unit 3: Data",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Exporting Visualizations and Data</span>"
    ]
  },
  {
    "objectID": "export.html#exporting-visualizations",
    "href": "export.html#exporting-visualizations",
    "title": "13  Exporting Visualizations and Data",
    "section": "\n13.5 Exporting Visualizations",
    "text": "13.5 Exporting Visualizations\n\n13.5.1 Option 1 - Point and Click\nThe point and click option is available after one creates a visualization.\n\nGo to the files, plots, and packages panel of RStudio.\nClick on the Plots tab.\nClick on the Export button as shown in Figure 13.2\n\nChoose whether to Save as Image or Save as PDF.\n\nRename file save to something more descriptive than RPlot\n\nClick Save\n\nFile should now be saved to your working directory - getwd() will identify that path, and it should be accessible through the Files tab of the\n\n\n\n\n\n\nFigure 13.2: Export button is here on my machine\n\n\n\n13.5.2 Option 2. For ggplot() files, use ggsave()\n\nThis is pretty straightforward and a bit more reproducible and customizable than the manual point and click process. A minimal example of a .png export is shown below.\n\nggsave(filename = 'boxplot.png')\n\nSaving 7 x 5 in image\n\n\nOne can then go to the Files panel, sort by modified and there should be a file named boxplot.png. Note that ggsave() defaults to saving the last image created.\nOne can save files as a variety of image formats, with specified dimensions.\n\nggsave(filename = 'boxplot.jpg', width = 5, height = 4, units = 'in')\n\nThere are a number of export options within ggsave(), but the defaults should be pretty good for now.\nLastly, it can sometimes be important to directly save a ggplot within R. Then one can manually assign the exact image file to export. The code below demonstrates this, and is most important when automating exporting many (e.g., 100s) of images.\n\nVis &lt;- SoCal_narrow |&gt; \n  filter(variable %in% c('Hispanic', 'White', 'AfricanAm',\n                         'NativeAm', 'OtherMult', 'AAPI')) |&gt; \n  ggplot(aes(x = variable, y = value, fill= County)) +\n  geom_boxplot() +\n  theme_bw() + \n  labs(x = '', y = 'Percent of population')\n\nggsave('boxplot.pdf', plot = Vis)\n\nSaving 7 x 5 in image\n\n\nIn this third example, we assign the image to the pointer Viz. Then the ggsave() option plot = is used to assign the image to save the file Viz to a .pdf export.",
    "crumbs": [
      "Home",
      "Unit 3: Data",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Exporting Visualizations and Data</span>"
    ]
  },
  {
    "objectID": "export.html#leaflet-visualization---static",
    "href": "export.html#leaflet-visualization---static",
    "title": "13  Exporting Visualizations and Data",
    "section": "\n13.6 Leaflet Visualization - Static",
    "text": "13.6 Leaflet Visualization - Static\nFirst, I am going to map Diesel PM for the SoCalEJ dataset in Leaflet. Figure 13.3 is the result.\n\npalDPM &lt;- colorNumeric(palette = 'YlOrBr', domain = SoCalEJ$DieselPM_P)\n\nleaflet(data = SoCalEJ) |&gt; \n  addTiles() |&gt; \n  setView(lat = 33.8, lng = -117.60, zoom = 9) |&gt; \n  addPolygons(stroke = FALSE,\n              fillColor = ~palDPM(DieselPM_P),\n              fillOpacity = 0.5) |&gt; \n  addLegend(pal = palDPM, \n            title = 'Diesel PM (%)', \n            values = ~DieselPM_P)\n\n\n\n\n\n\nFigure 13.3: Diesel PM percentiles in SoCal\n\n\n\n\n13.6.1 Option 1. Point and Click.\nRepeat similar steps as shown in Section 13.5.1. Here are the steps for exporting an interactive HTML map manually.\n\nGo to the files, plots, and packages panel of RStudio.\nClick on the Plots tab.\nClick on the Export button as shown in Figure 13.2\n\nChoose to Save as Web Page.\n\nRename file save to something more descriptive than RPlot\n\nClick Save\n\nFile should now be saved to your working directory - getwd() will identify that path, and it should be accessible through the Files tab. On my machine it also loads directly into my default web browser.\n\n13.6.2 Option 2. htmlwidgets package for exports\nAs always, there is a package to do a specific thing that be done point and click style. htmlwidgets provides the saveWidget() function for exporting leaflet maps as HTML.\n\n13.6.2.1 Install the package and load it.\n\ninstall.packages('htmlwidgets')\n\n\nlibrary(htmlwidgets)\n\n\n13.6.2.2 Save Leaflet as a Static Image\nFirst, the map needs to be assigned a name. In this case, it is assigned the name DPM_map.\n\nDPM_map &lt;- leaflet(data = SoCalEJ) |&gt; \n  addTiles() |&gt; \n  setView(lat = 33.8, lng = -117.60, zoom = 9) |&gt; \n  addPolygons(stroke = FALSE,\n              fillColor = ~palDPM(DieselPM_P),\n              fillOpacity = 0.5) |&gt; \n  addLegend(pal = palDPM, \n            title = 'Diesel PM (%)', \n            values = ~DieselPM_P)\n\nLook at the Environment panel and DPM_map is now an environmental data of type Large leaflet (8 elements, 11.6 MB).\nNow the map can be saved using saveWidgets()\n\nsaveWidget(widget = DPM_map, file = 'DPM_map.html')\n\nAnd that’s it. Check in Files and we have the DPM_map.html.",
    "crumbs": [
      "Home",
      "Unit 3: Data",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Exporting Visualizations and Data</span>"
    ]
  },
  {
    "objectID": "munging.html",
    "href": "munging.html",
    "title": "14  Data Science 101",
    "section": "",
    "text": "14.1 Introduction\nMr. Wickham is of course quoting Tolstoy, but his observation is poignant and correct. Much of the work in data visualization is finagling one’s dataset.\nToday, we will focus on some key functions to tidy messy data, as compiled by me. Examples will be provided using data from previous lectures.\nLet’s get started with initializing today’s R script to include the libraries we’ll be using. Start with tidyverse and sf.\nlibrary(tidyverse)\nlibrary(sf)",
    "crumbs": [
      "Home",
      "Unit 3: Data",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Data Science 101</span>"
    ]
  },
  {
    "objectID": "munging.html#introduction",
    "href": "munging.html#introduction",
    "title": "14  Data Science 101",
    "section": "",
    "text": "‘Tidy datasets are all alike, but every messy dataset is messy in its own way.’\n— Hadley Wickham\n\n\n\nHadley Wickham - Creator of the Tidyverse\n\n\n\n\n\n\n14.1.1 st_transform()\n\nst_transform() transforms or converts coordinates of simple feature geospatial data. Spatial projections are fraught with peril in geospatial visualizations. Our first function is one that has been used a bunch of times - st_transform(). Leaflet needs its data transformed into WGS84 and st_transform() is the function that makes the coordinate reference system go the right place.\n\nURL.path &lt;- 'https://raw.githubusercontent.com/RadicalResearchLLC/EDVcourse/main/CalEJ4/CalEJ.geoJSON'\nSoCalEJ &lt;- st_read(URL.path) |&gt;\n  st_transform(\"+proj=longlat +ellps=WGS84 +datum=WGS84\")\n\nReading layer `CalEJ' from data source \n  `https://raw.githubusercontent.com/RadicalResearchLLC/EDVcourse/main/CalEJ4/CalEJ.geoJSON' \n  using driver `GeoJSON'\nSimple feature collection with 3747 features and 66 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 97418.38 ymin: -577885.1 xmax: 539719.6 ymax: -236300\nProjected CRS: NAD83 / California Albers\n\n\n\n14.1.2 filter()\n\nfilter() is used to subset a data table, retaining any rows that meet the conditions of the filter.\n\n\nfilter() is used on numbers by applying operators (e.g., &gt;, =, &lt;, &gt;=, &lt;=).\n\n\nfilter() can be applied on character strings by using the identity operator ==.\n\n\nfilter() can be applied to multiple character strings by using the %in% operator on lists.\n\nIn the first example, filter()was applied to remove the values that were set at -999; we only believed values from 0-100 were reasonable. Figure 14.1 uses filter() to remove those values. I’ve also shown an example without the filter applied in Figure 14.2 . Not removing those rows messes up our visualization.\n\nSoCalEJ |&gt;\n  filter(AsthmaP &gt;= 0) |&gt;\n  ggplot(aes(x = County, y = AsthmaP)) +\n  geom_boxplot()\n\n\n\n\n\n\nFigure 14.1: Asthma census tract distribution by county\n\n\n\n\n\nSoCalEJ |&gt;\n  #filter()\n  ggplot(aes(x = County, y = AsthmaP)) +\n  geom_boxplot()\n\n\n\n\n\n\nFigure 14.2: Asthma census tract distribution by county without filter\n\n\n\n\nIn Chapter 9 we also applied filter in two successive data transformations that are good examples of data munging. After creating a narrow data set, we apply filter(value &gt;=0) to remove all negative values. Then we applied filter(variable %in% c('OzoneP', 'DieselPM_P', 'PolBurdP')) to select three specific variable choices out of the 55 we had available. If we exclude that second filter, the plot becomes crazy busy.\n\n# select socioeconomic indicators and make them narrow - only include counties above 70%\nSoCal_narrow &lt;- SoCalEJ |&gt;\n  st_set_geometry(value = NULL) |&gt;\n  pivot_longer(cols = c(5:66), names_to = 'variable', values_to = 'value') |&gt;\n  filter(value &gt;=0)\n\nSoCal_narrow |&gt;\n  filter(variable %in% c('OzoneP', 'DieselPM_P', 'PolBurdP')) |&gt;\n  ggplot(aes(x = County, y = value, fill= variable)) +\n  geom_boxplot()\n\n\n\n\n\n\nSoCal_narrow |&gt;\n  #filter(variable %in% c('OzoneP', 'DieselPM_P', 'PolBurdP')) |&gt;\n  ggplot(aes(x = County, y = value, fill= variable)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\n14.1.3 select()\n\nselect() variables (i.e., columns) in a data table for retention.\n\n\nselect() can be applied to subsets column number or name.\n\nselect() also has some pattern matching helpers.\n\nChapter 7 included an example of using select() on the SoCalEJ dataset. The raw dataset is MESSY!\n\n14.1.4 mutate()\n\nmutate() adds new variables and preserves existing ones. mutate() can be used to overwrite existing variables - careful with name choices.\nThis is a great function for synthesizing information, transformations, and combining variables.\n\nchanging units - use mutate()\n\ncreating a rate or normalizing data - use mutate()\n\nneed a new variable or category - use mutate()\n\n\nI used mutate() to create a decimal date function for our CH4 visualizations in Chapter 12.\nLet’s use mutate() to convert the estimate the census tract population of Hispanic and African American groups in SoCalEJ. In SoCalEJ, we have a variable called TotPop19, which is the total population in 2019. We also have the percentage of population by census tract in each ethnic group. We can use mutate to create new variables.\n\nSoCalEJ2 &lt;- SoCalEJ |&gt; \n  #select only used variables\n  select(Tract, TotPop19, Hispanic, AfricanAm) |&gt; \n  #filter to make sure we don't divide by zero population tracts\n  filter(TotPop19 &gt; 0 & Hispanic &gt;= 0 & AfricanAm &gt;= 0) |&gt; \n  #mutate to create new variables\n  mutate(HispTot = round(Hispanic*TotPop19/100, 0),\n         AfriAmTot = round(AfricanAm*TotPop19/100,0))\n\nhead(SoCalEJ2)\n\nSimple feature collection with 6 features and 6 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -117.874 ymin: 33.5556 xmax: -117.7157 ymax: 33.64253\nGeodetic CRS:  +proj=longlat +ellps=WGS84 +datum=WGS84\n       Tract TotPop19 Hispanic AfricanAm                       geometry HispTot\n1 6059062640     3741  16.4662    4.9452 MULTIPOLYGON (((-117.7178 3...     616\n2 6059062641     5376  22.0238    1.3765 MULTIPOLYGON (((-117.7166 3...    1184\n3 6059062642     2834   5.6104    0.3881 MULTIPOLYGON (((-117.8596 3...     159\n4 6059062643     7231   4.4530    0.0000 MULTIPOLYGON (((-117.7986 3...     322\n5 6059062644     8487   8.8488    0.0000 MULTIPOLYGON (((-117.8521 3...     751\n6 6059062645     6527   3.8302    0.2911 MULTIPOLYGON (((-117.8269 3...     250\n  AfriAmTot\n1       185\n2        74\n3        11\n4         0\n5         0\n6        19\n\n\nNow let’s make a figure of the Hispanic population in the Inland Empire.\n\nggplot() +\n  geom_sf(data = SoCalEJ2, aes(fill = HispTot)) +\n  theme_minimal() +\n  scale_fill_viridis_c(direction = -1) +\n  coord_sf(xlim =c(-118, -117),\n           ylim = c(33.4, 34.2)) +\n  labs(fill = 'Hispanic population')\n\n\n\n\n\n\n\n\n14.1.4.1 Exercise\n\nMake a figure of African American (or another ethnic/racial variable) population.\nChoose a different set of Southern California coordinates to focus in on.\n\nChoose another fill scheme (either another viridis or another scale_fill_brewer).\n\n14.1.5 summarize()\n\nsummarize() creates a new table that reduces a dataset to a summary of all observations. When combined with the group_by() function, it allows extremely powerful manipulation and generation of summary statistics about a dataset.\nThis example also includes the group_by() function. This function identifies categories to summarize the data by. The SoCalEJ_narrow dataset has some simple grouping categories that can be used to show this.\n\nSoCal_basic &lt;- SoCal_narrow |&gt;\n  group_by(variable) |&gt;\n  summarize(count = n(), average = mean(value), min = min(value), max =       max(value), stdev = sd(value))\n\nhead(SoCal_basic)\n\n# A tibble: 6 × 6\n  variable  count average    min   max stdev\n  &lt;chr&gt;     &lt;int&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 AAPI       3728   13.3  0       87.6  14.8\n2 AfricanAm  3728    6.44 0       84.7  10.1\n3 Asthma     3737   50.4  4.28   203.   26.9\n4 AsthmaP    3737   49.7  0.0125  99.9  27.7\n5 CIscore    3693   33.9  1.80    82.4  16.7\n6 CIscoreP   3693   59.9  0.151  100.   27.3\n\n\nAnd of course, we can combine our functions to dig even deeper. This example focuses on just a few variables to group the data into smaller subsets of County.\n\nSoCal_complicated &lt;- SoCal_narrow |&gt;\n  filter(variable %in% c('CIscoreP', 'AsthmaP', 'LowBirWP', 'CardiovasP')) |&gt;\n  group_by(variable, County) |&gt;\n  summarize(count = n(), average = mean(value), min = min(value), max =       max(value), stdev = sd(value))\n\n`summarise()` has grouped output by 'variable'. You can override using the\n`.groups` argument.\n\nhead(SoCal_complicated, 15)\n\n# A tibble: 15 × 7\n# Groups:   variable [4]\n   variable   County         count average     min   max stdev\n   &lt;chr&gt;      &lt;chr&gt;          &lt;int&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 AsthmaP    Los Angeles     2334    53.4  0.0125  99.3  28.2\n 2 AsthmaP    Orange           582    27.9  0.137   70.4  18.4\n 3 AsthmaP    Riverside        453    49.6  2.88    94.1  22.5\n 4 AsthmaP    San Bernardino   368    60.9  0.897   99.9  24.5\n 5 CIscoreP   Los Angeles     2297    66.2  0.290  100.   26.2\n 6 CIscoreP   Orange           580    42.4  0.151   97.4  26.1\n 7 CIscoreP   Riverside        450    48.8  1.06    99.3  23.9\n 8 CIscoreP   San Bernardino   366    61.2  4.34    99.6  23.0\n 9 CardiovasP Los Angeles     2334    54.4  0.0499  99.2  27.0\n10 CardiovasP Orange           582    33.0  0.249   77.4  17.7\n11 CardiovasP Riverside        453    71.9  2.62    99.4  22.5\n12 CardiovasP San Bernardino   368    77.3 14.2    100    19.0\n13 LowBirWP   Los Angeles     2279    55.4  0       99.9  29.2\n14 LowBirWP   Orange           571    42.8  0       99.4  26.5\n15 LowBirWP   Riverside        430    49.7  0.128  100.   25.8",
    "crumbs": [
      "Home",
      "Unit 3: Data",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Data Science 101</span>"
    ]
  },
  {
    "objectID": "import3.html",
    "href": "import3.html",
    "title": "15  Importing Data - Part 2",
    "section": "",
    "text": "15.1 Load libraries and install new package\nLoad tidyverse, sf, leaflet, and htmltools.\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(sf)\n\nLinking to GEOS 3.12.1, GDAL 3.8.4, PROJ 9.3.1; sf_use_s2() is TRUE\n\nlibrary(leaflet)\nlibrary(htmltools)\nI will also demonstrate the use of the tidycensus package. Install and load the tidycensus package.\ninstall.packages('tidycensus')\nlibrary(tidycensus)",
    "crumbs": [
      "Home",
      "Unit 3: Data",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Importing Data - Part 2</span>"
    ]
  },
  {
    "objectID": "import3.html#load-libraries-and-install-new-package",
    "href": "import3.html#load-libraries-and-install-new-package",
    "title": "15  Importing Data - Part 2",
    "section": "",
    "text": "15.1.1 Extra step - register for API key from the Census\nRegister at this url https://api.census.gov/data/key_signup.html.\nOrganization is probably Pitzer College.\nCheck your email for a very long api key. Copy that key.\nRun the following line of code to activate your key for the session. The key is used in every request to access data from the census API (application programming interface).\n\n# Replace &lt;YOUR KEY HERE&gt; with the key from the census data api service\ncensus_api_key('&lt;YOUR KEY HERE&gt;', install = TRUE)",
    "crumbs": [
      "Home",
      "Unit 3: Data",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Importing Data - Part 2</span>"
    ]
  },
  {
    "objectID": "import3.html#import-geospatial-census-data",
    "href": "import3.html#import-geospatial-census-data",
    "title": "15  Importing Data - Part 2",
    "section": "\n15.2 Import Geospatial Census data",
    "text": "15.2 Import Geospatial Census data\ntidycensus allows one to pull specific geospatial census datasets quickly into R for use in mapping and visualization applications. This should be pretty good for the individual and group projects.\nFirst, we need to alter a setting in the options to allow R to directly import geospatial files from the census API.\n\noptions(tigris_use_cache = TRUE)\n\n\n15.2.1 Example 1 - Los Angeles census tracts\ntidycensus accepts state names (e.g. “Wisconsin”), state postal codes (e.g. “WI”), and state FIPS codes (e.g. “55”), so a user can choose what they are most comfortable with.\nFirst, let’s pull a census tract dataset to show how it works and how it quickly gets us spatial information. I will start with an example from Los Angeles County. We are pulling American Community Survey data using get_acs(). The arguments are state, county, geography - which we choose as census tract, variable - which we choose as B19013_001 which is median income, geometry, and year.\n\nLA &lt;- get_acs(\n  state = \"CA\",\n  county = \"Los Angeles\",\n  geography = \"tract\",\n  variables = \"B19013_001\",\n  geometry = TRUE,\n  year = 2020\n)\n\nNow let’s map it using leaflet. I want to show the median income as a filled color and I’ll choose the viridis palette. Remember to define the color palette first. Figure 15.1 shows the result.\n\npalIncome &lt;- colorNumeric(palette = 'magma', domain = LA$estimate)\n\nLA %&gt;% \n  filter(!is.na(estimate)) %&gt;% \n  st_transform(\"+proj=longlat +ellps=WGS84 +datum=WGS84\") %&gt;%\n  leaflet() %&gt;% \n  addTiles() %&gt;% \n  addProviderTiles(provider = providers$CartoDB.Positron) %&gt;% \n  addPolygons(color = ~palIncome(estimate),\n              fillOpacity = 0.7,\n              weight = 0.5,\n              label = ~htmlEscape(str_c('$',estimate))) %&gt;% \n  addLegend(pal = palIncome,\n            title = 'Median Income',\n            values = ~estimate)\n\n\n\n\n\n\nFigure 15.1: Median income for 2015-2020 for Los Angeles County - data from the ACS\n\n\n\nThe variables that are available from the U.S. census can be perused using the following code bits. See the tidycensus page for more details. There are 25,000+ variables to choose from and it goes a bit too deep to go into those here.\n\nv20 &lt;- load_variables(2020, \"acs5\", cache = TRUE)\n\nhead(v20)\n\n# A tibble: 6 × 4\n  name       label                                   concept    geography  \n  &lt;chr&gt;      &lt;chr&gt;                                   &lt;chr&gt;      &lt;chr&gt;      \n1 B01001_001 Estimate!!Total:                        SEX BY AGE block group\n2 B01001_002 Estimate!!Total:!!Male:                 SEX BY AGE block group\n3 B01001_003 Estimate!!Total:!!Male:!!Under 5 years  SEX BY AGE block group\n4 B01001_004 Estimate!!Total:!!Male:!!5 to 9 years   SEX BY AGE block group\n5 B01001_005 Estimate!!Total:!!Male:!!10 to 14 years SEX BY AGE block group\n6 B01001_006 Estimate!!Total:!!Male:!!15 to 17 years SEX BY AGE block group\n\n#View(v20)\n\n\n15.2.2 Example 2 - Jefferson County, Texas - Racial variables\nKnowing income is nice, but a lot of the Environmental (in)Justice section was also focused on racial and ethnic variables. Here’s an example for Jefferson County, Texas to pull decennial census data on racial composition at the county census tract level.\n\n# These are the list of racial variables from the decennial census\nracevars &lt;- c(White = \"P2_005N\", \n              Black = \"P2_006N\", \n              Asian = \"P2_008N\", \n              Hispanic = \"P2_002N\")\n\nJeffCo &lt;- get_decennial(\n  geography = \"tract\",\n  variables = racevars,\n  state = \"TX\",\n  county = \"Jefferson County\",\n  geometry = TRUE,\n  summary_var = \"P2_001N\",\n  year = 2020\n) \n\nhead(JeffCo)\n\nSimple feature collection with 6 features and 5 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -94.13939 ymin: 29.86557 xmax: -93.92767 ymax: 30.06859\nGeodetic CRS:  NAD83\n# A tibble: 6 × 6\n  GEOID       NAME        variable value summary_value                  geometry\n  &lt;chr&gt;       &lt;chr&gt;       &lt;chr&gt;    &lt;dbl&gt;         &lt;dbl&gt;        &lt;MULTIPOLYGON [°]&gt;\n1 48245002100 Census Tra… White       97          2983 (((-94.13939 30.05566, -…\n2 48245002100 Census Tra… Black     1940          2983 (((-94.13939 30.05566, -…\n3 48245002100 Census Tra… Asian       23          2983 (((-94.13939 30.05566, -…\n4 48245002100 Census Tra… Hispanic   851          2983 (((-94.13939 30.05566, -…\n5 48245006100 Census Tra… White       25          1055 (((-93.95282 29.87938, -…\n6 48245006100 Census Tra… Black      874          1055 (((-93.95282 29.87938, -…\n\n\nData looks good. Figure 15.2 shows a facet wrap display of the racial data. This is a very modest remix of the example from the tidycensus spatial data example for Harris County.\n\nJeffCo %&gt;%\n  #Create a percent variable for the four racial categories\n  mutate(percent = 100 * (value / summary_value)) %&gt;%\n  #Make a ggplot map\n  ggplot(aes(fill = percent)) +\n  facet_wrap(~variable) +\n  geom_sf(color = NA) +\n  theme_void() + \n  scale_fill_viridis_c() + \n  labs(fill = \"% of population\\n(2020 Census)\")\n\n\n\n\n\n\nFigure 15.2: Racial demographic information for Jefferson County, Texas from the decennial census for 2020.\n\n\n\n\nThis is a good start, especially if one overlaid the location of major petrochemical facilities from the TRI dataset on it.\n\n15.2.3 Example from Cancer Alley\nTorHoerman Law identifies three parishes in Louisiana as part of cancer alley - St. Charles, St. James, and St. John the Baptist.\nCan we pull and display three parishes (i.e., Louisiana county equivalents) simultaneously?\n\n#Note, we use the same racial demographic variables in previous example\n\n#Combine the three parishes into a single list for pulling\nparishes &lt;- c('St. John the Baptist',\n              'St. James',\n              'St. Charles')\n\nCancerAlley &lt;- get_decennial(\n  geography = \"tract\",\n  variables = racevars,\n  #change the state\n  state = \"LA\",\n  #change the county \n  county = parishes,\n  geometry = TRUE,\n  summary_var = \"P2_001N\",\n  year = 2020\n) \n\nhead(CancerAlley)\n\nSimple feature collection with 6 features and 5 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -90.54614 ymin: 30.04627 xmax: -90.47054 ymax: 30.10198\nGeodetic CRS:  NAD83\n# A tibble: 6 × 6\n  GEOID       NAME        variable value summary_value                  geometry\n  &lt;chr&gt;       &lt;chr&gt;       &lt;chr&gt;    &lt;dbl&gt;         &lt;dbl&gt;        &lt;MULTIPOLYGON [°]&gt;\n1 22095070800 Census Tra… White      147          1930 (((-90.54614 30.07102, -…\n2 22095070800 Census Tra… Black     1727          1930 (((-90.54614 30.07102, -…\n3 22095070800 Census Tra… Asian        0          1930 (((-90.54614 30.07102, -…\n4 22095070800 Census Tra… Hispanic    32          1930 (((-90.54614 30.07102, -…\n5 22095070300 Census Tra… White     2497          6123 (((-90.5018 30.07859, -9…\n6 22095070300 Census Tra… Black     2655          6123 (((-90.5018 30.07859, -9…\n\n\nThe data looks promising. Opening the new CancerAlley table shows that there are census tracts from each of the parishes listed. Figure 15.3 shows the same style of display for Cancer Alley as display for Jefferson County, TX. Note that Ascenscion and Iberville Parishes may warrant inclusion as well.\n\nCancerAlley %&gt;%\n  #Create a percent variable for the four racial categories\n  mutate(percent = 100 * (value / summary_value)) %&gt;%\n  ggplot(aes(fill = percent)) +\n  facet_wrap(~variable) +\n  #included tract lines for clarity\n  geom_sf(color = 'black') +\n  theme_void() + \n  scale_fill_viridis_c() + \n  labs(fill = \"% of population\\n(2020 Census)\")\n\n\n\n\n\n\nFigure 15.3: Racial demographic information for Cancer Alley from the decennial census for 2020.",
    "crumbs": [
      "Home",
      "Unit 3: Data",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Importing Data - Part 2</span>"
    ]
  },
  {
    "objectID": "assignment3.html",
    "href": "assignment3.html",
    "title": "16  Assignments for Unit 3 - Data",
    "section": "",
    "text": "16.1 Group Presentation\nDue to present in class on October 29, 2024. Teams of approximately 4 students. End product is a 6-10 minute presentation in class using slides. It will include descriptions of the plan for individual and group data visualizations.\nIt should include approximately:",
    "crumbs": [
      "Home",
      "Unit 3: Data",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Assignments for Unit 3 - Data</span>"
    ]
  },
  {
    "objectID": "assignment3.html#group-presentation",
    "href": "assignment3.html#group-presentation",
    "title": "16  Assignments for Unit 3 - Data",
    "section": "",
    "text": "Note\n\n\n\nAbsence from in-class presentation will need a Doctor’s note or similar documentation to be able to make-up this assignment during Office Hours.\n\n\n\n\n2 minutes of introduction and background\n2 minutes defining the scope of your Project\n\nSpatial extent (e.g., state of California)\nTemporal extent (i.e., year-range)\nEnvironmental variable(s) to be examined (air pollution, water, radiation, climate, etc.)\n\n\n2 minutes on what the team’s goal is and what data will be acquired for communicating that story.\n2 minutes on the visualization strategies that will help tell the story. What will the project look like (hand-drawn sketches or design mock-ups are fine).\n\n\n16.1.1 Grading for Group Presentation Project\nTotal of 150 points.\n\n30 points from other group members (participation grade - anonymous)\n60 points for quality of presentation slides\n\n10 points for spelling/grammar/formatting\n30 points for content, text, and visualizations\n20 points for covering 4 topic areas\n\n\n60 points for presentation\n\n20 points for covering 4 topic areas\n10 points for having approximately equal talking time among team members\n30 points for content mastery and for answering questions from classmates and Professor Mike",
    "crumbs": [
      "Home",
      "Unit 3: Data",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Assignments for Unit 3 - Data</span>"
    ]
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "17  Research sites",
    "section": "",
    "text": "17.1 Group Project Topics",
    "crumbs": [
      "Home",
      "Unit 4: Special Projects",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Research sites</span>"
    ]
  },
  {
    "objectID": "research.html#group-project-topics",
    "href": "research.html#group-project-topics",
    "title": "17  Research sites",
    "section": "",
    "text": "Air Quality Infographic - focused on PM2.5 and CO2\nPM2.5 Concentrations in and around Fontana and Bloomington\nPM2.5 Emissions sources in and around Fontana and Bloomington\nSpatial Visualization of Road Widening and Warehouse Growth in IE (or just one county - Riverside or San Bernardino)\nSocioeconomic Statistics of Riverside County Warehouses",
    "crumbs": [
      "Home",
      "Unit 4: Special Projects",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Research sites</span>"
    ]
  },
  {
    "objectID": "research.html#overview-of-the-iqair-ccaej-redford-conservancy-monitoring-study",
    "href": "research.html#overview-of-the-iqair-ccaej-redford-conservancy-monitoring-study",
    "title": "17  Research sites",
    "section": "17.2 Overview of the IQAir CCAEJ Redford Conservancy Monitoring Study",
    "text": "17.2 Overview of the IQAir CCAEJ Redford Conservancy Monitoring Study\nFrom the grant application, which is in Canvas.\nCCAEJ will engage with the community of South Fontana/Bloomington to collaborate on an inclusive, community-centered planning process that can maximize the benefits of the real-time air monitoring data collection, analysis and advocacy steps. The planning process will involve participatory decision-making that promotes involvement and empowers a diverse segment of the populace, with a view of actively involving lower income populations and communities of color, empowering those most affected to shape the decisions that will impact their lives. Preliminary, mid-project and post-project semi-structured interviews will be conducted with residents (N:5-10) to identify understandings of pollutants and causes, and subjective experiences with air quality issues, the planning process, and advocacy efforts.\nThe Redford Conservancy will assist with data analysis and visualization, providing mapping in GIS, creating maps of areas as well as visualization of the findings of the research over the course of the grants. The RRC will work with CCAEJ and IQAir Foundation to tailor visualizations to appropriate public communication in both English and in Spanish. In addition, RRC will gather ethnographic data through interviews to round out local knowledge of air quality perceptions, understandings, and experience. Faculty and students will engage in research in both English and Spanish, translating findings into English.\nGoals:\n\nEducate and empower identified community members living within the targeted area to recognize air quality issues in relation to their community and the consequential health risks.\nDevelop a network of air quality monitors in identified communities to fill the air quality data gap within targeted areas.\nGenerate data and a better understanding of air quality issues that the local community and regulators can use to spur enforcement or regulatory action.\nUse the information generated by the sensor network to improve community health by letting residents know when to limit their exposure in certain areas or during different times when air pollution is elevated.\n\n\n17.2.1 Infographic Sources\n\nAir Quality Index\nAir Quality Communication Presentation\nBest Practices for Communicating sensor data\nIQAir Foundation\n\nNext Tuesday we’ll cover Air Quality 101 as part of our lecture.\n\n\n17.2.2 Concentrations of PM2.5 and CO2\n\nPM2.5 data sources include regulatory data and sensor data.\nIQAir API - requires registration and has limits.\nPurpleAir\nEPA AQS API - check Canvas, I will put all the PM2.5 concentration data for 2020-2024 for Riverside and San Bernardino Counties on there.\n\nNOAA GMD - also look at the way we pulled data in 1.5 and for methane in 12.2.5.\n\n\n\n17.2.3 Emissions of PM2.5 and CO2\n#fig-SCAQMD_EI shows the total emissions by source category.\n\n\n\n\n\n\nFigure 17.1: Sources of PM2.5\n\n\n\n\nSCAQMD PM Air Quality Management Plan Ch.3 Emissions Inventory\nSCAQMD Facilities\nEMFAC2021 on-road and off-road database\nRestaurants? I didn’t find a good open source dataset for restaurant info. There’s almost certainly a way to get this via OpenStreetMap. Needs more research.\nRoads and Rail - possibly from TIGRIS or from Caltrans\n\n\n\n17.2.4 Road Widening Projects and Warehouse Data\n\nRiverside County\nSan Bernardino County - check current, completed, planning, and under construction.\nSee 10.3.2 for an example of the warehouse dataset.\n\n\n\n17.2.5 Socioeconomic data\n\ntidycensus has the most recent data\nSoCalEJ has census block level socioeconomic data from 2015-2019.\n\n\n\n17.2.6 Community Organization Data\n\nComing Soon",
    "crumbs": [
      "Home",
      "Unit 4: Special Projects",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Research sites</span>"
    ]
  },
  {
    "objectID": "AirQuality.html",
    "href": "AirQuality.html",
    "title": "18  Air Quality",
    "section": "",
    "text": "18.1 Layers of the Atmosphere\nThe atmospheric layers are characterized by differences in average temperature and pressure that define boundaries.\nWe live in the troposphere and air quality problems associated with people breathing are all there. The ozone hole is in the stratosphere. Stuff going on above the stratosphere may as well be in outer space.\nFigure 18.1 shows the temperature profile of the atmosphere.\nknitr::include_graphics('https://www.noaa.gov/sites/default/files/2023-05/atmprofile.jpg')\n\n\n\n\n\n\nFigure 18.1: NOAA Atmospheric Layers - from https://www.noaa.gov/jetstream/atmosphere",
    "crumbs": [
      "Home",
      "Unit 4: Special Projects",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Air Quality</span>"
    ]
  },
  {
    "objectID": "AirQuality.html#composition-of-the-atmosphere",
    "href": "AirQuality.html#composition-of-the-atmosphere",
    "title": "18  Air Quality",
    "section": "\n18.2 Composition of the Atmosphere",
    "text": "18.2 Composition of the Atmosphere\nThe atmosphere is mostly molecular nitrogen (N2) at 78% and molecular oxygen (O2) at 21%. Argon is just under 1%. Water vapor (H2O) is at 0.4%.\nEverything else is at trace levels - less than 0.1%. Carbon dioxide (CO2) is at 0.042%. A bunch of the noble gases (Ne, He, Kr, Xe), methane, and a few other pollutants like ozone and carbon monoxide are at part per million levels.\nSee: NOAA for a quantitative table of the main components of the atmosphere.",
    "crumbs": [
      "Home",
      "Unit 4: Special Projects",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Air Quality</span>"
    ]
  },
  {
    "objectID": "AirQuality.html#chemistry-of-the-atmosphere",
    "href": "AirQuality.html#chemistry-of-the-atmosphere",
    "title": "18  Air Quality",
    "section": "\n18.3 Chemistry of the Atmosphere",
    "text": "18.3 Chemistry of the Atmosphere\nMolecular nitrogen and oxygen are pretty unreactive, but sometimes they get zapped with sufficient energy from the sun’s photons or lightning to start chemical reactions.\nAtmospheric constituents that are chemically stable and long-lived get transported long-distances and become well-mixed over long-distances. Constituents that are short-lived and/or chemically reactive have sharp gradients in concentrations.\nFigure 18.2 from Atmospheric Chemistry and Physics illustrates this idea.\n\n\n\n\n\nFigure 18.2: Senfeld and Pandis Figure 1.17",
    "crumbs": [
      "Home",
      "Unit 4: Special Projects",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Air Quality</span>"
    ]
  },
  {
    "objectID": "AirQuality.html#lets-talk-about-particulate-matter",
    "href": "AirQuality.html#lets-talk-about-particulate-matter",
    "title": "18  Air Quality",
    "section": "\n18.4 Let’s talk about Particulate Matter",
    "text": "18.4 Let’s talk about Particulate Matter\nThis is a decent overview from Urban Emissions Info.\nLet’s go there and see how they discuss it.\nHere’s the US EPA and California Air Resources Board pages.\nHere’s a complicated paper comparing particulates in six major metropolitan areas.\nAnd here’s a complicated Nature paper that shows that within metropolitan area spatial variability in concentration (i.e. intra-urban) is smaller by at least a factor of 2 than between metropolitan area variations (i.e. inter-urban).\n\n18.4.1 PM2.5 emissions sources you can see\n\n\nDiesel PM from trucks, locomotives, cargo-handling equipment, generators, and construction equipment - Wood/vegetation burning from wildfires or fireplaces.\nBarbecue, grilling, smoking\nCigarettes, vaping, pipes, etc.\nIndustrial smokestacks (sometimes).\n\nFacility emissions are available from the US EPA National Emissions Inventory. Check Canvas for an xlsx spreadsheet for NEI emissions data if you are in that group.\n\n18.4.2 PM2.5 emissions sources you can’t see\n\nGaseous precursors that react in the atmosphere to form PM\n\nNOx, SO2, VOCs, NH3.\n\nUltrafine particles that are too small to absorb or scatter light. Note, this can give that appearance of haze when you look through the atmosphere towards a landmark, but it is obscured.",
    "crumbs": [
      "Home",
      "Unit 4: Special Projects",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Air Quality</span>"
    ]
  },
  {
    "objectID": "joins.html",
    "href": "joins.html",
    "title": "19  Joins",
    "section": "",
    "text": "19.1 Load libraries\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(sf)\n\nLinking to GEOS 3.12.1, GDAL 3.8.4, PROJ 9.3.1; sf_use_s2() is TRUE\n\nlibrary(leaflet)",
    "crumbs": [
      "Home",
      "Unit 4: Special Projects",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Joins</span>"
    ]
  },
  {
    "objectID": "joins.html#california-place-boundaries",
    "href": "joins.html#california-place-boundaries",
    "title": "19  Joins",
    "section": "\n19.2 California Place Boundaries",
    "text": "19.2 California Place Boundaries\nAll of the groups are using data from SoCal. We could grab a dataset from SCAG. This data is from 2019 though, so boundaries may not be up-to-date.\nHowever, instead we’re going to learn a different approach and import the data using the tigris package. Hat tip and extra credit to Anna for using this in her assignment.\nFirst, we need to install the new package.\n\ninstall.packages('tigris')\n\nThen, once installed, we load the library.\n\nlibrary(tigris)\n\nTo enable caching of data, set `options(tigris_use_cache = TRUE)`\nin your R script or .Rprofile.\n\n\nAs always, there’s a bunch of cool functions. The one we will use is called places(). It pulls cities and census designated places geospatial information.\nIn this example, I\n\n#read the data\nCA_places &lt;- places(state = 'CA', cb = T, #pull for California, lower detail level\n                    year = 2022, \n                    progress_bar = FALSE) |&gt; \n  st_transform(crs = 4326)\n  \nhead(CA_places)\n\nSimple feature collection with 6 features and 12 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -120.5913 ymin: 33.05765 xmax: -116.9939 ymax: 35.22005\nGeodetic CRS:  WGS 84\n  STATEFP PLACEFP  PLACENS         AFFGEOID   GEOID         NAME\n1      06   55156 02411359 1600000US0655156 0655156     Palmdale\n2      06   22804 02410455 1600000US0622804 0622804    Escondido\n3      06   02924 02409738 1600000US0602924 0602924        Arvin\n4      06   16350 02410232 1600000US0616350 0616350       Corona\n5      06   43224 02410875 1600000US0643224 0643224 Los Alamitos\n6      06   31414 02410672 1600000US0631414 0631414    Guadalupe\n           NAMELSAD STUSPS STATE_NAME LSAD     ALAND AWATER\n1     Palmdale city     CA California   25 274705334 629569\n2    Escondido city     CA California   25  96726218 276445\n3        Arvin city     CA California   25  12482061      0\n4       Corona city     CA California   25 103320711  53926\n5 Los Alamitos city     CA California   25  10380819 161830\n6    Guadalupe city     CA California   25   3394103  10948\n                        geometry\n1 MULTIPOLYGON (((-118.2877 3...\n2 MULTIPOLYGON (((-117.0237 3...\n3 MULTIPOLYGON (((-118.8511 3...\n4 MULTIPOLYGON (((-117.673 33...\n5 MULTIPOLYGON (((-118.093 33...\n6 MULTIPOLYGON (((-120.5912 3...\n\n\nFigure 19.1 shows a map of Cities using leaflet.\n\nleaflet() |&gt; \n  addTiles() |&gt; \n  addPolylines(data = CA_places)\n\n\n\n\n\n\nFigure 19.1: California cities and census designated place boundaries\n\n\n\nThat’s just too much information. For the Fontana and Bloomington projects, we probably want to just show those two communities. Let’s use the %in% operator to filter() those two places from the list.\nFigure 19.2 shows a leaflet map of the cities, zoomed into the main valley.\n\nFontanaBloomington &lt;- CA_places |&gt; \n  filter(NAME %in% c('Fontana', 'Bloomington'))\n\n  leaflet() |&gt; \n  addTiles() |&gt; \n  addPolygons(data = FontanaBloomington,\n              color = 'black',\n              weight = 1,\n              fillOpacity = 0.3,\n              label = ~NAME)\n\n\n\n\n\n\nFigure 19.2: Fontana and Bloomington on a leaflet map",
    "crumbs": [
      "Home",
      "Unit 4: Special Projects",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Joins</span>"
    ]
  },
  {
    "objectID": "joins.html#spatial-joins",
    "href": "joins.html#spatial-joins",
    "title": "19  Joins",
    "section": "\n19.3 Spatial Joins",
    "text": "19.3 Spatial Joins\nSpatial joins use geospatial information to identify overlapping geometries in space. Thus, if one has two geospatial datasets, one can quickly identify intersections of interest.\n\n19.3.1 Example 1 - Spatial Joins\nLet’s pick all the cities and census designated places in Los Angeles County.\nStep 1 - pull Los Angeles County spatial boundaries using the county\n\n#read the data\nLA_county &lt;- counties(state = 'CA', cb = T, progress_bar = FALSE) |&gt;  #pull for California, lower detail level, year = 2022 ) \n  st_transform(crs = 4326) |&gt; \n  filter(NAME == 'Los Angeles') # Just pick LA County\n\nRetrieving data for the year 2022\n\n\nCheck if it worked by making a basic leaflet map. Figure 19.3\n\nleaflet() |&gt; \n  addTiles() |&gt; \n  addPolygons(data = LA_county,\n              color = 'black')\n\n\n\n\n\n\nFigure 19.3: Los Angeles County on a leaflet map\n\n\n\nGreat!\nNow let’s spatially filter the Los Angeles places using st_filter() from the sf package.\nThe syntax is to provide the basic dataset to filter first, then the geospatial polygon to filter on second.\n\nLA_places &lt;- CA_places |&gt; \n  st_filter(LA_county, #\n            .predicate = st_covered_by) # This adjective includes fully 'covered' places - not ones that merely intersect partially, \n# which is the default\n\nhead(LA_places)\n\nSimple feature collection with 6 features and 12 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -118.6682 ymin: 33.70515 xmax: -117.9162 ymax: 34.76915\nGeodetic CRS:  WGS 84\n  STATEFP PLACEFP  PLACENS         AFFGEOID   GEOID            NAME\n1      06   55156 02411359 1600000US0655156 0655156        Palmdale\n2      06   71806 02411897 1600000US0671806 0671806    Sierra Madre\n3      06   19990 02410361 1600000US0619990 0619990          Duarte\n4      06   44000 02410877 1600000US0644000 0644000     Los Angeles\n5      06   45400 02411020 1600000US0645400 0645400 Manhattan Beach\n6      06   40130 02411620 1600000US0640130 0640130       Lancaster\n              NAMELSAD STUSPS STATE_NAME LSAD      ALAND   AWATER\n1        Palmdale city     CA California   25  274705334   629569\n2    Sierra Madre city     CA California   25    7643252    10258\n3          Duarte city     CA California   25   17376217        0\n4     Los Angeles city     CA California   25 1218634998 80417875\n5 Manhattan Beach city     CA California   25   10189493 16389932\n6       Lancaster city     CA California   25  244189279   651713\n                        geometry\n1 MULTIPOLYGON (((-118.2877 3...\n2 MULTIPOLYGON (((-118.0682 3...\n3 MULTIPOLYGON (((-117.9902 3...\n4 MULTIPOLYGON (((-118.6682 3...\n5 MULTIPOLYGON (((-118.4228 3...\n6 MULTIPOLYGON (((-118.3252 3...\n\n\nOk, let’s make a map to see if these are just the correct cities.\nFigure 19.4 shows the place and city boundaries in blue, and LA County in red.\n\nleaflet() |&gt; \n  addTiles() |&gt; \n  addPolylines(data = LA_county,\n              color = 'red') |&gt; \n  addPolygons(data = LA_places,\n              color = 'blue',\n              fillOpacity = 0.2,\n              label = ~NAME,\n              weight = 2)\n\n\n\n\n\n\nFigure 19.4: Los Angeles County and its cities and places on a leaflet map.",
    "crumbs": [
      "Home",
      "Unit 4: Special Projects",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Joins</span>"
    ]
  },
  {
    "objectID": "joins.html#database-joins",
    "href": "joins.html#database-joins",
    "title": "19  Joins",
    "section": "\n19.4 Database Joins",
    "text": "19.4 Database Joins\nDataset CA_places has the geospatial information on city and census designated place boundaries. Some of the other datasets you are using have the information you would like to compare. How can we merge or join these disparate data layers for visualization and analysis?\nThere are two types of joins that can help to merge disparate datasets.\nThe first type of join is based on unique records in both datasets. If you have a column in dataset A and it has some matching records in dataset B, there are ways to join the two datasets.\nHere’s all the permutations. - inner_join() - keep only records present in both dataset A and B. - left_join() - keep all records from the dataset A and only matching variables from B. Fill missing with NA. - right_join() - keep all records from the dataset B and only matching variables from A. Fill missing with NA. - full_join() - keep all records from A and B - fill NA in any records where dataset is missing from one of the datasets. - anti_join() - keep only records from A that don’t occur in B.\nThe example below shows Venn diagrams of all the permutations.\n\n\nJoin Venn Diagrams - credit Tavareshugo github\n\n\n19.4.1 Example #2 - population data\nThe California Department of Finance provides city and place information on households and populations. I’d like to map population of cities and places using the latest and greatest data.\nFirst, let me scrape the data using a new package for reading MS Excel spreadsheets - readxl.\nWe need to install the new package which reads Excel files.\n\ninstall.packages('openxlsx')\n\nThen, once installed, we load the library.\n\nlibrary(openxlsx)\n\nScrape the data using the read.xlsx() function from openxlsx\n\nTbl_E1 &lt;- read.xlsx(xlsxFile = 'https://dof.ca.gov/wp-content/uploads/sites/352/Forecasting/Demographics/Documents/E-1_2024_InternetVersion.xlsx',                   sheet = 'E-1 CityCounty2024', #multiple sheets, we need to specify the sheetname\n                    rows = 5:599) |&gt; # this is filtering for rows with decent data\n  rename(place = 'State/County/City',\n         pop2023 = '1/1/2023',\n         pop2024 = '1/1/2024')\n\nhead(Tbl_E1)\n\n       place  pop2023  pop2024     Change\n1 California 39061058 39128162  0.1717926\n2    Alameda  1650656  1641869 -0.5323338\n3    Alameda    77237    78071  1.0797934\n4     Albany    20354    20325 -0.1424781\n5   Berkeley   125181   125327  0.1166311\n6     Dublin    72681    72917  0.3247066\n\n\nAlright, we have population data and place names. Let’s see if we can show city populations using a join function.\n\nLA_places_pop &lt;- LA_places |&gt; \n  left_join(Tbl_E1, by = c('NAME' = 'place')) |&gt; # this tells it what columns to join the two tables by\n  filter(!is.na(pop2024)) |&gt; # remove census places with no population data in this table\n  filter(NAME != 'Los Angeles') # remove Los Angeles City because it is too big\n\npalPop &lt;- colorNumeric(domain = LA_places_pop$pop2024,\n                       palette = 'Spectral', reverse = T)\n\nTime for a map. Figure 19.5 shows population of cities in Los Angeles County other than the city of Los Angeles.\n\nleaflet() |&gt; \n  addProviderTiles(provider = providers$CartoDB.Positron) |&gt; \n  addPolygons(data = LA_places_pop,\n              color = ~palPop(pop2024),\n              fillOpacity = 0.8,\n              label = ~str_c(NAME, ', ', pop2024), # add concatenated string label with city name and population\n              weight = 2) |&gt; \n  addLegend(data = LA_places_pop,\n            title = '2024 population',\n            pal = palPop,\n            values = ~pop2024)\n\n\n\n\n\n\nFigure 19.5: Los Angeles city populations other than leaflet map.",
    "crumbs": [
      "Home",
      "Unit 4: Special Projects",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Joins</span>"
    ]
  },
  {
    "objectID": "roads.html",
    "href": "roads.html",
    "title": "20  Roads",
    "section": "",
    "text": "20.1 Load libraries\nlibrary(tigris)\n\nTo enable caching of data, set `options(tigris_use_cache = TRUE)`\nin your R script or .Rprofile.\n\nlibrary(sf)\n\nLinking to GEOS 3.12.1, GDAL 3.8.4, PROJ 9.3.1; sf_use_s2() is TRUE\n\nlibrary(leaflet)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\nknitr::opts_chunk$set(message = NA)",
    "crumbs": [
      "Home",
      "Unit 4: Special Projects",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Roads</span>"
    ]
  },
  {
    "objectID": "roads.html#load-libraries",
    "href": "roads.html#load-libraries",
    "title": "20  Roads",
    "section": "",
    "text": "20.1.1 Tigris function for scraping road data.\nThe aptly named roads() function will pull road linestrings.\n\nRiv_roads &lt;- roads(state = 'CA', county = 'Riverside', year = 2023, progress_bar = FALSE) |&gt; \n  st_transform(crs = 4326)\n\nhead(Riv_roads)\n\nSimple feature collection with 6 features and 4 fields\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: -117.5613 ymin: 33.49898 xmax: -116.3334 ymax: 33.87767\nGeodetic CRS:  WGS 84\n       LINEARID               FULLNAME RTTYP MTFCC\n1  110458254891       Armstrong Dr Exd     M S1400\n2  110458266265 Gabino Canyon Rd N Spr     M S1400\n3 1102175699783           Porphyry Spr     M S1400\n4  110458248901        Pub Utility Acc     M S1400\n5 1103677442858        White Horse Pvt     M S1740\n6 1103678156677            Bus Park Dr     M S1400\n                        geometry\n1 LINESTRING (-117.5613 33.83...\n2 LINESTRING (-117.51 33.5231...\n3 LINESTRING (-117.5396 33.87...\n4 LINESTRING (-117.1296 33.50...\n5 LINESTRING (-116.3343 33.75...\n6 LINESTRING (-117.1577 33.49...\n\n\nThere are 58,448 road segments from 2023. That’s a lot of lines for a map and takes too long to load. Feel free to make a map with it if you want to hear your computer hum.\nFor the purpose of the road widening project, we’re mostly focusing on big roads getting widened. The documentation for roads() states that the RTTYP category provides six types.\nI’m going to filter just the Interstate (I) and State (S) roads for this version.\n\nRiv_major_roads &lt;- Riv_roads |&gt; \n  filter(RTTYP %in% c('I', 'S'))\n\nThere are only 81 major roads segments that meet that criteria.\n\n  leaflet() |&gt; \n  addTiles() |&gt; \n  addPolylines(data = Riv_major_roads,\n              color = 'black',\n              weight = 3,\n              dashArray = '2,6', #new feature - makes lines dashed - line lenght, space length\n              label = ~FULLNAME)\n\n\n\n\n\n\nFigure 20.1: Riverside roads on a leaflet map\n\n\n\nNow let’s add Riverside County warehouses for a quick look at the proximity of those facilities to current major roadways. We did this in lesson 10.3.2\n\nWH.url &lt;- 'https://raw.githubusercontent.com/RadicalResearchLLC/WarehouseMap/main/WarehouseCITY/geoJSON/comboFinal.geojson'\nwarehouses &lt;- st_read(WH.url) |&gt;  \n  st_transform(crs = 4326) |&gt; \n  filter(county == 'Riverside County')\n\nReading layer `comboFinal' from data source \n  `https://raw.githubusercontent.com/RadicalResearchLLC/WarehouseMap/main/WarehouseCITY/geoJSON/comboFinal.geojson' \n  using driver `GeoJSON'\nSimple feature collection with 9084 features and 8 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -118.8037 ymin: 33.43325 xmax: -114.4085 ymax: 35.55527\nGeodetic CRS:  WGS 84\n\n\nFigure 20.2 shows the major roads and warehouse land-use.\n\npalWH &lt;- colorFactor(palette = c('orange', 'gold', 'brown'),\n                     domain = warehouses$category)\npalRoadType &lt;- colorFactor(palette = c('darkblue', 'purple'),\n                           domain = Riv_major_roads$RTTYP)\n\nleaflet() |&gt; \n  addProviderTiles(provider = providers$CartoDB.Positron) |&gt; ## We modified this line to change to satellite view  \n  addPolygons(data = warehouses,\n              color = ~palWH(category),\n              weight = 1,\n              fillOpacity = 0.7) |&gt; \n  addLegend(data = warehouses,\n            pal = palWH,\n            values = ~category) |&gt; \n  addPolylines(data = Riv_major_roads,\n            color = ~palRoadType(RTTYP),\n            weight = 3,\n            dashArray = '2,6',\n            label = ~FULLNAME) |&gt; \n  setView(zoom = 10, lat = 33.8, lng = -117.15) \n\n\n\n\n\n\nFigure 20.2: Riverside roads and warehouses on a leaflet map\n\n\n\nThis is a good start, but there’s some missing pieces that we can show a few examples for road widening.\nLet’s filter() down a data table for a couple of key roads that are approved or under review for road widening.\n\nroads2widen &lt;- Riv_roads |&gt; \n  filter(FULLNAME %in% c('Cajalco Rd', 'Cajalco Expy', 'Ramona Expy', 'Gilman Springs Rd'))\n\nThis selects just a few individual county level roads. Let’s add those to a new map.\nFigure 20.3 shows the result.\n\npalWH &lt;- colorFactor(palette = c('orange', 'gold', 'brown'),\n                     domain = warehouses$category)\npalRoadType &lt;- colorFactor(palette = c('darkblue', 'purple'),\n                           domain = Riv_major_roads$RTTYP)\n\nleaflet() |&gt; \n  addProviderTiles(provider = providers$CartoDB.Positron) |&gt; ## We modified this line to change to satellite view  \n  addPolygons(data = warehouses,\n              color = ~palWH(category),\n              weight = 1,\n              fillOpacity = 0.7) |&gt; \n  addLegend(data = warehouses,\n            pal = palWH,\n            values = ~category,\n            position = 'bottomleft') |&gt; \n  addPolylines(data = Riv_major_roads,\n            color = ~palRoadType(RTTYP),\n            weight = 3,\n            dashArray = '2,6',\n            label = ~FULLNAME) |&gt; \n  setView(zoom = 10, lat = 33.8, lng = -117.15) |&gt; \n  addPolylines(data = roads2widen,\n               color = 'red',\n               weight = 4,\n              dashArray = '2,6',\n              label = ~FULLNAME) |&gt; \n  addLegend(title = 'Roads',\n            colors = c('darkblue', 'purple', 'red'),\n            labels = c('Interstate', 'State Route', 'Road widening'),\n            position = 'bottomleft')\n\n\n\n\n\n\nFigure 20.3: Riverside roads and widening roads with warehouses on a leaflet map",
    "crumbs": [
      "Home",
      "Unit 4: Special Projects",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Roads</span>"
    ]
  },
  {
    "objectID": "shiny.html",
    "href": "shiny.html",
    "title": "21  Shiny Apps",
    "section": "",
    "text": "21.1 Shiny\nshiny is an R package that is used to build interactive web apps. In the modern day, it is useful to be able to build visualizations that others can view on the internet(s).\nI will show you how to build a simple web app using the SoCalEJ dataset today.\nFirst, we install shiny.\ninstall.packages('shiny')\nSecond, we restart RStudio so that the shiny template is loaded into RStudio.\nThree, we load some libraries.\nlibrary(tidyverse)\nlibrary(shiny)\nlibrary(sf)\nlibrary(leaflet)",
    "crumbs": [
      "Home",
      "Unit 4: Special Projects",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Shiny Apps</span>"
    ]
  },
  {
    "objectID": "shiny.html#shiny",
    "href": "shiny.html#shiny",
    "title": "21  Shiny Apps",
    "section": "",
    "text": "Shiny Firefly Reference\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nClose and restart RStudio!",
    "crumbs": [
      "Home",
      "Unit 4: Special Projects",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Shiny Apps</span>"
    ]
  },
  {
    "objectID": "shiny.html#prepare-the-dataset",
    "href": "shiny.html#prepare-the-dataset",
    "title": "21  Shiny Apps",
    "section": "\n21.2 Prepare the dataset",
    "text": "21.2 Prepare the dataset\nFor most apps, we want to prepare the dataset for display. While we can technically do all the import and munging in shiny within an app, it is often better to do the pre-processing of data first, as it will improve the app performance to not have to download data and munge it to make a figure.\nWe will acquire and tidy the SoCalEJ dataset we’ve used many times.\nAcquire the data - same as in Section 9.1\n\nURL.path &lt;- 'https://raw.githubusercontent.com/RadicalResearchLLC/EDVcourse/main/CalEJ4/CalEJ.geoJSON'\nSoCalEJ &lt;- st_read(URL.path) |&gt; \n  st_transform(\"+proj=longlat +ellps=WGS84 +datum=WGS84\")\n\nReading layer `CalEJ' from data source \n  `https://raw.githubusercontent.com/RadicalResearchLLC/EDVcourse/main/CalEJ4/CalEJ.geoJSON' \n  using driver `GeoJSON'\nSimple feature collection with 3747 features and 66 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 97418.38 ymin: -577885.1 xmax: 539719.6 ymax: -236300\nProjected CRS: NAD83 / California Albers\n\n\nTidy the data by transforming it from a wide to a narrow dataset - much the same as in Section 11.3 - Tidy and Transform.\n\n# select indicators and make them narrow - we remove geometry because it \n# duplicates geometry in the pivot\nSoCal_narrow1 &lt;- SoCalEJ |&gt; \n  st_set_geometry(value = NULL) |&gt; \n  pivot_longer(cols = c(5:66), names_to = 'variable', values_to = 'value') |&gt; \n  filter(value &gt;=0)\n\nhead(SoCal_narrow1, 5)\n\n# A tibble: 5 × 6\n       Tract   ZIP County ApproxLoc   variable     value\n       &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;       &lt;chr&gt;        &lt;dbl&gt;\n1 6059062640 92656 Orange Aliso Viejo TotPop19 3741     \n2 6059062640 92656 Orange Aliso Viejo CIscore     9.64  \n3 6059062640 92656 Orange Aliso Viejo CIscoreP   12.1   \n4 6059062640 92656 Orange Aliso Viejo Ozone       0.0517\n5 6059062640 92656 Orange Aliso Viejo OzoneP     65.4   \n\n# Select census tracts and geometry\n\nSoCal_geometry &lt;- SoCalEJ |&gt; \n  select(Tract, geometry)\n\nhead(SoCal_geometry, 5)\n\nSimple feature collection with 5 features and 1 field\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -117.874 ymin: 33.5556 xmax: -117.7157 ymax: 33.64253\nGeodetic CRS:  +proj=longlat +ellps=WGS84 +datum=WGS84\n       Tract                       geometry\n1 6059062640 MULTIPOLYGON (((-117.7178 3...\n2 6059062641 MULTIPOLYGON (((-117.7166 3...\n3 6059062642 MULTIPOLYGON (((-117.8596 3...\n4 6059062643 MULTIPOLYGON (((-117.7986 3...\n5 6059062644 MULTIPOLYGON (((-117.8521 3...\n\n# Join narrow data and geometry again - this adds the geometry back in for the map\n\nSoCal_narrow &lt;- SoCal_narrow1 |&gt; \n  left_join(SoCal_geometry) |&gt;   \n  #this fixes the dataset to \n  st_as_sf() |&gt; \n  #standard projection transformation.\n  st_transform(\"+proj=longlat +ellps=WGS84 +datum=WGS84\")\n\nhead(SoCal_narrow, 5)\n\nSimple feature collection with 5 features and 6 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -117.7306 ymin: 33.5556 xmax: -117.7163 ymax: 33.57209\nGeodetic CRS:  +proj=longlat +ellps=WGS84 +datum=WGS84\n# A tibble: 5 × 7\n       Tract   ZIP County ApproxLoc   variable   value                  geometry\n       &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt;        &lt;MULTIPOLYGON [°]&gt;\n1 6059062640 92656 Orange Aliso Viejo TotPop19 3.74e+3 (((-117.7178 33.55696, -…\n2 6059062640 92656 Orange Aliso Viejo CIscore  9.64e+0 (((-117.7178 33.55696, -…\n3 6059062640 92656 Orange Aliso Viejo CIscoreP 1.21e+1 (((-117.7178 33.55696, -…\n4 6059062640 92656 Orange Aliso Viejo Ozone    5.17e-2 (((-117.7178 33.55696, -…\n5 6059062640 92656 Orange Aliso Viejo OzoneP   6.54e+1 (((-117.7178 33.55696, -…\n\n# remove interim dataframes to optimize dataset management\nrm(ls = SoCal_geometry, SoCalEJ)\n\nLast data processing step is to create a list of variables and arrange them alphabetically. Use select() to keep only the variable, distinct() to keep only one instance of each variable, and arrange() to order them.\n\nlist &lt;- SoCal_narrow1 |&gt; \n  select(variable) |&gt; \n  distinct() |&gt; \n  arrange(variable)\n\nhead(list, 10)\n\n# A tibble: 10 × 1\n   variable  \n   &lt;chr&gt;     \n 1 AAPI      \n 2 AfricanAm \n 3 Asthma    \n 4 AsthmaP   \n 5 CIscore   \n 6 CIscoreP  \n 7 Cardiovas \n 8 CardiovasP\n 9 Child_10  \n10 Cleanup",
    "crumbs": [
      "Home",
      "Unit 4: Special Projects",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Shiny Apps</span>"
    ]
  },
  {
    "objectID": "shiny.html#shiny-time",
    "href": "shiny.html#shiny-time",
    "title": "21  Shiny Apps",
    "section": "\n21.3 Shiny time",
    "text": "21.3 Shiny time\n\n21.3.1 Resources on Shiny\nTutorial Cheatsheet\n\n21.3.2 Step 1 - New file - Shiny App\n\nGo to the File menu\nSelect New File\n\nSelect Shiny Web App…\n\nA window will pop up asking you to name the app - type SoCalEJ\n\nPress the Create button\n\nFigure 21.1 shows where to go.\n\n\n\n\n\nFigure 21.1: New Shiny App from the menu\n\n\n\n21.3.3 Step 2 - Shiny App Default Template\nA new file called app.R should open. The default app.R template is a functional app.\nPress Run App as shown in Figure 21.2 .\n\n\n\n\n\nFigure 21.2: Run app button is here\n\n\nWhen you press the button, an App called ‘Old Faithful Geyser Data’ appears. The slider input is interactive, allowing the user to choose the number of histogram bins.\nLet’s hear a Whoot if this worked for you.\nClose the app (not minimize!) by pressing the STOP button in RStudio or closing the app window.\n\n21.3.4 Step 3 - Modify the app.\nWe are going to change things one at a time. Debugging shiny apps is even harder than normal R code.\n\n21.3.4.1 Change the title\nLet’s change the title first. Currently it is: titlePanel(\"Old Faithful Geyser Data\"),\nReplace the existing line 16 with the new title.\n\ntitlePanel(\"SoCal Enviroscreen App\"),\n\n\n21.3.4.2 Replace sliderInput() with selectInput()\n\nWhile sliderInput() is great for numbers, it is not useful for selecting categorical variables like the SoCalEJ dataset contains.\nReplace code lines 21-25 with a selectInput() shown below. This creates a dropdown menu. We will design it to allow the user to select the variable of interest list$variable and the default selection DieselPM_P.\nHowever, it will also break the figure below. That’s ok because we’re going to replace it in the next two steps.\n\n    selectInput(inputId = 'metric',\n      label = 'Pick a metric',\n      choices = list$variable, \n      selected = 'DieselPM_P')\n\nFigure 21.3 displays the output when running the app after this change.\n\n\n\n\n\nFigure 21.3: Broken figure dropdown version\n\n\n\n21.3.4.3 Change the histogram output to SoCal_narrow1 data\nLet’s make a working figure. A histogram is fine as an example, but we’ll do a ggplot following the style of Figure 11.8.\nLet’s use that code chunk from Lecture 9 as the basis for our new figure. The one thing we need to do is filter() the variable based on the user input. The user selected input$metric takes the selection from the dropdown menu and reacts whenever the user picks a different option. So instead of using a filter on a named variable like DieselPM_P, we have a reactive variable which the user can modify through the dropdown menu.\nWe need to replace the original histogram output from lines 39-45 to our slightly modified ggplot code from Figure 11.8.\n\n      # dataset is SoCal_narrow1\n      SoCal_narrow1 |&gt; \n        # filter based on the input$metric\n        filter(variable == input$metric)  |&gt; \n        #create a ggplot by county, value\n        ggplot(aes(x = value, fill = County)) +\n        geom_histogram() +\n        theme_bw() +\n        facet_wrap(~County) +\n        labs(x = input$metric, \n             y = 'Count of census tracts') \n\nThis whole code chunk exists within the wrapper of output$distPlot &lt;- renderPlot({}). That part of the code doesn’t have to change at all. That part of the code chunk tells the app to make a plot and render it in the app. The name of the plot is distPlot. The ui portion of the app on line 29 describes where it goes - in the main panel.\nIf this code chunk works for you, pressing Run App will generate a shiny app that has a reactive dropdown menu allowing you to create 50+ histograms. Shiny apps are amazingly powerful tools to generate the same plot N different ways.\n\n21.3.4.4 Make it spatial with a leaflet interactive map.\nAdding a map to an app is the capstone.\nThere’s two parts to add.\n\nadd leafletOutput('map') to the ui in the mainPanel()\n\nadd output$map &lt;- renderLeaflet({CODE GOES HERE}) to the server.\n\nFirst, let’s do the ui part cause that is easy. On line 28 there is a code chunk that currently says mainPanel(plotOutput(\"distPlot)). Replace that with the code chunk below that puts in a comma and a leafletOutput('map').\n\n        mainPanel(\n           plotOutput(\"distPlot\"),\n           leafletOutput(\"map\")\n        )\n\nNext, we’ll paste in a big old code chunk to insert a leaflet map. It has four parts.\n\nThe wrapper\n\nA reactive table that only includes the selected metric.\nA reactive color palette\nA leaflet map\n\nI’m just going to show it all at once and we’ll diagram it within the app from innermost to outermost.\n\noutput$map &lt;- renderLeaflet({\n      \n      # filter geometry dataset for the user input\n     metricSoCal &lt;- SoCal_narrow |&gt; \n       filter(variable == input$metric) |&gt; \n\n      # Create a dynamic color palette based on the metric\n     palM &lt;- colorNumeric(palette = 'magma', \n        domain = metricSoCal$value)\n      \n        #This is the map - note that the legend title is dynamic\n      leaflet(metricSoCal) |&gt; \n        addTiles() |&gt; \n        setView(lat = 33.8, lng = -117.60, zoom = 9) |&gt; \n        addPolygons(color = ~palM(value),\n                    weight = 1) |&gt; \n        addLegend(pal = palM,\n                  title = input$metric,\n                  values = ~value)\n    })",
    "crumbs": [
      "Home",
      "Unit 4: Special Projects",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Shiny Apps</span>"
    ]
  },
  {
    "objectID": "maps4.html",
    "href": "maps4.html",
    "title": "22  Advanced Spatial Visualization",
    "section": "",
    "text": "22.1 Visual Categories and Encodings\nLet’s go back to the beginning of this course. There are 3 categories of information that can be displayed.\nLecture 1.2.1\nThe three types of data can be encoded in:\nAn advanced spatial visualization covering multiple layers of information needs to use multiple sets of encodings to convey information quickly and intuitively while not overwhelming the audience.",
    "crumbs": [
      "Home",
      "Unit 4: Special Projects",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Advanced Spatial Visualization</span>"
    ]
  },
  {
    "objectID": "maps4.html#visual-categories-and-encodings",
    "href": "maps4.html#visual-categories-and-encodings",
    "title": "22  Advanced Spatial Visualization",
    "section": "",
    "text": "Quantitative\nQualitative\nSpatial\n\n\n\n\nGeometric primitives - points, lines, and areas\nVisual channels - size, color, shape, position, angle, and texture",
    "crumbs": [
      "Home",
      "Unit 4: Special Projects",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Advanced Spatial Visualization</span>"
    ]
  },
  {
    "objectID": "maps4.html#circles-lines-and-polygons---oh-my",
    "href": "maps4.html#circles-lines-and-polygons---oh-my",
    "title": "22  Advanced Spatial Visualization",
    "section": "\n22.2 Circles, Lines, and Polygons - Oh My!",
    "text": "22.2 Circles, Lines, and Polygons - Oh My!\nFancy maps need distinct visual encodings, so the eye can be drawn the salient features.\nOne key way to do this is through ensuring different types/styles/aesthetics are displayed as unique fingerprints of visual encodings.\nAnother way to do this is through small multiples figures, as we showed in #fig-facetEJMap. However, that was a facet_wrap() done on a static map. It is also possible to do small multiples with leaflet maps.\nFirst, get all the libraries we need loaded up.\n\nlibrary(tidyverse)\nlibrary(leaflet)\nlibrary(sf)\n\nNow, we’ll need a new library called leafsync.\nFirst, install it.\n\ninstall.packages('leafsync')\n\nThen load the library.\n\nlibrary(leafsync)\n\n\n22.2.1 Example 1 - SoCalEJ multiples\nLet’s test this out with just Orange County data from SoCalEJ - our robust and useful testing dataset.\nFirst, we pull the data, if it isn’t already loaded.\n\nURL.path &lt;- 'https://raw.githubusercontent.com/RadicalResearchLLC/EDVcourse/main/CalEJ4/CalEJ.geoJSON'\nOCEJ &lt;- st_read(URL.path) |&gt;  \n  st_transform(\"+proj=longlat +ellps=WGS84 +datum=WGS84\") |&gt; \n  filter(County == 'Orange') |&gt; \n  select(White, Hispanic, AAPI, geometry)\n\nReading layer `CalEJ' from data source \n  `https://raw.githubusercontent.com/RadicalResearchLLC/EDVcourse/main/CalEJ4/CalEJ.geoJSON' \n  using driver `GeoJSON'\nSimple feature collection with 3747 features and 66 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 97418.38 ymin: -577885.1 xmax: 539719.6 ymax: -236300\nProjected CRS: NAD83 / California Albers\n\n\nLet’s create two figures and save them as individual maps before displaying them using leafsync. Let’s show White, Asian, and Hispanic populations for Orange County.\nFirst, we create individual maps. The template is shown below for the White population in #fig-White\n\npalW &lt;- colorNumeric(palette = 'GnBu', \n                     domain = OCEJ$White)\n\nleaflet() |&gt; \n  addTiles() |&gt; \n  addPolygons(data = OCEJ,\n              color = ~palW(White),\n              fillOpacity = 0.8,\n              weight = 1) |&gt; \n  addLegend(data = OCEJ,\n            pal = palW,\n            values = ~White,\n            title = '% White')\n\n\n\n\n\n\nFigure 22.1: Percentage of residents by census tract identified as White.\n\n\n\nOk, the map is straightforward. To make a synced multiples map, we have to do three steps.\n\nMake a map of each type we want to show.\nAssign the map to a named variable. In this case, we’ll call the maps white, hispanic, and aapi and assign the leaflet maps to those variables using the &lt;- operator.\nUse the sync() function from leafsync to show them.\n\nThe code for steps 1 and 2 are just slight variations on the white map we showed above, repeated three times.\nFigure 22.2 shows the end result.\n\npalW &lt;- colorNumeric(palette = 'RdPu', \n                     domain = OCEJ$White)\npalH &lt;- colorNumeric(palette = 'RdPu',\n                     domain = OCEJ$Hispanic)\npalA &lt;- colorNumeric(palette = 'RdPu',\n                     domain = OCEJ$AAPI)\nwhite &lt;- \n  leaflet() |&gt; \n  addTiles() |&gt; \n  addPolygons(data = OCEJ,\n              color = ~palW(White),\n              fillOpacity = 0.8,\n              weight = 1) |&gt; \n  addLegend(data = OCEJ,\n            pal = palW,\n            values = ~White,\n            title = '% White')\n\nhispanic &lt;- leaflet() |&gt; \n  addTiles() |&gt; \n  addPolygons(data = OCEJ,\n              color = ~palH(Hispanic),\n              fillOpacity = 0.8,\n              weight = 1) |&gt; \n  addLegend(data = OCEJ,\n            pal = palH,\n            values = ~Hispanic,\n            title = '% Hispanic')\n\naapi &lt;- \n  leaflet() |&gt; \n  addTiles() |&gt; \n  addPolygons(data = OCEJ,\n              color = ~palA(AAPI),\n              fillOpacity = 0.8,\n              weight = 1) |&gt; \n  addLegend(data = OCEJ,\n            pal = palA,\n            values = ~AAPI,\n            title = '% AAPI')\n\n## This shows all three together\nsync(white, hispanic, aapi)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 22.2: leaflet maps\n\n\n\nThe cool thing about sync is it zooms in and out as a combination, so if they are all on the same scale, we can really see some differences.",
    "crumbs": [
      "Home",
      "Unit 4: Special Projects",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Advanced Spatial Visualization</span>"
    ]
  },
  {
    "objectID": "roads_rails_tracts_buffers.html",
    "href": "roads_rails_tracts_buffers.html",
    "title": "23  Roads, Rails, Tracts, Buffers, and Spatial joins",
    "section": "",
    "text": "23.1 Libraries\nLoad the libaries first!\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(sf)\n\nLinking to GEOS 3.12.1, GDAL 3.8.4, PROJ 9.3.1; sf_use_s2() is TRUE\n\nlibrary(leaflet)\nlibrary(tigris)\n\nTo enable caching of data, set `options(tigris_use_cache = TRUE)`\nin your R script or .Rprofile.",
    "crumbs": [
      "Home",
      "Unit 4: Special Projects",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Roads, Rails, Tracts, Buffers, and Spatial joins</span>"
    ]
  },
  {
    "objectID": "roads_rails_tracts_buffers.html#tigris-and-its-rails-function.",
    "href": "roads_rails_tracts_buffers.html#tigris-and-its-rails-function.",
    "title": "23  Roads, Rails, Tracts, Buffers, and Spatial joins",
    "section": "\n23.2 tigris and its rails() function.",
    "text": "23.2 tigris and its rails() function.\nMultiple projects may need to have rail data.\nLet’s import some California rail data for visualization.\ntigris has the rails() function which is very similar to the roads() function from last week’s class.\nUnfortunately, it cannot be geographically pulled for specific areas, it only includes the whole country.\n\nrails &lt;- rails(progress_bar = FALSE, year = 2023) \n\nhead(rails)\n\nSimple feature collection with 6 features and 3 fields\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: -79.4782 ymin: 35.0574 xmax: -79.23766 ymax: 35.51799\nGeodetic CRS:  NAD83\n     LINEARID               FULLNAME MTFCC                       geometry\n1 11020239500  Norfolk Southern Rlwy R1011 LINESTRING (-79.47058 35.44...\n2 11020239501  Norfolk Southern Rlwy R1011 LINESTRING (-79.46687 35.44...\n3 11020239503  Norfolk Southern Rlwy R1011 LINESTRING (-79.46687 35.44...\n4 11020239575 Seaboard Coast Line RR R1011 LINESTRING (-79.43695 35.11...\n5 11020239576 Seaboard Coast Line RR R1011 LINESTRING (-79.47821 35.05...\n6 11020239577 Seaboard Coast Line RR R1011 LINESTRING (-79.43695 35.11...\n\n\nIt is in the coordinate reference system of NAD83 and includes the full dataset for the US. You can map this in leaflet, but it is a big dataset and we want to focus on our spatial domains for projects, just the Inland Empire counties of Riverside and San Bernardino.\nThankfully, we learned how to apply a spatial filter using st_filter() last week as well.\n\nIE_counties &lt;- counties(state = 'CA', cb = T, progress_bar = FALSE) |&gt;  #pull for California, lower detail level, year = 2022 ) \n  filter(NAME %in% c('San Bernardino', 'Riverside'))\n\nRetrieving data for the year 2022\n\nhead(IE_counties)\n\nSimple feature collection with 2 features and 12 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -117.8025 ymin: 33.42593 xmax: -114.1312 ymax: 35.80963\nGeodetic CRS:  NAD83\n  STATEFP COUNTYFP COUNTYNS       AFFGEOID GEOID           NAME\n1      06      065 00277297 0500000US06065 06065      Riverside\n2      06      071 00277300 0500000US06071 06071 San Bernardino\n               NAMELSAD STUSPS STATE_NAME LSAD       ALAND    AWATER\n1      Riverside County     CA California   06 18671846195 242807981\n2 San Bernardino County     CA California   06 51976311092  96418406\n                        geometry\n1 MULTIPOLYGON (((-117.6767 3...\n2 MULTIPOLYGON (((-117.8025 3...\n\n\nAnd now we have the two datasets we need to trim the rail data down to the local scale. Apply st_filter() to get an IE_rails dataset.\n\nIE_rail &lt;- st_filter(rails, IE_counties) \n\nhead(IE_rail) # down to only 826 records from 120k\n\nSimple feature collection with 6 features and 3 fields\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: -117.6677 ymin: 33.73872 xmax: -116.2508 ymax: 34.00415\nGeodetic CRS:  NAD83\n      LINEARID       FULLNAME MTFCC                       geometry\n1 110458253217       Railroad R1011 LINESTRING (-116.2744 33.74...\n2 110458253218       Railroad R1011 LINESTRING (-117.1677 34.00...\n3 110458254923 At and Sf Rlwy R1011 LINESTRING (-117.5418 33.87...\n4 110458254925 At and Sf Rlwy R1011 LINESTRING (-117.6677 33.87...\n5 110458254934 At and Sf Rlwy R1011 LINESTRING (-117.2324 33.76...\n6 110458254935 At and Sf Rlwy R1011 LINESTRING (-117.2334 33.76...\n\nrm(ls = rails) # this removes the big rails dataset to free up memory, you can always reimport it.\n\nFigure 23.1 shows the rail dataset for the Inland Empire. This is our basic map.\n\nIE_rail |&gt; \n  st_transform(crs =4326) |&gt; # coordinate reference transformation to WGS84 for visualization \n  leaflet() |&gt; \n  addTiles() |&gt; \n  setView(lat = 34, lng = -117.3, zoom = 10) |&gt; \n  addPolylines(color = 'black',\n               weight = 4,\n               dashArray = '2,5') |&gt; \n  addPolylines(data = IE_counties,\n               color = 'blue',\n               weight =3)\n\nWarning: sf layer has inconsistent datum (+proj=longlat +datum=NAD83 +no_defs).\nNeed '+proj=longlat +datum=WGS84'\n\n\n\n\n\n\n\nFigure 23.1: Inland Empire rail links from tigris\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nAdding fancy overlay options incoming!\n\n\nFigure 23.2 shows the rail layer and a rail tile layer, and also displays the addLayersControl() function for allowing layers to be toggled on-off by the user.\n\nIE_rail2 &lt;- IE_rail |&gt; \n    st_transform(crs = 4326)\n\nleaflet() |&gt; \n  addTiles() |&gt; #define group names\n  addProviderTiles(providers$OpenRailwayMap, group = 'RailTile') |&gt; #this is a standard provider tile\n  addLayersControl( #This provides a toggle option for layers \n    baseGroups = c('Basemap', 'RailTile'), #this is the names for tiles\n    overlayGroups = c('RailTigris', 'County') #this is names for datalayers - polylines, polygons, markers, etc.\n   )  |&gt;  \n  setView(lat = 34, lng = -117.3, zoom = 10) |&gt; \n  addPolylines(data = IE_rail2,\n               color = 'black',\n               weight = 4,\n               dashArray = '2,5',\n               group = 'RailTigris') |&gt; \n  addPolylines(data = IE_counties,\n               color = 'blue',\n               weight =3,\n               group = 'County')\n\nWarning: sf layer has inconsistent datum (+proj=longlat +datum=NAD83 +no_defs).\nNeed '+proj=longlat +datum=WGS84'\n\n\n\n\n\n\n\nFigure 23.2: Inland Empire rail links with overlays!",
    "crumbs": [
      "Home",
      "Unit 4: Special Projects",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Roads, Rails, Tracts, Buffers, and Spatial joins</span>"
    ]
  },
  {
    "objectID": "roads_rails_tracts_buffers.html#st_buffer",
    "href": "roads_rails_tracts_buffers.html#st_buffer",
    "title": "23  Roads, Rails, Tracts, Buffers, and Spatial joins",
    "section": "\n23.3 st_buffer()\n",
    "text": "23.3 st_buffer()\n\nThe next function I want to demonstrate is the st_buffer() function. This allows you to create a user-defined distance from a geospatial object to help identify nearby features.\nLet’s show an example using the IE_rail dataset.\n\nIERailBuffer &lt;- IE_rail |&gt; \n  st_buffer(dist = 500, nQuadSegs = 8) |&gt; # units are in the base units of the projection - could be meters or lat-lng degrees.  NAD83 is a meter based projection, WGS84 is lat-lng\n  st_simplify(dTolerance = 150) |&gt; #this smooths some wiggles and makes the data smaller bigger dTolerance would mean less wiggles\n  st_transform(crs = 4326)\n\nhead(IERailBuffer) \n\nSimple feature collection with 6 features and 3 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: -117.6736 ymin: 33.73431 xmax: -116.2456 ymax: 34.0087\nGeodetic CRS:  WGS 84\n      LINEARID       FULLNAME MTFCC                       geometry\n1 110458253217       Railroad R1011 POLYGON ((-116.2746 33.7546...\n2 110458253218       Railroad R1011 POLYGON ((-117.173 34.00299...\n3 110458254923 At and Sf Rlwy R1011 POLYGON ((-117.5437 33.8720...\n4 110458254925 At and Sf Rlwy R1011 POLYGON ((-117.6545 33.8744...\n5 110458254934 At and Sf Rlwy R1011 POLYGON ((-117.2283 33.7684...\n6 110458254935 At and Sf Rlwy R1011 POLYGON ((-117.2283 33.7684...\n\n\nNote that IE_rail was a LINESTRING whereas IERailBuffer is now a POLYGON type.\nFigure 23.3 shows a simple version of how it changed the data.\n\nleaflet() |&gt; \n  addTiles() |&gt; \n  setView(lat = 34, lng = -117.3, zoom = 10) |&gt; \n  addPolygons(data = IERailBuffer,\n               color = 'black',\n               weight = 2,\n               fillOpacity = 0.2,\n               label = ~FULLNAME)\n\n\n\n\n\n\nFigure 23.3: IE Rail with a 500m buffer\n\n\n\n\n23.3.1 Analysis Relevance\nWhat projects might looking at a buffered region make sense?\n\nEmissions - pollution from nearby sources - buffer around Fontana/Bloomington to show adjacent sources outside of the jurisdictional boundaries\nSocioeconomic status - buffers around tracts for adjacent warehouses (or vice versa)\nRoad widening - buffers around road widening for nearby warehouses",
    "crumbs": [
      "Home",
      "Unit 4: Special Projects",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Roads, Rails, Tracts, Buffers, and Spatial joins</span>"
    ]
  },
  {
    "objectID": "animate.html",
    "href": "animate.html",
    "title": "24  Animations - Two ways",
    "section": "",
    "text": "24.1 Animations\nVisualizations that include movement are a another way of creating salience. However, a bad animation doesn’t add anything to the visualization and just requires more time to view the same information than a good visualizationn.\nFirst, let’s show a couple of examples using gganimate which works to extend the grammar of ggplot2. Then we will also show a couple of examples using shiny and its sliderInput() animation.",
    "crumbs": [
      "Home",
      "Unit 4: Special Projects",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Animations - Two ways</span>"
    ]
  },
  {
    "objectID": "animate.html#packages-and-libraries",
    "href": "animate.html#packages-and-libraries",
    "title": "24  Animations - Two ways",
    "section": "\n24.2 Packages and libraries",
    "text": "24.2 Packages and libraries\nInstall gganimate and gifski. Apple computers may default to image-magick, but I can’t test that.\n\ninstall.packages('gganimate')\ninstall.packages('gifski') # Note that gifski may require other dependencies such as a Rust installation.  \n\nLoad libraries used for visualization today.\n\nlibrary(tidyverse) ## or dplyr and tidyr\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(sf)\n\nLinking to GEOS 3.12.1, GDAL 3.8.4, PROJ 9.3.1; sf_use_s2() is TRUE\n\nlibrary(gganimate)\nlibrary(gifski)\nlibrary(leaflet)\nlibrary(shiny)",
    "crumbs": [
      "Home",
      "Unit 4: Special Projects",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Animations - Two ways</span>"
    ]
  },
  {
    "objectID": "animate.html#gganimate",
    "href": "animate.html#gganimate",
    "title": "24  Animations - Two ways",
    "section": "\n24.3 gganimate\n",
    "text": "24.3 gganimate\n\n\n24.3.1 Contrived example - Keeling Curve at Mauna Loa\nIn Lecture 1 we plotted the growing concentration of CO2 at Mauna Loa, the famous Keeling Curve.\nLet’s revisit that.\nFirst, import the data from NOAA CMDL.\n\n#read raw data\nco2 &lt;- read_table('https://www.gml.noaa.gov/webdata/ccgg/trends/co2/co2_mm_mlo.txt',\n                  skip = 57 ) \n#fix column headers\nfieldNames &lt;- c('year', 'month', 'decDate', 'meanCO2', 'trendedCO2', 'days', 'stdev', 'unc')\ncolnames(co2) &lt;- fieldNames\n# check dataset back rows\ntail(co2)\n\n# A tibble: 6 × 8\n   year month decDate meanCO2 trendedCO2  days stdev   unc\n  &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1  2024     5   2024.    427.       424.    29  0.76  0.27\n2  2024     6   2024.    427.       424.    20  0.65  0.28\n3  2024     7   2025.    426.       425.    24  0.69  0.27\n4  2024     8   2025.    423.       425.    22  1.08  0.44\n5  2024     9   2025.    422.       425.    18  0.41  0.18\n6  2024    10   2025.    422.       426.    22  0.35  0.14\n\n\nNext, let’s create a relatively simple visualization of the Keeling Curve using ggplot2. Figure 24.1 shows the result.\n\n  ggplot(data = co2, aes(x = decDate, y = meanCO2)) +\n  geom_point(color = 'black', size = 1) +\n  theme_bw() +\n  labs(x = 'Year', y = 'Concentration of CO2 (ppm)', 'Keeling Curve @ Mauna Loa')\n\n\n\n\n\n\nFigure 24.1: Concentration (ppm) of CO2 measured at Mauna Loa\n\n\n\n\n\n24.3.2 Add in the animation steps\nIn many environmental data sets, we will want to show changes over time. gganimate has a built-in function for time animations called [transition_time()](https://gganimate.com/reference/transition_time.html).\nLet’s do the most super-basic animation and add that function to the Keeling Curve visualization. Figure 24.2 shows the most basic animation when adding a time increment.\n\nggplot(data = co2, aes(x = decDate, y = meanCO2)) +\n  geom_point(color = 'black', size = 1) +\n  theme_bw() +\n  labs(x = 'Year', y = 'Concentration of CO2 (ppm)', 'Keeling Curve @ Mauna Loa') +\n  transition_time(year) # This step adds the by year animation.\n\n\n\n\n\n\nFigure 24.2: Animated Concentration (ppm) of CO2 measured at Mauna Loa\n\n\n\n\nPretty cool, but we don’t see the old data so it looks just like a migrating flock of points. If we want to show other points along the graph, we can use shadow_mark() to show other points along the graph. Arguments for past and future allow us to choose include either or both of those points.\nshadow_mark(past = TRUE, future = FALSE, ..., exclude_layer = NULL)\nFigure 24.3 shows the result, while adding in a color argument to shadow_mark to show the old data differently.\n\nggplot(data = co2, aes(x = decDate, y = meanCO2)) +\n  geom_point(color = 'black', size = 1) +\n  theme_bw() +\n  labs(x = 'Year', y = 'Concentration of CO2 (ppm)', \n       title = 'Year: {frame_time}') +\n  transition_time(year) +\n  shadow_mark(past = TRUE, color = 'gray') # this leaves behind the animated data already shown.  Feel free to choose a different color.\n\n\n\n\n\n\nFigure 24.3: Animated Concentration (ppm) of CO2 measured at Mauna Loa with shadowed past\n\n\n\n\nPretty close, but that Year title is horrible and the ten significant figures past the decimal point is very aesthetically displeasing. I must fix that using the round() function.\nThe interesting thing about that curly bracket notation is it can deal with variables and code directly. So let’s modify that directly.\nFigure 24.4 shows the fixed title.\n\nggplot(data = co2, aes(x = decDate, y = meanCO2)) +\n  geom_point(color = 'black', size = 1) +\n  theme_bw() +\n  labs(x = 'Year', y = 'Concentration of CO2 (ppm)', \n       title = 'Year: {round(frame_time, 0)}') + # the rounded value of the year is inserted here, with zero to make it an integer.\n  transition_time(year) +\n  shadow_mark(past = TRUE, color = 'grey') \n\n\n\n\n\n\nFigure 24.4: Animated Concentration (ppm) of CO2 measured at Mauna Loa with shadowed past and rounded Year in title.\n\n\n\n\n\n24.3.3 Example 2: Animating a ggplot map\nImport warehouse data for Riverside County only - let’s limit the scope.\n\nWH.url &lt;- 'https://raw.githubusercontent.com/RadicalResearchLLC/WarehouseMap/main/WarehouseCITY/geoJSON/comboFinal.geojson'\nwarehouses &lt;- st_read(WH.url) |&gt;  \n  filter(county == 'Riverside County') |&gt;  \n  st_transform(crs = 4326)\n\nReading layer `comboFinal' from data source \n  `https://raw.githubusercontent.com/RadicalResearchLLC/WarehouseMap/main/WarehouseCITY/geoJSON/comboFinal.geojson' \n  using driver `GeoJSON'\nSimple feature collection with 9084 features and 8 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -118.8037 ymin: 33.43325 xmax: -114.4085 ymax: 35.55527\nGeodetic CRS:  WGS 84\n\nhead(warehouses)\n\nSimple feature collection with 6 features and 8 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -117.6015 ymin: 33.8773 xmax: -117.5314 ymax: 33.97265\nGeodetic CRS:  WGS 84\n        apn shape_area category year_built                 class\n1 115050036     343900 Existing       2000 warehouse/dry storage\n2 115060057     206900 Existing       1980 warehouse/dry storage\n3 115670012      71800 Existing       1999 warehouse/dry storage\n4 144010064      93200 Existing       2018 warehouse/dry storage\n5 144010070     206100 Existing       2018 warehouse/dry storage\n6 144010076     190000 Existing       1980 warehouse/dry storage\n            county unknown place_name                       geometry\n1 Riverside County   FALSE     Corona MULTIPOLYGON (((-117.5428 3...\n2 Riverside County    TRUE     Corona MULTIPOLYGON (((-117.5533 3...\n3 Riverside County   FALSE     Corona MULTIPOLYGON (((-117.5314 3...\n4 Riverside County   FALSE   Eastvale MULTIPOLYGON (((-117.5946 3...\n5 Riverside County   FALSE   Eastvale MULTIPOLYGON (((-117.6003 3...\n6 Riverside County    TRUE   Eastvale MULTIPOLYGON (((-117.5962 3...\n\n\nMake a basic warehouse map near my house using ggplot and geom_sf. Figure 24.5 shows a basic map of warehouses in ggplot.\n\nggplot(data = warehouses) +\n  geom_sf() +\n  coord_sf(xlim = c(-117.35, -117.1),\n           ylim = c(33.8,33.95), crs = 4326) +\n  theme_void() \n\n\n\n\n\n\nFigure 24.5: A simple map of warehouses near Mike’s house\n\n\n\n\nLet’s animate it. We’ll add a second step to control the animation speed and frames. First, we add transition_time() and shadow_mark() in a way identical to our CO2 figure.\nPass the ggplot code chunk into a variable. This variable is then run through an animate() function to control the frame rate and number of frames displayed.\nFigure 24.6 shows the time series animation.\n\ndata4map &lt;- ggplot(data = warehouses) +\n  geom_sf(fill = 'black') +\n  coord_sf(xlim = c(-117.35, -117.1),\n           ylim = c(33.8,33.95), crs = 4326) +\n  theme_void() + \n  transition_time(year_built) + # new line to animate by year_built\n  shadow_mark(past = TRUE, color = 'grey20', fill = 'grey') + # new line to leave built warehouses as grey blocks\n  labs(title = 'Year: {round(frame_time, 0)}') # new title for figure showing the rounded year year\n\nanimate(data4map, nframes = 46, fps = 3, end_pause = 10) # this is a wrapper to control the speed of the animation.  \n\n\n\n\n\n\nFigure 24.6: An animated map of warehouses growing near Mike’s house\n\n\n\n\nExcellent! We can also add an underlying map of jurisdictions or a tile layer to make it a bit prettier.\nLet’s use tigris places function to add some jurisdictions.\n\nlibrary(tigris)\n\n#read the data\nRiverside_places &lt;- places(state = 'CA', cb = T, #pull for California, lower detail level\n                    year = 2022, \n                    progress_bar = FALSE) |&gt; \n  filter(NAME %in% c('Moreno Valley', 'Mead Valley', 'March ARB', 'Perris', 'Riverside', 'Nuevo')) |&gt; \n  st_transform(crs = 4326)\n  \nhead(Riverside_places)\n\nSimple feature collection with 6 features and 12 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -117.5239 ymin: 33.71387 xmax: -117.0883 ymax: 34.01935\nGeodetic CRS:  WGS 84\n  STATEFP PLACEFP  PLACENS         AFFGEOID   GEOID          NAME\n1      06   62000 02410965 1600000US0662000 0662000     Riverside\n2      06   52624 02408960 1600000US0652624 0652624         Nuevo\n3      06   45680 02408176 1600000US0645680 0645680     March ARB\n4      06   46646 02583077 1600000US0646646 0646646   Mead Valley\n5      06   56700 02411403 1600000US0656700 0656700        Perris\n6      06   49270 02411159 1600000US0649270 0649270 Moreno Valley\n            NAMELSAD STUSPS STATE_NAME LSAD     ALAND AWATER\n1     Riverside city     CA California   25 210174364 797658\n2          Nuevo CDP     CA California   57  17531432      0\n3      March ARB CDP     CA California   57  30946983  36554\n4    Mead Valley CDP     CA California   57  49432252      0\n5        Perris city     CA California   25  81748619 279334\n6 Moreno Valley city     CA California   25 132891703 546511\n                        geometry\n1 MULTIPOLYGON (((-117.5238 3...\n2 MULTIPOLYGON (((-117.1695 3...\n3 MULTIPOLYGON (((-117.3194 3...\n4 MULTIPOLYGON (((-117.3313 3...\n5 MULTIPOLYGON (((-117.2627 3...\n6 MULTIPOLYGON (((-117.2965 3...\n\n\nAnd this chapter from the ggplot2 book is much better than my lecture on color - please review.\n\ndata4map2 &lt;- ggplot() +\n  geom_sf(data = Riverside_places, aes(fill = NAME), alpha = 0.2) +\n  geom_sf(data = warehouses, color = 'black', fill = '#653503',\n          inherit.aes = FALSE) +\n  coord_sf(xlim = c(-117.35, -117.1),\n           ylim = c(33.7,33.95), crs = 4326) +\n  theme_void() + \n  transition_time(year_built) +\n  shadow_mark(past = TRUE, color = '#653503', fill = 'grey') +\n  labs(title = 'Year: {round(frame_time, 0)}') +\n  scale_fill_brewer(palette = 'Accent')\n\nanimate(data4map2, nframes = 47, fps = 3, end_pause = 10)\n\n\n\n\n\n\nFigure 24.7: An animated map of warehouses growing near Mike’s house with a background layer\n\n\n\n\nIt could still be better adding roads and rail and other features and making the colors less ridiculous, but this is a good enough example for today.",
    "crumbs": [
      "Home",
      "Unit 4: Special Projects",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Animations - Two ways</span>"
    ]
  },
  {
    "objectID": "animate.html#animations-in-shiny-using-sliderinput",
    "href": "animate.html#animations-in-shiny-using-sliderinput",
    "title": "24  Animations - Two ways",
    "section": "\n24.4 Animations in Shiny using sliderInput()\n",
    "text": "24.4 Animations in Shiny using sliderInput()\n\n\n24.4.1 Example 3 - Animate Old Faithful Histogram\nCreate a new shiny App called ‘animate.R’ as shown in Section 19.3.2.\nThis should create a new Shiny App using the Old Faithful Geyser data. We’re going to animate this using the following argument within the sliderInput() function.\nanimate = TRUE.\nThe code chunk within the app should look like this, starting at line 21 on my machine:\n\n            sliderInput(\"bins\",\n                        \"Number of bins:\",\n                        min = 1,\n                        max = 50,\n                        value = 30,\n                        # NEW ARGUMENT HERE!\n                        animate = TRUE)\n\nIf we run the app by pressing the Run App button, a shiny App should pop-up. I’ll show that within the Shiny App. A blue play button will appear on the bottom-right of the slider. Pressing the play button advances through the slider increments and updates the histogram.\nEasy!\nThere are additional options for controlling the interval rate and whether it loops.\nWe can modify the code to show that.\n\n            sliderInput(\"bins\",\n                        \"Number of bins:\",\n                        min = 1,\n                        max = 50,\n                        value = 30,\n                        # NEW ARGUMENT HERE!\n                        animate = animationOptions(\n                          loop = TRUE, interval = 300)\n                        )",
    "crumbs": [
      "Home",
      "Unit 4: Special Projects",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Animations - Two ways</span>"
    ]
  },
  {
    "objectID": "dataTheory.html",
    "href": "dataTheory.html",
    "title": "25  Data Import - Theory and Practice",
    "section": "",
    "text": "25.1 Overview\nSome sources to peruse for more information on data import using R.\n- R for Data Science 2e - Data Import\n- An Introduction to R - Data\n- Reading, Writing and Converting Simple Features\n- Introduction to Data Science - Importing Data",
    "crumbs": [
      "Home",
      "Unit 4: Special Projects",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Data Import - Theory and Practice</span>"
    ]
  },
  {
    "objectID": "dataTheory.html#overview",
    "href": "dataTheory.html#overview",
    "title": "25  Data Import - Theory and Practice",
    "section": "",
    "text": "Clearly define the research question\nIdentify relevant authoritative data sources\nCritically evaluate the data sources\nImport the data\nReview and tidy the data\n\n\n\n25.1.1 Step 0. Clearly define the question or objective\nThe zeroeth step is to identify what it is you are planning to do. Delineat the scope of the project for the purposes of defining the data needs.\n\nSpatial extent\nTemporal extent\nVariables/parameters (environmental variables, socioeconomic information, categorical information, jurisdictions)\n\n\n25.1.1.1 Example - Particulate emissions sources affecting Fontana and Bloomington\n\nSpatial extent - The city of Fontana and the census designated place of Bloomington boundaries with a 1 km buffer\nTemporal extent - Emissions sources operational as of 2020 or later.\nVariables/parameters - Major roads, local roads, rail, railyards, industrial facilities in emissions inventory, small area sources like gas stations, restaurants, truck terminals, and warehouses\nDQOs - authoritative sources (census, government agency datasets, OpenStreetMaps, Peer Reviewed Data Products)\n\n\n\n25.1.1.2 Example - Road widening projects in Riverside County and induced Freight VMT/demand\n\nSpatial extent - Western Riverside County - west of Banning\nTemporal extent - Projects constructed between 2014-2024 and planned through 2030\nVariables/parameters - Road links, vehicle and truck AADT, warehouse land-use, residential developments\n\n\n\n\n25.1.2 Step 1. Identify relevant authoritative data sources\nWhat is the best dataset that meets the needs of the research question? What authoritative sources provide information?\nLook for government agencies, peer-reviewed data, and other similar authoritative and quality assured sources.\n\n25.1.2.1 Example - PM emissions\n\nRoad and rail datasets from US Census Tiger files (tigris) or Caltrans\nLarge Facility information from US EPA, CARB or SCAQMD\nSmall localized sources like restaurants and small industrial facilities (Warehouse CITY or CoStar)\n\n\n\n25.1.2.2 Example - Road Widening\n\nRCTC project information\nRoad link information from US Census Tiger files (tigris) or Caltrans\nTraffic and truck volume AADT (annual average daily traffic) data from Caltrans\nWarehouse data from Warehouse CITY or CoStar ($$)\nResidential information from County of Riverside (unincorporated) and municipalities\n\n\n\n\n25.1.3 Step 2. Critically evaluate the data sources\nPrior to acquiring the data, it is important to evaluate its utility for your goals, to the extent possible.\n\nWhat type of data is it? (geospatial, tabular, categorical, qualitative)\nDoes it cover the spatial domain?\nIs it the right temporal period?\nDoes it have the right variables?\nWhat format is it in?\n\nDoes it require any specific citations, licenses, and/or attribution for use?\n\n\n\n\n\n\n\nSometimes these things can’t be evaluated until you import the data. Iterate through step 2-4 as needed.\n\n\n\n\n25.1.3.1 Example - PM emissions\nThe tigris package is a wealth of primary authoritative data. - roads() data is authoritative, recent, spatially comprehensive, geospatial data. ✅\n- rails() data - same as roads, but does require some spatial tidying. ✅ - places() data - usable to zoom in on spatial scope - requires some spatial tidying/buffering. ✅\nUS EPA data for National emissions inventory - this has location information for large sources. ✅\nSCAQMD data for source magnitudes from PM AQMP, see table and links to primary sources. ✅\nRestaurant data - I haven’t found a good source for tabular and spatial data on restaurants. Google API might work but requires registration.\n\n\n25.1.3.2 Example - Roads\nRCTC website has project information, but it isn’t directly downloadable/scrapable. Tidying required. ✅\nThe tigris package is a wealth of primary authoritative data. - roads() data is authoritative, recent, spatially comprehensive, geospatial data. ✅\nCaltrans open data has point data for AADT and truck AADT. Will require spatial joins to road links. Single year of data. Spreadsheets contain multiple years that can be linked to this.\nRiverside County open data has zoning polygons for identifying residential areas, but not necessarily new areas. Other municipalities may be needed to cover specific road segments.\n\n\n\n25.1.4 Step 3. Import Data\n3.1 Identify the data format 3.2 Can I scrape it directly? Is there a url or API or a package in R that will allow me to scrape it? If so, that’s probably the best choice for a reproducible workflow. Examples include tidycensus, tigris, warehouses, co2, and SoCalEJ datasets. 3.3 If not, download it.\n3.3a - Unzip it if zipped or compressed. 3.3b - move data to working directory or point import script to location it is stored. Keeping data in a specific working directory is a better practice.\n3.4 - Import it using appropriate function (st_read(), read_table(), read_csv(), etc.) 3.5 - Review data import 3.5a - look at data table 3.5b - make a basic visualization\n\n25.1.4.1 Example - PM emissions\nScrape roads(), rails(), and places() and warehouses.\nImport facility data from F.I.N.D tool at SCAQMD using openxlsx::read.xlsx().\nGoogle maps api call for restaurants or manually assemble restaurant list - both could be slow and painful.\n\n\n25.1.4.2 Example - Roads\nScrape roads() and warehouses.\nDownload passenger and truck AADT data from Caltrans.\nResidential zoning is available for the county and potentially some municipalities for download.\nAssemble road widening project data from county website into data table(s).\n\n\n\n25.1.5 Step 4. Review and tidy data.\nUse head(), tail(), to examine imported rows.\n\nCheck that the number of records seems reasonable - a zero record import is usually bad\nCheck for column names, and check that they are R friendly (avoid spaces, special characters)\nCheck column types - common errors include dates imported as character types or other incorrect assignments.\nCheck for NA and null records, rows, and columns.\nFor geospatial data - check coordinate reference system\n\nAs necessary, tidy the data to get it in the form you need. See all the data science steps in Data Science 101",
    "crumbs": [
      "Home",
      "Unit 4: Special Projects",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Data Import - Theory and Practice</span>"
    ]
  },
  {
    "objectID": "assignment4.html",
    "href": "assignment4.html",
    "title": "26  Assignments for Unit 4 - Projects",
    "section": "",
    "text": "26.1 Individual Visualization\nDue by email/canvas at 9:45 AM on November 12, 2024.\nAssignment is a single visualization of data on the team’s chosen topic. Assignment should provide an exported visualization .png, .pdf, .html and the .R script used to generate the visualization. Source of data should be clearly identified as a comment within the .R script.\nDescribe a minimum of three features that were selected to explore the dataset, either in the .R script or in the email. Bullet points are fine.",
    "crumbs": [
      "Home",
      "Unit 4: Special Projects",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Assignments for Unit 4 - Projects</span>"
    ]
  },
  {
    "objectID": "assignment4.html#grading-for-individual-visualization-project",
    "href": "assignment4.html#grading-for-individual-visualization-project",
    "title": "26  Assignments for Unit 4 - Projects",
    "section": "26.2 Grading for Individual Visualization Project",
    "text": "26.2 Grading for Individual Visualization Project\nTotal of 200 points\n\n20 points for data source attribution\n30 points for description of three features\n60 points for three features implementation in visualization\n45 points for aesthetics\n45 points for labels, legend, and in-visualization annotation",
    "crumbs": [
      "Home",
      "Unit 4: Special Projects",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Assignments for Unit 4 - Projects</span>"
    ]
  },
  {
    "objectID": "assignment4.html#group-visualization",
    "href": "assignment4.html#group-visualization",
    "title": "26  Assignments for Unit 4 - Projects",
    "section": "26.3 Group Visualization",
    "text": "26.3 Group Visualization\nDue Date\nA group presentation on the progress to date will occur on December 3rd, 2024. Final group visualization will be due December 10th, 2024 at noon (PST).\nAssignment is a visualization on the team’s chosen topic. Assignment should provide an exported visualization .png, .pdf, .html and the .R script used to generate the visualization(s). Source(s) of data should be clearly identified as a comment within the .R script.\nDescribe the features that were selected to explore the dataset, either in the .R script or in the email. Bullet points are fine.\nDocument in a single paragraph how the visualization shows the group theme. I want a brief story of how the visualization illustrates that.",
    "crumbs": [
      "Home",
      "Unit 4: Special Projects",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Assignments for Unit 4 - Projects</span>"
    ]
  },
  {
    "objectID": "assignment4.html#grading-for-group-visualization-project",
    "href": "assignment4.html#grading-for-group-visualization-project",
    "title": "26  Assignments for Unit 4 - Projects",
    "section": "26.4 Grading for Group Visualization Project",
    "text": "26.4 Grading for Group Visualization Project\nTotal of 250 points\n\n30 points for data source attribution\n30 points for description of features\n70 points for features implementation in visualization(s)\n30 points for aesthetics\n30 points for labels, legend, and in-visualization annotation\n30 points for the visualization providing insight into the group theme\n30 points for group participation - (team grading)",
    "crumbs": [
      "Home",
      "Unit 4: Special Projects",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Assignments for Unit 4 - Projects</span>"
    ]
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Appendix A — Course Syllabus",
    "section": "",
    "text": "A.1 Course Description\nThis is an introductory course on the theory and practice of effective communication with quantitative data. This course will introduce the theory of data visualization, discuss the ethics of data visualization, provide hands-on training in acquiring, tidying, and visualizing quantitative environmental data, and critically examine current environmental justice tools (CalEnviroScreen, EPA’s EJScreen, CEJST).",
    "crumbs": [
      "Home",
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Course Syllabus</span>"
    ]
  },
  {
    "objectID": "syllabus.html#course-materials",
    "href": "syllabus.html#course-materials",
    "title": "Appendix A — Course Syllabus",
    "section": "\nA.2 Course Materials",
    "text": "A.2 Course Materials\nGo here to see all course materials.\nThe course materials are built using the quarto environment which is R Markdown adjacent, but allows a user to embed functional code in R, Python, Julia, and/or Observable JS.\nAll code used is on github\nAny readings assigned will be emailed to students and/or posted to Box for download. Box will be linked within the course materials for assignments.\n\n\n\n\n\n\nCourse layout and materials listed on course materials page are subject to change due to shifting conditions that occur during the course of the semester. I will update course materials on a weekly basis to reflect the changing conditions and expectations throughout the semester.",
    "crumbs": [
      "Home",
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Course Syllabus</span>"
    ]
  },
  {
    "objectID": "syllabus.html#course-goals-learning-objectives",
    "href": "syllabus.html#course-goals-learning-objectives",
    "title": "Appendix A — Course Syllabus",
    "section": "\nA.3 Course Goals & Learning Objectives",
    "text": "A.3 Course Goals & Learning Objectives\nUpon successful completion of this course, students will\n\nacquire critical thinking skills on the display of visual information\nimprove team-based collaboration abilities by working on group projects\nknow how to acquire, tidy, and summarize quantitative environmental datasets\ncreate graphs and maps in R\n\ncritically review current environmental justice tools such as CalEnviroScreen and EJScreen\n\nunderstand basic theory and ethics of environmental data visualization",
    "crumbs": [
      "Home",
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Course Syllabus</span>"
    ]
  },
  {
    "objectID": "syllabus.html#student-learning-outcomes",
    "href": "syllabus.html#student-learning-outcomes",
    "title": "Appendix A — Course Syllabus",
    "section": "\nA.4 Student Learning Outcomes",
    "text": "A.4 Student Learning Outcomes\n\nCritical Thinking, Quantitative Reasoning, and Effective Expression - Upon completion of this course, student’s will be able to critically examine the elements of environmental data visualization, create effective maps and graphs of environmental data, and effectively communicate environmental ideas in a quantitative visual manner. It will also focus on the theory and ethics of data visualization with readings and critical analysis of existing environmental justice tools.\nSocial Justice Theory - This course will identify and quantitatively explore the unequal distribution and access to natural resources (Environmental Justice) within California and the United States. Students will critically examine the ways Environmental Justice is currently characterized by government agencies and describe the types of data visualization used to quantitatively identify areas of disproportional impact and understand the limitations of existing tools in promoting effective change.",
    "crumbs": [
      "Home",
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Course Syllabus</span>"
    ]
  },
  {
    "objectID": "syllabus.html#classroom-approach",
    "href": "syllabus.html#classroom-approach",
    "title": "Appendix A — Course Syllabus",
    "section": "\nA.5 Classroom Approach",
    "text": "A.5 Classroom Approach\nIn-person class time will include approximately 30% of time to devoted to lectures, 40% to hands-on coding and data visualization, and 30% to discussion.",
    "crumbs": [
      "Home",
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Course Syllabus</span>"
    ]
  },
  {
    "objectID": "syllabus.html#assignments-and-activities",
    "href": "syllabus.html#assignments-and-activities",
    "title": "Appendix A — Course Syllabus",
    "section": "\nA.6 Assignments and Activities",
    "text": "A.6 Assignments and Activities\n\nClassroom participation during discussion sections\nClassroom participation during coding sessions\nAssigned reading (news and peer-review articles, online books)\nAssigned review of web tools (CalEnviroScreen, EJScreen, CEJST)\nGroup coding projects\nClass presentation on an environmental data visualization\nIndividual projects generating, displaying, or critiquing environmental data visualizations\nGroup project creating an environmental data visualization",
    "crumbs": [
      "Home",
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Course Syllabus</span>"
    ]
  },
  {
    "objectID": "syllabus.html#grading-and-assessment-breakdown",
    "href": "syllabus.html#grading-and-assessment-breakdown",
    "title": "Appendix A — Course Syllabus",
    "section": "\nA.7 Grading and Assessment Breakdown",
    "text": "A.7 Grading and Assessment Breakdown\nThe class will be based on a total of 1000 points.\n\nThe three individual projects displaying or reviewing environmental data will count for 150 or 200 points (500 in total).\nA written and presnted critique of an existing environmental data visualization\nRemix of an environmental justice dataset from an existing EJ tool\nA student proposed and executed environmental data visualization (map, figure, infographic, interactive tool)\nThe group project displaying environmental data visualization will count for 250 points\nA group presentation on environmental data visualization in Environmental Justice tools will count for 150 points\nClassroom participation will count for 150 points throughout the course of the semester. These points will be awarded for attendance and active participation in classroom discussion and coding sessions. Online attendance and participation will count towards this activity in case of sickness or inability to attend in-person.\nMissing classroom discussion and coding sessions can be made up for 80% credit within two weeks or 50% credit within four weeks by attending online or in-person office hours.\nExtra credit - Identifying errors in course lectures, asking questions of guest speakers, and bringing snacks to enhance class morale can earn extra credit in 10 point chunks.\n\nStudents get a single free individual or group project that can be late by up to seven days from the due date. For the group project, if a member of the group has already used their individual free late project, the group cannot use other individual free periods. If any other project is late, scores will be reduced by 10% per day beyond the due date of the project.\nThe assigned presentation date requires a doctor’s note for an excused absence. If unexcused, the student can make up the presentation or final exam for 75% credit within one week of the assigned date.\nThis course will be attempting to engage in ungrading to give scholars more control over their own review of their progress and success in the class. The goal is for internal motivation and ownership of the definition of classroom success. In practice, ungrading is essentially an agreement to allow students to redo and resubmit assignments up until the last day of class. This requires students to communicate in writing that the intent is to resubmit an assignment within 10 days of receiving the initial grade. Late assignment penalties will still apply.\nTable A.1\n\n\n\nTable A.1: Breakdown of course points\n\n\n\n\n\n\n\n\nAssignments\nPoints\n\n\n\nGroup Project - data visualization\n250\n\n\nIndividual project 1 - written critique of existing visualization\n150\n\n\nIndividual Project 2 - - EJ tool remix\n150\n\n\nIndividual Project 3 - data visualization\n200\n\n\nGroup Presentation on EJ tools\n150\n\n\nClassroom participation\n100\n\n\nExtra Credit\n50",
    "crumbs": [
      "Home",
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Course Syllabus</span>"
    ]
  },
  {
    "objectID": "syllabus.html#office-hours",
    "href": "syllabus.html#office-hours",
    "title": "Appendix A — Course Syllabus",
    "section": "\nA.8 Office Hours",
    "text": "A.8 Office Hours\n\nMonday 2:00 - 3:00 zoom\n\nTuesdays 9:00 - 9:30 (before class) - in classroom?\nThursday at 9:00 - 9:30 (before class) - in classroom?\nFriday 9:00 - 10:00 zoom",
    "crumbs": [
      "Home",
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Course Syllabus</span>"
    ]
  },
  {
    "objectID": "syllabus.html#contact-info",
    "href": "syllabus.html#contact-info",
    "title": "Appendix A — Course Syllabus",
    "section": "\nA.9 Contact Info",
    "text": "A.9 Contact Info\nThe best way to get a hold of me is email: michael_mccarthy@pitzer.edu You are also likely to see emails from my non-pitzer email: mikem@radicalresearch.llc",
    "crumbs": [
      "Home",
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Course Syllabus</span>"
    ]
  },
  {
    "objectID": "syllabus.html#course-materials-1",
    "href": "syllabus.html#course-materials-1",
    "title": "Appendix A — Course Syllabus",
    "section": "\nA.10 Course materials",
    "text": "A.10 Course materials\nAll course materials will be hosted on github.\nGo here to see all course materials.",
    "crumbs": [
      "Home",
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Course Syllabus</span>"
    ]
  },
  {
    "objectID": "syllabus.html#covid-policy",
    "href": "syllabus.html#covid-policy",
    "title": "Appendix A — Course Syllabus",
    "section": "\nA.11 COVID policy",
    "text": "A.11 COVID policy\nCOVID-19 safety guidelines and recommendations continue to evolve. Please read Pitzer’s COVID Policies and visit the Student Health Services (SHS) COVID page for the latest campus information and guidance.\n\n\nSneezeCFD\n\nAll applicable campus and LA County policies will apply in this class and I will follow them to the extent feasible. Mask wearing is optional under Pitzer and LA County guidelines, and therefore it is in this classroom as well. If you need to sneeze, cover up with an elbow…\nIf we do have a COVID breakout or otherwise require hybrid/remote class environments through Zoom, I will post meeting links via email.",
    "crumbs": [
      "Home",
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Course Syllabus</span>"
    ]
  },
  {
    "objectID": "syllabus.html#readings",
    "href": "syllabus.html#readings",
    "title": "Appendix A — Course Syllabus",
    "section": "\nA.12 Readings",
    "text": "A.12 Readings\nThe theory of environmental data visualization will use selected readings assigned to students. Every other class will involve a discussion of an assigned reading or analysis of an environmental data visualization.\nAssigned readings will include selections from books, peer-review journal articles, websites, and newspaper articles. A selection of readings that are likely to be discussed in the course includes:\n\nSelections from Tufte’s book ‘The Visual Display of Quantitative Information’ (Tufte 2001)\n\nSelections from Tufte’s book ‘Envisioning Information’ (Tufte 2013)\n\nSelections from Hadley Wickham’s “Grammar of Graphics” with ggplot2, shiny, and other R packages. (Wickham et al. 2019)\n\nA selection of Peer-review journal articles listed in the references below. (Horton, Nowak, and Haegeli 2020; Kelleher and Wagener 2011; van Beek et al. 2020; Gommeh, Dijstelbloem, and Metze 2021; Murchie and Diomede 2020; Grainger, Mao, and Buytaert 2016)\n\nNewspaper articles such as the LA Times article by Professor Phillips on Inland Empire Warehouse growth (Facebook et al. 2022)\n\nReadings on the meaning, use, and technical limitations of Environmental Justice tools\n\n\n\n\n\n\n\n\n\n\n\n\n\nFacebook, Twitter, Show more sharing options, Facebook, Twitter, LinkedIn, Email, Copy Link URLCopied!, and Print. 2022. “Op-Ed: We Mapped the Warehouse Takeover of the Inland Empire. The Results Are Overwhelming.” Los Angeles Times. https://www.latimes.com/opinion/story/2022-05-01/inland-empire-warehouse-growth-map-environment.\n\n\nGommeh, Efrat, Huub Dijstelbloem, and Tamara Metze. 2021. “Visual Discourse Coalitions: Visualization and Discourse Formation in Controversies over Shale Gas Development.” Journal of Environmental Policy & Planning 23 (3): 363–80. https://doi.org/10.1080/1523908X.2020.1823208.\n\n\nGrainger, Sam, Feng Mao, and Wouter Buytaert. 2016. “Environmental Data Visualisation for Non-Scientific Contexts: Literature Review and Design Framework.” Environmental Modelling & Software 85 (November): 299–318. https://doi.org/10.1016/j.envsoft.2016.09.004.\n\n\nHorton, Simon, Stan Nowak, and Pascal Haegeli. 2020. “Enhancing the Operational Value of Snowpack Models with Visualization Design Principles.” Natural Hazards and Earth System Sciences 20 (6): 1557–72. https://doi.org/10.5194/nhess-20-1557-2020.\n\n\nKelleher, Christa, and Thorsten Wagener. 2011. “Ten Guidelines for Effective Data Visualization in Scientific Publications.” Environmental Modelling & Software 26 (6): 822–27. https://doi.org/10.1016/j.envsoft.2010.12.006.\n\n\nMurchie, Karen J., and Dylan Diomede. 2020. “Fundamentals of Graphic Designessential Tools for Effective Visual Science Communication.” FACETS 5 (1): 409–22. https://doi.org/10.1139/facets-2018-0049.\n\n\nTufte, Edward R. 2001. The Visual Display of Quantitative Information, 2nd Ed. 2nd edition. Cheshire, Conn: Graphics Press.\n\n\n———, ed. 2013. Envisioning Information. 14. print. Cheshire, Conn: Graphics Press.\n\n\nvan Beek, Lisette, Tamara Metze, Eva Kunseler, Hiddo Huitzing, Filip de Blois, and Arjan Wardekker. 2020. “Environmental Visualizations: Framing and Reframing Between Science, Policy and Society.” Environmental Science & Policy 114 (December): 497–505. https://doi.org/10.1016/j.envsci.2020.09.011.\n\n\nWickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy D’Agostino McGowan, Romain FranÃois, Garrett Grolemund, et al. 2019. “Welcome to the tidyverse.” Journal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686.",
    "crumbs": [
      "Home",
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Course Syllabus</span>"
    ]
  },
  {
    "objectID": "tools.html",
    "href": "tools.html",
    "title": "Appendix B — R and RStudio Installation and Overview",
    "section": "",
    "text": "B.1 Download and Install R\nR is maintained and made available through the web-page at The Comprehensive R Archive Network aka CRAN.\nThe top of the CRAN web page provides three links for downloading R. Follow the link that describes the OS of your computer: Mac, Windows, or Linux.",
    "crumbs": [
      "Home",
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>R and RStudio Installation and Overview</span>"
    ]
  },
  {
    "objectID": "tools.html#download-and-install-r",
    "href": "tools.html#download-and-install-r",
    "title": "Appendix B — R and RStudio Installation and Overview",
    "section": "",
    "text": "B.1.1 R for macOS\nTo install R on a Mac, click the “Download R for macOS” link. Next, click on the R-4.4.1 package link (or the package link for the most current release of R). An installer will download to guide you through the installation process. As a starting point, I recommend that most users should choose the default installation settings until they are familiar with R.\n\n\nB.1.2 R for Windows\nTo install R on Windows, click the “Download R for Windows” link. Next, click on the install R for the first time link which contains base R. Then click on the link that says Download R-4.4.1 for Windows. A win.exe file will be downloaded to your downloads folder. Open that file after it downloads and follow the installation process. As a starting point, I recommend that most users should choose the default installation settings until they are familiar with R.\n\n\nB.1.3 R for Linux\nR comes preinstalled on many Linux systems, but update to the newest version of R if yours is out of date. The CRAN website provides files for different Linux builds including Debian, Fedora, Redhat, Suse, and Ubuntu. Choose the appropriate directory and the installation procedure for your flavor of Linux.",
    "crumbs": [
      "Home",
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>R and RStudio Installation and Overview</span>"
    ]
  },
  {
    "objectID": "tools.html#about-r",
    "href": "tools.html#about-r",
    "title": "Appendix B — R and RStudio Installation and Overview",
    "section": "B.2 About R",
    "text": "B.2 About R\nR is not a friendly computer program like a web browser or a Microsoft Office product. R is a programming language, like C++, Python, or Java. R can be run like an old-school UNIX terminal, writing commands directly onto monitors with green fonts; imagine Neo in the Matrix or Angelina Jolie in Hackers. While one can still do that, most folks use the RStudio IDE.\n\n\n\nHackers use green fonts and dark mode.",
    "crumbs": [
      "Home",
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>R and RStudio Installation and Overview</span>"
    ]
  },
  {
    "objectID": "tools.html#rstudio-ide",
    "href": "tools.html#rstudio-ide",
    "title": "Appendix B — R and RStudio Installation and Overview",
    "section": "B.3 RStudio IDE",
    "text": "B.3 RStudio IDE\nRStudio is an application - like MS Word or a browser. However, Rstudio is an app that helps you write in “R”, not English. It is literally a foreign language (unless you know it already). And just like any language, R has its own syntax, grammar, and vocabulary. It takes training and time to learn R, but I believe it is worth it.\nWe will use RStudio in this course because it makes using R much easier. Also, the RStudio interface looks about the same for different operating systems, which will help me because I am a Windows user and I’ve observed that Claremont Colleges’ scholars tend to lean towards macOS.\nDownload RStudio desktop here - I recommend the free open source version. Make sure to pick the version that is appropriate for your OS. Then follow the installation instructions.\nOnce installed, RStudio can be opened like any other program or app on your computer; usually just click on the icon on the desktop. When you open it, it should look something like Figure D.1.\n\n\n\n\n\n\nFigure B.1: RStudio\n\n\n\nHere, there are four separate windows or “Panels”.\n\nThe source panel is top-left and this is basically a text editor where you type R code or regular text. Code here gets colored and looks different if it is doing various R things. More on this later. Code doesn’t get immediately executed here; it is more like a holding place for writing/testing/debugging bigger “scripts” or programs.\nThe console/terminal is bottom-left. The terminal is the 1980s window that does commands directly. Type print(‘Hello World’), press enter/return, and you’ve written some code. This console is what R would look like if you ran it without RStudio.\nThe top-right is the file manager panel. It shows files in your directory, plots, packages, and help files.\nThe bottom-right is the programming environment. It contains things you’ve loaded or coded. Right now it should be empty because you’ve done neither.\n\nOf course all of these can be moved around to match how you like to set it up. And I need a different color-scheme than the ugly default.\nSelect the Tools menu, followed by Global Options in the dropdown Tools menu. Navigate to appearance and you should be able to alter the color scheme to something more your style. My preferred style is Merbivore Soft as shown in Figure B.2.\n\n\n\n\n\n\nFigure B.2: goodRstudio\n\n\n\n\n\n\n\n\n\nDo I still need to download R?\n\n\n\nEven if you use RStudio, you’ll still need to download R to your computer (or use R and RStudio cloud versions). RStudio helps you use the version of R that lives on your computer, but it doesn’t come with a version of R on its own.\nI recommend downloading R and RStudio for compatibility, but the RStudio Cloud does exist and does mostly the same thing. I will be less helpful at supporting the Cloud version for this class.\nIn this class, I will be testing out web assembly live versions that allow executable code within the browser. This will remove version control and package issues, since it will live within the browser when posted. Check each coding lesson for Noodle Zone areas to work within the browser if your code is not working on your local machine.",
    "crumbs": [
      "Home",
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>R and RStudio Installation and Overview</span>"
    ]
  },
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "Appendix C — Resources",
    "section": "",
    "text": "Base R is the collection of functions that come preloaded with R. A function is a code block that performs a task. Functions that come in R are in-built functions. However, the real power of R (and programming languages more generally) are the large number of user-built functions that are built to perform tasks that can be loaded into base R.\nSets of functions that have a common purpose are bundled together in R in “packages”. There are literally thousands of packages custom-built for R.\nDuring this course, I will ask you to use some important packages for data acquisition, tidying, and visualization. There are two steps to using a package in R.\nFirst, it needs to be installed. Here’s a non-functional example.\n\ninstall.packages('&lt;PACKAGE.NAME&gt;')\n\nAnd this example will install an actual package called janitor. This package features a nice import function that fixes column names on import called clean_names().\n\ninstall.packages('janitor')\n\nIn a meta way, you now have used a function to install a package. ‘install.package()’ is an example of an in-built R function.\nOnce a package is installed, it needs to be loaded. While the installation has put the package on your computer, you need to tell R that it should load these functions for use.\n\n\n\n\n\n\nR and other programming languages only do EXACTLY what they are told to do.\n\n\n\nLoading a package is accomplished by calling base R library function. Success is indicated with a message like “Attaching package: ‘janitor’\n\nlibrary(janitor)\n\n\nAttaching package: 'janitor'\n\n\nThe following objects are masked from 'package:stats':\n\n    chisq.test, fisher.test\n\n\nLibraries have to be loaded each time you restart your R session. The current libraries loaded can be founded in the file manager panel under the Packages tab.\n\nC.0.1 Color Resources\nCourtesy of Dalai Vo - Fall 2022 EA 078 student - a color palette cheatsheet",
    "crumbs": [
      "Home",
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Resources</span>"
    ]
  },
  {
    "objectID": "debugging.html",
    "href": "debugging.html",
    "title": "Appendix D — Debugging Steps",
    "section": "",
    "text": "D.1 Is there a typo?\nRunning any line of code or code block will fail if there is a typo.\nCheck for:",
    "crumbs": [
      "Home",
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Debugging Steps</span>"
    ]
  },
  {
    "objectID": "debugging.html#is-there-a-typo",
    "href": "debugging.html#is-there-a-typo",
    "title": "Appendix D — Debugging Steps",
    "section": "",
    "text": "typos or misspelling of variables, functions,\ncapitalization\nmissing punctuation (parenthesis, comma, pipe, +)",
    "crumbs": [
      "Home",
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Debugging Steps</span>"
    ]
  },
  {
    "objectID": "debugging.html#is-the-packagelibrary-installed-and-loaded",
    "href": "debugging.html#is-the-packagelibrary-installed-and-loaded",
    "title": "Appendix D — Debugging Steps",
    "section": "D.2 Is the package/library installed and loaded?",
    "text": "D.2 Is the package/library installed and loaded?\nIf you are calling a function, the library needs to be installed and loaded (or called explicitly).\n\nCheck if the library is loaded in the Files, Plots, Packages... under the Packages tab. A package is loaded if there is a checkmark by it.\nCheck if the package is installed. Same procedure as above, but the package won’t even be listed if it isn’t installed.\nCall the package explicitly using the syntax &lt;PACKAGE.NAME&gt;::&lt;FUNCTION.NAME&gt;. An example would be janitor::clean_names().\n\nIf you don’t know where the panels are - see this image and description. We’ll use it a few times.\n\n\n\n\n\n\nFigure D.1: RStudio\n\n\n\nHere, there are four separate windows or “Panels”.\n\nThe source panel is top-left and this is basically a text editor where you type R code or regular text. Code here gets colored and looks different if it is doing various R things. More on this later. Code doesn’t get immediately executed here; it is more like a holding place for writing/testing/debugging bigger “scripts” or programs.\nThe console/terminal is bottom-left. The terminal is the 1980s window that does commands directly. Type print(‘Hello World’), press enter/return, and you’ve written some code. This console is what R would look like if you ran it without RStudio.\nThe top-right is the file manager or Files, Plots, Packages... panel. It shows files in your directory, plots, packages, and help files.\nThe bottom-right is the programming environment or Environment panel. It contains things you’ve loaded or coded. Right now it should be empty because you’ve done neither.",
    "crumbs": [
      "Home",
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Debugging Steps</span>"
    ]
  },
  {
    "objectID": "debugging.html#is-the-data-loaded-in-the-global-environment",
    "href": "debugging.html#is-the-data-loaded-in-the-global-environment",
    "title": "Appendix D — Debugging Steps",
    "section": "D.3 Is the data loaded in the global environment?",
    "text": "D.3 Is the data loaded in the global environment?\nThe data needs to be imported and loaded into the global environment before it can be manipulated.\n\nCheck the Environment panel to see if the dataset of interest is loaded and named as expected.",
    "crumbs": [
      "Home",
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Debugging Steps</span>"
    ]
  },
  {
    "objectID": "debugging.html#is-the-syntax-correct",
    "href": "debugging.html#is-the-syntax-correct",
    "title": "Appendix D — Debugging Steps",
    "section": "D.4 Is the syntax correct?",
    "text": "D.4 Is the syntax correct?\nThe syntax for the code needs to be correct, and needs to follow the idiomatic rules of R.\n\nShould it be a pipe |&gt; or a +?\nDoes the function include all required arguments?\nDid I pass in the data?",
    "crumbs": [
      "Home",
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Debugging Steps</span>"
    ]
  },
  {
    "objectID": "debugging.html#does-a-simpler-version-work",
    "href": "debugging.html#does-a-simpler-version-work",
    "title": "Appendix D — Debugging Steps",
    "section": "D.5 Does a simpler version work?",
    "text": "D.5 Does a simpler version work?\nWhen coding in class, we’ll often start simple, then add complexity for our visualization code. The reason for that is to make sure the very basic things are all there and working, and also because it makes debugging much simpler.\nIf a long piece of code breaks, try to see if a minimum reproducible example does work. This is standard for trouble-shooting to make sure none of the previous four things are the problem. Try to make it a single line of code with the most basic functionality and see if that works. Then add one line at a time to see where things break.",
    "crumbs": [
      "Home",
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Debugging Steps</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Facebook, Twitter, Show more sharing options, Facebook, Twitter,\nLinkedIn, Email, Copy Link URLCopied!, and Print. 2022.\n“Op-Ed: We Mapped the Warehouse Takeover\nof the Inland Empire. The Results Are\nOverwhelming.” Los Angeles Times.\nhttps://www.latimes.com/opinion/story/2022-05-01/inland-empire-warehouse-growth-map-environment.\n\n\nGommeh, Efrat, Huub Dijstelbloem, and Tamara Metze. 2021. “Visual\nDiscourse Coalitions: Visualization and Discourse Formation in\nControversies over Shale Gas Development.” Journal of\nEnvironmental Policy & Planning 23 (3): 363–80. https://doi.org/10.1080/1523908X.2020.1823208.\n\n\nGrainger, Sam, Feng Mao, and Wouter Buytaert. 2016. “Environmental\nData Visualisation for Non-Scientific Contexts: Literature\nReview and Design Framework.” Environmental Modelling &\nSoftware 85 (November): 299–318. https://doi.org/10.1016/j.envsoft.2016.09.004.\n\n\nHorton, Simon, Stan Nowak, and Pascal Haegeli. 2020. “Enhancing\nthe Operational Value of Snowpack Models with Visualization Design\nPrinciples.” Natural Hazards and Earth System Sciences\n20 (6): 1557–72. https://doi.org/10.5194/nhess-20-1557-2020.\n\n\nKelleher, Christa, and Thorsten Wagener. 2011. “Ten Guidelines for\nEffective Data Visualization in Scientific Publications.”\nEnvironmental Modelling & Software 26 (6): 822–27. https://doi.org/10.1016/j.envsoft.2010.12.006.\n\n\nMurchie, Karen J., and Dylan Diomede. 2020. “Fundamentals of\nGraphic Designessential Tools for Effective Visual Science\nCommunication.” FACETS 5 (1): 409–22. https://doi.org/10.1139/facets-2018-0049.\n\n\nTufte, Edward R. 2001. The Visual Display of\nQuantitative Information, 2nd Ed. 2nd\nedition. Cheshire, Conn: Graphics Press.\n\n\n———, ed. 2013. Envisioning Information. 14. print.\nCheshire, Conn: Graphics Press.\n\n\nvan Beek, Lisette, Tamara Metze, Eva Kunseler, Hiddo Huitzing, Filip de\nBlois, and Arjan Wardekker. 2020. “Environmental Visualizations:\nFraming and Reframing Between Science, Policy and\nSociety.” Environmental Science & Policy 114\n(December): 497–505. https://doi.org/10.1016/j.envsci.2020.09.011.\n\n\nWickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy\nD’Agostino McGowan, Romain FranÃois, Garrett Grolemund, et al. 2019.\n“Welcome to the tidyverse.”\nJournal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686.",
    "crumbs": [
      "Home",
      "Appendices",
      "References"
    ]
  },
  {
    "objectID": "weather.html",
    "href": "weather.html",
    "title": "Unit 1: Introduction to Data Visualization",
    "section": "",
    "text": "The most common environmental dataset that people encounter is the weather. Every major type of media devotes time to the weather: newspapers, news shows, apps, twitter bots, and The Weather Channel.\nIt is a great example for us to start talking about environmental data. Additionally, the weather is often confused for the Climate. Given the existential threat of Climate Change, we’ll explore, replicate, and maybe improve some climate visualizations.\n\nComparative Analysis\nExamine Figure 1 from the Los Angeles Times, a Twitter emoji weather map, and a screenshot from Wunderground.\n\n\n\n\n\n\nWhat are your impressions of these different visualizations?\n\n\n\n\n\n\n\n\n\nFigure 1: Los Angeles Times Weather Map\n\n\n\n\n\n\n\n\n\nFigure 2: Twitter emoji Map\n\n\n\n\n\n\n\n\n\nFigure 3: Wunderground weather website and app",
    "crumbs": [
      "Home",
      "Unit 1: Introduction to Data Visualization"
    ]
  },
  {
    "objectID": "EJ.html",
    "href": "EJ.html",
    "title": "Unit 2: Environmental Justice",
    "section": "",
    "text": "This unit focuses on the identification and quantification of the unequal distribution and access to natural resources within California and the United States - Environmental Justice",
    "crumbs": [
      "Home",
      "Unit 2: Environmental Justice"
    ]
  },
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "Unit 3: Data",
    "section": "",
    "text": "Data\nUnfortunately, no class on Environmental Data Visualization can be complete without at least an introduction to the import and export of data and visualizations. In previous units, we’ve covered the basics of visualization, and the basics of environment through the lens of environmental justice. In this unit, we deal with the least satisfactory and potentially most frustrating component of the chain - data.\nI’ll provide an overview of data import functions and data export functions. Then we will have a guest lecture on data ownership and context that goes into the theory and ethics of data generation and ownership.\nFigure 1 provides a meme of the iconic android Data from Star Trek: TNG",
    "crumbs": [
      "Home",
      "Unit 3: Data"
    ]
  },
  {
    "objectID": "data.html#data",
    "href": "data.html#data",
    "title": "Unit 3: Data",
    "section": "",
    "text": "Figure 1: Brent Spiner",
    "crumbs": [
      "Home",
      "Unit 3: Data"
    ]
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Unit 4: Special Projects",
    "section": "",
    "text": "Projects\nIn this unit, we will focus on the topics of interest for the class projects. I will provide a basic overview of the topic areas, but the class will spend most of the time leading the research. Three graded assignments are associated with this section.",
    "crumbs": [
      "Home",
      "Unit 4: Special Projects"
    ]
  },
  {
    "objectID": "projects.html#projects",
    "href": "projects.html#projects",
    "title": "Unit 4: Special Projects",
    "section": "",
    "text": "Group presentation - groups of four\nIndividual visualization - individually developed prototypes on the theme of the group.\nGroup project - groups of four",
    "crumbs": [
      "Home",
      "Unit 4: Special Projects"
    ]
  }
]